## 1) **데이터 기본 상태 확인**

* [ ] 데이터 크기: rows, columns
* [ ] 컬럼별 타입(type) 점검
* [ ] PK로 쓸 수 있는 고유 ID 존재 여부
* [ ] 스키마가 기획 문서/ERD와 일치하는지
* [ ] 예상보다 row 수가 많거나 적지 않은지(로그 누락 체크)

---

## 2) **결측치(Null) 점검**

* [ ] 컬럼별 결측치 비율
* [ ] 결측치가 특정 날짜/특정 ID에 집중되어 있는지
* [ ] Null이 허용 가능한 컬럼인지 (도메인 지식 대조)
* [ ] 결측치 발생 원인 추정(log missing, ETL 실패 등)

---

## 3) **중복 데이터 점검**

* [ ] PK 기준으로 중복 row 존재 여부
* [ ] 특정 기준(User ID, session ID)에서 중복 행동 여부
* [ ] Kafka Duplicate 가능성(At-least once 처리) 분석
* [ ] 중복 제거 시 전체 row가 얼마나 감소하는지

---

## 4) **값의 분포/범위 점검**

* [ ] 수치형 컬럼 값이 합리적인 범위인지

  * 예: CTR 0~1, 클릭수 음수값 없음
* [ ] 날짜/시간 값 파싱 가능한지
* [ ] 범주형 컬럼의 unique 종류 수
* [ ] 희소(high-cardinality) 범주형 여부(메모리/성능 영향)

---

## 5) **이상치(Outlier) 확인**

* [ ] 극단적인 값 존재 여부
* [ ] 상식적으로 불가능한 값(음수, 비정상 코드값)
* [ ] outlier가 수집 오류인지, 실제 이벤트인지 판단
* [ ] outlier 비율이 전체 데이터에 큰 영향 주는지

---

## 6) **비즈니스 로직 기반 유효성 검사**

* [ ] 클릭수 ≤ 노출수 (이 조건 어기면 ETL 오류)
* [ ] timestamp 정렬이 자연스럽게 이어지는지
* [ ] 캠페인/광고 ID가 실제 존재하는 ID인지
* [ ] session_time = out - in 계산 시 음수 아닌지

---

## 7) **스키마 드리프트 체크**

Kafka/실시간 로그에서 자주 발생함.

* [ ] 신규 컬럼이 갑자기 등장했는가
* [ ] 기존 컬럼이 사라졌는가
* [ ] 타입이 변했는가 (int → string)
* [ ] 값의 범주가 갑자기 증가했는가

---

## 8) **날짜/시간 데이터 품질 체크**

* [ ] timestamp 포맷이 일관적인지
* [ ] timezone 변동 문제 없는지
* [ ] 하루 단위/시간 단위 누락 없나
* [ ] 로그가 밀리거나 순서가 이상하지 않은지

---

## 9) **관계(Join 가능 여부) 점검**

ETL/ELT 때 골치 아픈 부분.

* [ ] User ID가 두 테이블 간 join 가능한지
* [ ] FK가 결측/깨짐 없이 일치하는가
* [ ] 다대일(one-to-many) 비율이 정상인지
* [ ] dim 테이블과 fact 테이블 연결 문제 없는지

---

## 10) **메타데이터 & 문서화**

* [ ] 스키마 정의서 작성(Create Table 문 포함)
* [ ] 컬럼 설명(Description)
* [ ] 단위(unit) 정리(예: ms vs sec)
* [ ] EDA 결과를 md로 문서화

---

# 🔥 요약 — “DE 실무 EDA 체크리스트” 핵심 8개만

```
(1) 스키마 일치 여부
(2) 데이터 크기 & 누락 여부
(3) 결측치 비율
(4) 중복 데이터
(5) 범위 / 분포 / 이상치
(6) 날짜/시간 누락·순서 오류
(7) 스키마 드리프트(타입변경/신규컬럼)
(8) Join 가능 여부(FK 유효성)
```



