# -*- coding: utf-8 -*-
"""
Flink CTR Streaming Job

ì‹¤ì‹œê°„ ê´‘ê³  í´ë¦­ ë°ì´í„° ì²˜ë¦¬:
- Kafkaì—ì„œ Avro í¬ë§· ì´ë²¤íŠ¸ ìˆ˜ì‹ 
- 1ë¶„/5ë¶„ Tumbling Windowë¡œ CTR ì§‘ê³„
- ì›ë³¸ ì´ë²¤íŠ¸ ë° ì§‘ê³„ ë©”íŠ¸ë¦­ì„ PostgreSQLì— ì €ì¥

ë°ì´í„° íë¦„:
  Kafka (Avro) â†’ Flink (íŒŒì‹±/ë³€í™˜) â†’ PostgreSQL (ì €ì¥)
"""

import sys
import json
import logging
from datetime import datetime
from typing import Dict, Any

# Flink
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.functions import MapFunction, WindowFunction, SinkFunction
from pyflink.datastream.window import TumblingEventTimeWindows
from pyflink.common.serialization import SimpleStringSchema
from pyflink.datastream.connectors.kafka import FlinkKafkaConsumer
from pyflink.common.time import Time

# Avro
import fastavro
import io

# PostgreSQL
import psycopg2
from psycopg2.extras import execute_batch

# Config
from config import FlinkConfig

# ============================================================
# ë¡œê¹… ì„¤ì •
# ============================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================
# ë°ì´í„° ëª¨ë¸
# ============================================================

class AdEvent:
    """
    ê´‘ê³  ì´ë²¤íŠ¸ ë°ì´í„° ëª¨ë¸

    Kafkaì—ì„œ ìˆ˜ì‹ í•œ Avro ë°ì´í„°ë¥¼ íŒŒì‹±í•œ Python ê°ì²´
    """

    def __init__(self, data: Dict[str, Any]):
        """
        Args:
            data: Avro íŒŒì‹± ê²°ê³¼ (ë”•ì…”ë„ˆë¦¬)
        """
        self.id = data.get('id')
        self.click = data.get('click')
        self.hour = data.get('hour')
        self.banner_pos = data.get('banner_pos')
        self.site_id = data.get('site_id')
        self.site_domain = data.get('site_domain')
        self.site_category = data.get('site_category')
        self.app_id = data.get('app_id')
        self.app_domain = data.get('app_domain')
        self.app_category = data.get('app_category')
        self.device_id = data.get('device_id')
        self.device_ip = data.get('device_ip')
        self.device_model = data.get('device_model')
        self.device_type = data.get('device_type')
        self.device_conn_type = data.get('device_conn_type')
        self.C1 = data.get('C1')
        self.C14 = data.get('C14')
        self.C15 = data.get('C15')
        self.C16 = data.get('C16')
        self.C17 = data.get('C17')
        self.C18 = data.get('C18')
        self.C19 = data.get('C19')
        self.C20 = data.get('C20')
        self.C21 = data.get('C21')
        # ìœˆë„ìš° ì²˜ë¦¬ìš© íƒ€ì„ìŠ¤íƒ¬í”„ (ë°€ë¦¬ì´ˆ)
        self.timestamp = int(datetime.now().timestamp() * 1000)

    def __str__(self):
        return f"AdEvent(id={self.id}, click={self.click}, site_id={self.site_id})"

    def to_tuple(self):
        """PostgreSQL INSERTìš© íŠœí”Œë¡œ ë³€í™˜"""
        return (
            self.id, self.click, self.hour, self.banner_pos,
            self.site_id, self.site_domain, self.site_category,
            self.app_id, self.app_domain, self.app_category,
            self.device_id, self.device_ip, self.device_model,
            self.device_type, self.device_conn_type,
            self.C1, self.C14, self.C15, self.C16, self.C17,
            self.C18, self.C19, self.C20, self.C21
        )


class CTRMetric:
    """
    CTR ì§‘ê³„ ì§€í‘œ ë°ì´í„° ëª¨ë¸

    Flink Windowì—ì„œ ì§‘ê³„í•œ ê²°ê³¼
    """

    def __init__(self, window_start: int, window_end: int,
                 impressions: int, clicks: int):
        """
        Args:
            window_start: ìœˆë„ìš° ì‹œì‘ ì‹œê°„ (ë°€ë¦¬ì´ˆ)
            window_end: ìœˆë„ìš° ì¢…ë£Œ ì‹œê°„ (ë°€ë¦¬ì´ˆ)
            impressions: ë…¸ì¶œ ê±´ìˆ˜ (click=0)
            clicks: í´ë¦­ ê±´ìˆ˜ (click=1)
        """
        self.window_start = datetime.fromtimestamp(window_start / 1000)
        self.window_end = datetime.fromtimestamp(window_end / 1000)
        self.impressions = impressions
        self.clicks = clicks
        self.ctr = (clicks / impressions * 100) if impressions > 0 else 0.0

    def __str__(self):
        return (f"CTRMetric(window={self.window_start}, "
                f"impressions={self.impressions}, clicks={self.clicks}, ctr={self.ctr:.2f}%)")

    def to_tuple(self):
        """PostgreSQL INSERTìš© íŠœí”Œë¡œ ë³€í™˜"""
        return (self.window_start, self.window_end, self.impressions, self.clicks, self.ctr)


# ============================================================
# Avro ìŠ¤í‚¤ë§ˆ ë¡œë“œ
# ============================================================

def load_avro_schema(schema_file: str) -> Dict[str, Any]:
    """
    Avro ìŠ¤í‚¤ë§ˆ íŒŒì¼ ë¡œë“œ

    Args:
        schema_file: ìŠ¤í‚¤ë§ˆ íŒŒì¼ ê²½ë¡œ

    Returns:
        Avro ìŠ¤í‚¤ë§ˆ (ë”•ì…”ë„ˆë¦¬)
    """
    try:
        with open(schema_file, 'r') as f:
            schema = json.load(f)
        logger.info(f"âœ… Loaded Avro schema from {schema_file}")
        return schema
    except Exception as e:
        logger.error(f"âŒ Failed to load schema from {schema_file}: {e}")
        raise


# ============================================================
# ì´ë²¤íŠ¸ íŒŒì‹± í•¨ìˆ˜
# ============================================================

class AvroDeserializer(MapFunction):
    """Avro ë°”ì´íŠ¸ë¥¼ AdEventë¡œ ë³€í™˜"""

    def __init__(self):
        self.ad_event_schema = None

    def open(self, runtime_context):
        """Flinkì—ì„œ í•¨ìˆ˜ ì´ˆê¸°í™” ì‹œ í˜¸ì¶œ"""
        self.ad_event_schema = load_avro_schema(FlinkConfig.AD_EVENT_SCHEMA_FILE)

    def map(self, value):
        """
        Kafka ë©”ì‹œì§€ (Avro ë°”ì´íŠ¸)ë¥¼ AdEventë¡œ ë³€í™˜

        Args:
            value: Avro ì¸ì½”ë”©ëœ ë°”ì´íŠ¸

        Returns:
            AdEvent ê°ì²´
        """
        try:
            # Avro ë°”ì´íŠ¸ ë””ì½”ë”©
            bytes_reader = io.BytesIO(value)
            reader = fastavro.reader(bytes_reader, reader_schema=self.ad_event_schema)
            record = next(reader)

            # AdEvent ê°ì²´ ìƒì„±
            event = AdEvent(record)
            logger.debug(f"âœ… Parsed: {event}")
            return event

        except Exception as e:
            logger.error(f"âŒ Failed to parse Avro message: {e}")
            raise


# ============================================================
# Kafka Consumer ì„¤ì •
# ============================================================

def setup_kafka_consumer() -> FlinkKafkaConsumer:
    """
    Kafka Consumer ì„¤ì • (Avro í¬ë§·)

    Returns:
        FlinkKafkaConsumer ê°ì²´
    """
    kafka_config = {
        'bootstrap.servers': FlinkConfig.BOOTSTRAP_SERVERS,
        'group.id': FlinkConfig.KAFKA_GROUP_ID,
        'auto.offset.reset': FlinkConfig.KAFKA_AUTO_OFFSET_RESET,
    }

    # SimpleStringSchema ì‚¬ìš© (Avro ë°”ì´íŠ¸ë¥¼ ì§ì ‘ ì²˜ë¦¬)
    consumer = FlinkKafkaConsumer(
        FlinkConfig.KAFKA_TOPIC_RAW,
        SimpleStringSchema(),  # ë°”ì´íŠ¸ ìˆ˜ì‹ 
        kafka_config
    )

    logger.info(f"âœ… Kafka Consumer configured: {FlinkConfig.KAFKA_TOPIC_RAW}")
    return consumer


# ============================================================
# Window ì§‘ê³„ í•¨ìˆ˜
# ============================================================

class CTRWindow(WindowFunction):
    """
    1ë¶„ ë˜ëŠ” 5ë¶„ Windowì—ì„œ CTR ê³„ì‚°

    ì…ë ¥: ìœˆë„ìš° ë‚´ ëª¨ë“  AdEvent
    ì¶œë ¥: CTRMetric (1ê°œ)
    """

    def apply(self, key, window, elements):
        """
        Window ì§‘ê³„ í•¨ìˆ˜

        Args:
            key: Group í‚¤ (í˜„ì¬ëŠ” ì‚¬ìš© ì•ˆ í•¨)
            window: ìœˆë„ìš° ë©”íƒ€ë°ì´í„°
            elements: ìœˆë„ìš° ë‚´ ëª¨ë“  AdEvent

        Yields:
            CTRMetric
        """
        # ì´ë²¤íŠ¸ ìˆ˜ì§‘
        events = list(elements)

        # ì§‘ê³„ ê³„ì‚°
        impressions = sum(1 for e in events if e.click == 0)
        clicks = sum(1 for e in events if e.click == 1)

        # ë©”íŠ¸ë¦­ ìƒì„±
        metric = CTRMetric(
            window_start=window.get_start(),
            window_end=window.get_end(),
            impressions=impressions,
            clicks=clicks
        )

        logger.info(f"ğŸ“Š Window Result: {metric}")
        yield metric


# ============================================================
# PostgreSQL Sink í•¨ìˆ˜
# ============================================================

class PostgreSQLRawEventsSink(SinkFunction):
    """ì›ë³¸ ì´ë²¤íŠ¸ë¥¼ PostgreSQL (realtime.ad_events)ì— ì €ì¥"""

    def __init__(self):
        self.conn = None
        self.batch = []

    def invoke(self, value: AdEvent, context=None):
        """
        ê° ì´ë²¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê³  ë°°ì¹˜ì— ì¶”ê°€

        Args:
            value: AdEvent ê°ì²´
            context: Flink ì»¨í…ìŠ¤íŠ¸ (ì„ íƒ)
        """
        if self.conn is None:
            self._connect()

        try:
            # ë°°ì¹˜ì— ì¶”ê°€
            self.batch.append(value.to_tuple())

            # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ INSERT
            if len(self.batch) >= FlinkConfig.BATCH_SIZE:
                self._flush()

        except Exception as e:
            logger.error(f"âŒ Error adding event to batch: {e}")

    def _connect(self):
        """PostgreSQL ì—°ê²° ì´ˆê¸°í™”"""
        try:
            self.conn = psycopg2.connect(
                host=FlinkConfig.POSTGRES_HOST,
                port=FlinkConfig.POSTGRES_PORT,
                database=FlinkConfig.POSTGRES_DB,
                user=FlinkConfig.POSTGRES_USER,
                password=FlinkConfig.POSTGRES_PASSWORD
            )
            logger.info(f"âœ… PostgreSQL connected: {FlinkConfig.POSTGRES_HOST}")
        except Exception as e:
            logger.error(f"âŒ PostgreSQL connection failed: {e}")
            raise

    def _flush(self):
        """ë°°ì¹˜ ë°ì´í„°ë¥¼ PostgreSQLì— INSERT"""
        if not self.batch or self.conn is None:
            return

        try:
            sql = f"""
                INSERT INTO {FlinkConfig.PG_TABLE_RAW_EVENTS}
                (id, click, hour, banner_pos, site_id, site_domain, site_category,
                 app_id, app_domain, app_category, device_id, device_ip, device_model,
                 device_type, device_conn_type, C1, C14, C15, C16, C17, C18, C19, C20, C21)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """

            with self.conn.cursor() as cursor:
                execute_batch(cursor, sql, self.batch)

            self.conn.commit()
            logger.info(f"âœ… Inserted {len(self.batch)} raw events to PostgreSQL")
            self.batch = []

        except Exception as e:
            logger.error(f"âŒ PostgreSQL INSERT failed: {e}")
            if self.conn:
                self.conn.rollback()

    def finish(self):
        """ë§ˆì§€ë§‰ ë°°ì¹˜ í”ŒëŸ¬ì‹œ ë° ì—°ê²° ì¢…ë£Œ"""
        try:
            self._flush()
            if self.conn:
                self.conn.close()
            logger.info("âœ… PostgreSQL connection closed")
        except Exception as e:
            logger.error(f"âŒ Error closing PostgreSQL: {e}")


class PostgreSQLMetricsSink(SinkFunction):
    """ì§‘ê³„ ë©”íŠ¸ë¦­ì„ PostgreSQL (realtime.ctr_metrics_*min)ì— ì €ì¥"""

    def __init__(self, table_name: str):
        self.table_name = table_name
        self.conn = None

    def invoke(self, value: CTRMetric, context=None):
        """
        ë©”íŠ¸ë¦­ì„ PostgreSQLì— INSERT

        Args:
            value: CTRMetric ê°ì²´
            context: Flink ì»¨í…ìŠ¤íŠ¸ (ì„ íƒ)
        """
        if self.conn is None:
            self._connect()

        try:
            sql = f"""
                INSERT INTO {self.table_name}
                (window_start, window_end, impressions, clicks, ctr)
                VALUES (%s, %s, %s, %s, %s)
                ON CONFLICT DO NOTHING
            """

            if self.conn:
                with self.conn.cursor() as cursor:
                    cursor.execute(sql, value.to_tuple())

                self.conn.commit()
                logger.info(f"âœ… Inserted metric to {self.table_name}: {value}")

        except Exception as e:
            logger.error(f"âŒ PostgreSQL INSERT failed: {e}")
            if self.conn:
                self.conn.rollback()

    def _connect(self):
        """PostgreSQL ì—°ê²° ì´ˆê¸°í™”"""
        try:
            self.conn = psycopg2.connect(
                host=FlinkConfig.POSTGRES_HOST,
                port=FlinkConfig.POSTGRES_PORT,
                database=FlinkConfig.POSTGRES_DB,
                user=FlinkConfig.POSTGRES_USER,
                password=FlinkConfig.POSTGRES_PASSWORD
            )
            logger.info(f"âœ… PostgreSQL connected for metrics: {self.table_name}")
        except Exception as e:
            logger.error(f"âŒ PostgreSQL connection failed: {e}")
            raise

    def finish(self):
        """ì—°ê²° ì¢…ë£Œ"""
        try:
            if self.conn:
                self.conn.close()
            logger.info(f"âœ… PostgreSQL connection closed for {self.table_name}")
        except Exception as e:
            logger.error(f"âŒ Error closing PostgreSQL: {e}")


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================

def main():
    """Flink CTR ìŠ¤íŠ¸ë¦¬ë° ì‘ì—… ë©”ì¸ í•¨ìˆ˜"""

    try:
        # ì„¤ì • ê²€ì¦
        FlinkConfig.validate()
        FlinkConfig.print_config()
        logger.info("âœ… Configuration validated")

    except Exception as e:
        logger.error(f"âŒ Configuration error: {e}")
        return 1

    try:
        # StreamExecutionEnvironment ì´ˆê¸°í™”
        env = StreamExecutionEnvironment.get_execution_environment()
        env.set_parallelism(FlinkConfig.PARALLELISM)

        # Checkpoint ì„¤ì • (Flink 1.17.1)
        # enable_checkpointing(interval_ms)ë¡œ ì„¤ì •
        env.enable_checkpointing(FlinkConfig.CHECKPOINT_INTERVAL)

        logger.info("âœ… StreamExecutionEnvironment initialized")
        logger.info(f"   Parallelism: {FlinkConfig.PARALLELISM}")
        logger.info(f"   Checkpoint Interval: {FlinkConfig.CHECKPOINT_INTERVAL}ms")

        # Kafka Consumer ì„¤ì •
        kafka_consumer = setup_kafka_consumer()

        # ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ìƒì„± ë° Avro ë””ì½”ë”©
        kafka_stream = env.add_source(kafka_consumer)
        logger.info("âœ… Kafka source added")

        # Avro ë””ì½”ë”©
        parsed_stream = kafka_stream.map(AvroDeserializer())
        logger.info("âœ… Avro deserialization pipeline configured")

        # ì›ë³¸ ì´ë²¤íŠ¸ë¥¼ PostgreSQLì— ì €ì¥ (ë³‘ë ¬ ì²˜ë¦¬)
        parsed_stream.add_sink(PostgreSQLRawEventsSink())
        logger.info("âœ… Raw events sink configured")

        # 1ë¶„ Window ì§‘ê³„
        ctr_1min_stream = (
            parsed_stream
            .key_by(lambda e: "default_key")  # ëª¨ë“  ì´ë²¤íŠ¸ë¥¼ ê°™ì€ ìœˆë„ìš°ë¡œ
            .window(TumblingEventTimeWindows.of(Time.milliseconds(FlinkConfig.WINDOW_SIZE_1MIN)))
            .apply(CTRWindow())
        )
        ctr_1min_stream.add_sink(PostgreSQLMetricsSink(FlinkConfig.PG_TABLE_METRICS_1MIN))
        logger.info("âœ… 1min window aggregation configured")

        # 5ë¶„ Window ì§‘ê³„
        ctr_5min_stream = (
            parsed_stream
            .key_by(lambda e: "default_key")
            .window(TumblingEventTimeWindows.of(Time.milliseconds(FlinkConfig.WINDOW_SIZE_5MIN)))
            .apply(CTRWindow())
        )
        ctr_5min_stream.add_sink(PostgreSQLMetricsSink(FlinkConfig.PG_TABLE_METRICS_5MIN))
        logger.info("âœ… 5min window aggregation configured")

        # ì‘ì—… ì‹¤í–‰
        logger.info("=" * 60)
        logger.info("Starting Flink CTR Streaming Job...")
        logger.info("=" * 60)

        env.execute("Flink CTR Streaming Job")
        logger.info("âœ… Job completed successfully!")
        return 0

    except Exception as e:
        logger.error(f"âŒ Job failed: {e}", exc_info=True)
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())
