# Week 1: ë°ì´í„° ìˆ˜ì§‘ & ìŠ¤íŠ¸ë¦¬ë° ê¸°ì´ˆ

**ëª©í‘œ:** Avazu ë°ì´í„°ë¥¼ Kafkaë¡œ ìˆ˜ì§‘í•˜ê³  ì™„ì „íˆ ì‘ë™í•˜ëŠ” ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì™„ì„±

**ê¸°ê°„:** 5ì¼ (ì›”~ê¸ˆ)
**ì¼ì¼ ë¶„ëŸ‰:** 2ì‹œê°„
**ì´ ì‹œê°„:** 10ì‹œê°„

---

## ğŸ“… ì£¼ê°„ ì¼ì •í‘œ

| ë‹¨ê³„ | ì£¼ì œ | ì‹œê°„ | ëˆ„ì  |
|------|------|------|------|
| **Day 0** | í”„ë¡œì íŠ¸ ì´ˆê¸° ì„¤ì • (ì‚¬ì „) | 0.5h | 0.5h |
| **ì›”** | Avazu ë°ì´í„° ë¶„ì„ & ìƒ˜í”Œë§ | 2h | 2.5h |
| **í™”** | Kafka + Schema Registry êµ¬ì¶• | 2h | 4.5h |
| **ìˆ˜** | Kafka Topic ìƒì„± & í…ŒìŠ¤íŠ¸ | 2h | 6.5h |
| **ëª©** | Python Kafka Producer + DLQ ê°œë°œ | 2h | 8.5h |
| **ê¸ˆ** | ëª¨ë‹ˆí„°ë§ & í†µí•© í…ŒìŠ¤íŠ¸ | 2h | 10.5h |

---

## ğŸ“Œ Day 0 (ì‚¬ì „): í”„ë¡œì íŠ¸ ì´ˆê¸° ì„¤ì • (30ë¶„)

### ëª©í‘œ
- Git ì´ˆê¸°í™” ë° ê¸°ë³¸ ì„¤ì •
- í´ë” êµ¬ì¡° ìƒì„±
- í•„ìš”í•œ íŒŒì¼ ë° í™˜ê²½ ì„¤ì •
- Python ì˜ì¡´ì„± ê´€ë¦¬

### ğŸ› ï¸ ì‹¤ìŠµ ë‚´ìš©

#### 0-1. Git ì´ˆê¸°í™” (5ë¶„)

```bash
# Git ì´ˆê¸°í™”
git init
git config user.name "Your Name"
git config user.email "your@email.com"

# GitHubì— í‘¸ì‹œí•  ê²½ìš°
git remote add origin https://github.com/your-repo/marketing_roas.git
```

#### 0-2. í´ë” êµ¬ì¡° ìƒì„± (10ë¶„)

**íŒŒì¼:** `scripts/init_project.sh`

```bash
#!/bin/bash

echo "ğŸ“ Creating project structure..."

# ë©”ì¸ ë””ë ‰í† ë¦¬
mkdir -p src/{analysis,kafka,flink,postgres,streamlit,airflow,monitoring}
mkdir -p data/{raw,processed,checkpoints}
mkdir -p config
mkdir -p schemas
mkdir -p scripts
mkdir -p tests
mkdir -p docs/{plan,eda_report}

# ì´ˆê¸° íŒŒì¼ë“¤ ìƒì„±
touch .gitignore
touch README.md
touch .env.example
touch requirements.txt

# ë””ë ‰í† ë¦¬ë³„ __init__.py ìƒì„±
touch src/__init__.py
touch src/analysis/__init__.py
touch src/kafka/__init__.py
touch src/flink/__init__.py
touch src/postgres/__init__.py

echo "âœ… Project structure created successfully"
echo ""
echo "ğŸ“‚ Directory tree:"
tree -L 2 --dirsfirst 2>/dev/null || find . -maxdepth 2 -type d | sort
```

**ì‹¤í–‰:**
```bash
bash scripts/init_project.sh
```

**ê²°ê³¼:**
```
marketing_roas/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ flink/
â”‚   â”œâ”€â”€ postgres/
â”‚   â”œâ”€â”€ streamlit/
â”‚   â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ monitoring/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ checkpoints/
â”œâ”€â”€ config/
â”œâ”€â”€ schemas/
â”œâ”€â”€ scripts/
â”œâ”€â”€ tests/
â”œâ”€â”€ docs/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ docker-compose.yml
```

#### 0-3. .gitignore ì‘ì„± (5ë¶„)

**íŒŒì¼:** `.gitignore`

```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# IDE
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Environment
.env
.env.local
.env.*.local

# Data
data/train.gz
data/raw/*.csv
data/processed/*.csv

# Logs
*.log
logs/

# Cache
.pytest_cache/
.cache/

# Jupyter
.ipynb_checkpoints/

# OS
.DS_Store
Thumbs.db

# Docker
docker-compose.override.yml
```

#### 0-4. Python í™˜ê²½ ì„¤ì • (10ë¶„)

**íŒŒì¼:** `requirements.txt`

```
# Kafka
kafka-python==2.0.2
confluent-kafka==2.3.0

# PyFlink (ì„ íƒì  - Week 2ì—ì„œ ì‚¬ìš©)
apache-flink==1.17.1

# Database
psycopg2-binary==2.9.9
sqlalchemy==2.0.23
alembic==1.12.1

# Data processing
pandas==2.1.3
numpy==1.26.2

# Utilities
python-dotenv==1.0.0
pyyaml==6.0.1
requests==2.31.0
click==8.1.7

# Monitoring
prometheus-client==0.19.0

# Web (Streamlit - Week 2ì—ì„œ ì‚¬ìš©)
streamlit==1.28.1

# Testing
pytest==7.4.3
pytest-cov==4.1.0

# Logging
python-json-logger==2.0.7
```

**ì„¤ì¹˜:**
```bash
pip install -r requirements.txt

# ë˜ëŠ” êµ¬ì²´ì ì¸ ë²„ì „ ê³ ì •
pip install --upgrade pip
pip install -r requirements.txt --upgrade
```

#### 0-5. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (5ë¶„)

**íŒŒì¼:** `.env.example`

```env
# Kafka ì„¤ì •
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_TOPIC_RAW=ad_events_raw
KAFKA_TOPIC_ERROR=ad_events_error
KAFKA_TOPIC_RETRY=ad_events_retry

# Schema Registry
SCHEMA_REGISTRY_URL=http://localhost:8081

# PostgreSQL (Week 2ì—ì„œ ì‚¬ìš©)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=marketing_roas
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres

# Redis (Week 2ì—ì„œ ì‚¬ìš©)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0

# Flink (Week 2ì—ì„œ ì‚¬ìš©)
FLINK_JOBMANAGER_RPC_ADDRESS=localhost
FLINK_JOBMANAGER_RPC_PORT=6123
FLINK_TASKMANAGER_RPC_PORT=6124

# Slack ì•Œë¦¼ (Week 3ì—ì„œ ì‚¬ìš©)
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_CHANNEL=#data-pipeline

# Airflow (Week 3ì—ì„œ ì‚¬ìš©)
AIRFLOW_HOME=/path/to/airflow
AIRFLOW__CORE__DAGS_FOLDER=/path/to/dags

# ë¡œê¹…
LOG_LEVEL=INFO
LOG_FORMAT=json
```

**ì‚¬ìš©ë²•:**
```bash
# .env.exampleì„ .envë¡œ ë³µì‚¬
cp .env.example .env

# í•„ìš”í•œ ê°’ë“¤ ìˆ˜ì •
# .env íŒŒì¼ì„ gitì— ì»¤ë°‹í•˜ì§€ ì•Šê¸° (ì´ë¯¸ .gitignoreì— í¬í•¨)
```

#### 0-6. ê¸°ë³¸ README ì‘ì„± (5ë¶„)

**íŒŒì¼:** `README.md`

```markdown
# Marketing ROAS: ì‹¤ì‹œê°„ ê´‘ê³  CTR ë¶„ì„ íŒŒì´í”„ë¼ì¸

ê´‘ê³  í´ë¦­ë¥ (CTR) ë¶„ì„ì„ ìœ„í•œ ì™„ì „í•œ ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.

## ğŸ“Š ì•„í‚¤í…ì²˜ ê°œìš”

```
[Avazu Data] â†’ [Kafka] â†’ [Flink] â†’ [PostgreSQL] â†’ [Streamlit/Metabase]
                           â†“
                    [Prometheus/Grafana]
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­
- Docker & Docker Compose
- Python 3.9+
- Git

### ì„¤ì¹˜

```bash
# 1. ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/your-repo/marketing_roas.git
cd marketing_roas

# 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# .env íŒŒì¼ ìˆ˜ì •

# 3. Python ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# 4. Docker ì„œë¹„ìŠ¤ ì‹œì‘
docker-compose up -d

# 5. ì´ˆê¸° ì„¤ì •
bash scripts/init_project.sh
```

## ğŸ“… ê°œë°œ ì¼ì •

- **Week 1**: Kafka + Producer (ë°ì´í„° ìˆ˜ì§‘)
- **Week 2**: Flink + Redis + Streamlit (ì‹¤ì‹œê°„ ì²˜ë¦¬)
- **Week 3**: Airflow + dbt + Grafana (ë°°ì¹˜ & ëª¨ë‹ˆí„°ë§)

## ğŸ“š ë¬¸ì„œ

- [í”„ë¡œì íŠ¸ ê³„íš](docs/plan/README.md)
- [Week 1 ìƒì„¸ ê³„íš](docs/plan/1week/README.md)
- [ì•„í‚¤í…ì²˜](docs/architecture/architecture_posgre.mermaid)

## ğŸ”§ ê°œë°œ í™˜ê²½

```bash
# Kafka ìƒíƒœ í™•ì¸
docker-compose ps

# ë¡œê·¸ í™•ì¸
docker-compose logs -f kafka

# Python ê°œë°œ
source venv/bin/activate  # ê°€ìƒí™˜ê²½ í™œì„±í™”
python -m pytest tests/   # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```

## ğŸ“ ì§€ì›

ë¬¸ì œê°€ ìˆìœ¼ë©´ GitHub Issuesë¥¼ í†µí•´ ë³´ê³ í•´ì£¼ì„¸ìš”.
```

#### 0-7. docker-compose.yml ìœ„ì¹˜ í™•ì¸ (5ë¶„)

ì´ë¯¸ ì‘ì„±ë˜ì—ˆë‹¤ë©´:
```bash
# docker-compose.ymlì´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸
ls -la docker-compose.yml
```

ì—†ë‹¤ë©´ Day 2ì—ì„œ ìƒì„±í•˜ê²Œ ë©ë‹ˆë‹¤.

### âœ… Day 0 ì™„ë£Œ ê¸°ì¤€

- [ ] Git ì´ˆê¸°í™” ì™„ë£Œ
- [ ] í´ë” êµ¬ì¡° ìƒì„± ì™„ë£Œ
- [ ] .gitignore ì‘ì„± ì™„ë£Œ
- [ ] requirements.txt ì‘ì„± ì™„ë£Œ
- [ ] .env.example ì‘ì„± ì™„ë£Œ
- [ ] README.md ì‘ì„± ì™„ë£Œ
- [ ] `git add . && git commit -m "Init: í”„ë¡œì íŠ¸ ì´ˆê¸° ì„¤ì •"`

### ğŸ“Š Day 0 ì‚°ì¶œë¬¼

```
marketing_roas/
â”œâ”€â”€ .git/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ flink/
â”‚   â”œâ”€â”€ postgres/
â”‚   â”œâ”€â”€ streamlit/
â”‚   â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ monitoring/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ checkpoints/
â”œâ”€â”€ config/
â”œâ”€â”€ schemas/
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ init_project.sh
â””â”€â”€ tests/
```

---

## ğŸ“Œ Day 1 (ì›”): Avazu ë°ì´í„° ë¶„ì„ & ìƒ˜í”Œë§ (2ì‹œê°„)

### ëª©í‘œ
- Avazu ë°ì´í„°ì˜ êµ¬ì¡°ì™€ íŠ¹ì„± íŒŒì•…
- ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ ë° EDA ìˆ˜í–‰
- ë°ì´í„° ì „ì²˜ë¦¬ ë°©ì•ˆ ìˆ˜ë¦½

### ğŸ“‹ í• ë‹¹ ì‹œê°„
| ì‘ì—… | ì‹œê°„ |
|------|------|
| ë°ì´í„° ë¶„ì„ | 40ë¶„ |
| EDA ìˆ˜í–‰ | 50ë¶„ |
| ë¬¸ì„œí™” | 30ë¶„ |

### ğŸ› ï¸ ì‹¤ìŠµ ë‚´ìš©

#### 1-1. ë°ì´í„° êµ¬ì¡° ë¶„ì„ (20ë¶„)

**íŒŒì¼:** `src/analysis/explore_data.py`

```python
import gzip
import pandas as pd
import numpy as np

# ì²« ë²ˆì§¸ ì¤„ ì½ê¸° (í—¤ë”)
with gzip.open('data/train.gz', 'rt', encoding='utf-8') as f:
    header = f.readline().strip().split(',')
    print("Column names:")
    for i, col in enumerate(header):
        print(f"  {i}: {col}")

# íŒŒì¼ í¬ê¸° í™•ì¸
import os
file_size_gb = os.path.getsize('data/train.gz') / (1024**3)
print(f"\nFile size: {file_size_gb:.2f} GB")

# ìƒ˜í”Œ ë°ì´í„° ì½ê¸°
sample_lines = []
with gzip.open('data/train.gz', 'rt', encoding='utf-8') as f:
    header = f.readline()
    for i in range(100):
        sample_lines.append(f.readline())

# DataFrameìœ¼ë¡œ ë³€í™˜
df_sample = pd.read_csv('data/train.gz', nrows=1000)
print(f"\nDataset shape: {df_sample.shape}")
print(f"\nData types:\n{df_sample.dtypes}")
print(f"\nBasic statistics:\n{df_sample.describe()}")
print(f"\nMissing values:\n{df_sample.isnull().sum()}")
```

**ì‹¤í–‰:**
```bash
python src/analysis/explore_data.py
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Column names:
  0: id
  1: click
  2: hour
  3: C1
  ...

File size: 7.82 GB

Dataset shape: (1000, 24)

Data types:
id        object
click      int64
hour       int64
...
```

#### 1-2. EDA ìˆ˜í–‰ (30ë¶„)

**íŒŒì¼:** `src/analysis/eda_analysis.py`

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv('data/train.gz', nrows=10000)

# 1. í´ë¦­ë¥  ë¶„ì„
click_rate = df['click'].mean() * 100
print(f"Overall Click-Through Rate: {click_rate:.2f}%")

# 2. ì‹œê°„ëŒ€ë³„ í´ë¦­ë¥ 
df['hour_str'] = df['hour'].astype(str).str[:2]
hourly_ctr = df.groupby('hour_str')['click'].mean() * 100
print(f"\nHourly CTR:\n{hourly_ctr}")

# 3. ë°°ë„ˆ ìœ„ì¹˜ë³„ í´ë¦­ë¥ 
if 'banner_pos' in df.columns:
    banner_ctr = df.groupby('banner_pos')['click'].mean() * 100
    print(f"\nBanner Position CTR:\n{banner_ctr}")

# 4. ê¸°ê¸° ìœ í˜•ë³„ í´ë¦­ë¥ 
device_ctr = df.groupby('device_type')['click'].mean() * 100
print(f"\nDevice Type CTR:\n{device_ctr}")

# 5. ë°ì´í„° í’ˆì§ˆ í™•ì¸
print(f"\nData Quality:")
print(f"  Total records: {len(df):,}")
print(f"  Duplicate IDs: {df['id'].duplicated().sum()}")
print(f"  Null values: {df.isnull().sum().sum()}")

# 6. ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
if 'site_category' in df.columns:
    print(f"\nTop site categories:")
    print(df['site_category'].value_counts().head(10))

# í†µê³„ ì €ì¥
stats = {
    'total_records': len(df),
    'click_rate': click_rate,
    'features': len(df.columns),
    'date_range': f"{df['hour'].min()} - {df['hour'].max()}"
}

import json
with open('data/data_stats.json', 'w') as f:
    json.dump(stats, f, indent=2)

print("\nâœ… Statistics saved to data/data_stats.json")
```

**ì‹¤í–‰:**
```bash
python src/analysis/eda_analysis.py
```

#### 1-3. ë°ì´í„° ìƒ˜í”Œë§ (30ë¶„)

**íŒŒì¼:** `src/data/sample_data.py`

```python
import pandas as pd
import os

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs('data/raw', exist_ok=True)

# ë‹¤ì–‘í•œ í¬ê¸°ì˜ ìƒ˜í”Œ ìƒì„±
sample_sizes = {
    'train_sample_1k.csv': 1000,
    'train_sample_10k.csv': 10000,
    'train_sample_50k.csv': 50000,
}

print("Creating sample datasets...")

for filename, size in sample_sizes.items():
    df = pd.read_csv('data/train.gz', nrows=size)
    output_path = f'data/raw/{filename}'
    df.to_csv(output_path, index=False)
    print(f"âœ… {filename} ({size:,} rows)")

# ìƒ˜í”Œ í†µê³„
df_sample = pd.read_csv('data/raw/train_sample_10k.csv')
print(f"\nSample statistics (10k):")
print(f"  Click rate: {df_sample['click'].mean()*100:.2f}%")
print(f"  Records: {len(df_sample):,}")
print(f"  Columns: {len(df_sample.columns)}")
```

**ì‹¤í–‰:**
```bash
python src/data/sample_data.py
```

#### 1-4. ë¬¸ì„œ ì‘ì„± (20ë¶„)

**íŒŒì¼:** `docs/eda_report.md`

```markdown
# Avazu ë°ì´í„° íƒìƒ‰ ë¶„ì„ ë³´ê³ ì„œ

## ë°ì´í„° ê°œìš”

| í•­ëª© | ê°’ |
|------|-----|
| ì „ì²´ ë ˆì½”ë“œ | 40,428,967 |
| íŒŒì¼ í¬ê¸° | 7.82 GB |
| ì»¬ëŸ¼ ìˆ˜ | 24 |
| ì‹œê°„ ë²”ìœ„ | 140102 ~ 141031 (30ì¼) |

## ì£¼ìš” ì§€í‘œ

### í´ë¦­ë¥  (CTR)
- ì „ì²´ CTR: **16.6%**
- ì‹œê°„ëŒ€ë³„ ë²”ìœ„: 14.2% ~ 18.5%
- ì¶”ì„¸: ì˜¤í›„ ì‹œê°„ëŒ€ì— ë†’ìŒ

### ë°°ë„ˆ ìœ„ì¹˜ë³„ CTR
- ìœ„ì¹˜ 0: 18.2%
- ìœ„ì¹˜ 1: 17.1%
- ìœ„ì¹˜ 2: 15.3%

### ê¸°ê¸° ìœ í˜•ë³„ CTR
- ê¸°ê¸° 0: 17.2%
- ê¸°ê¸° 1: 15.8%

## ë°ì´í„° í’ˆì§ˆ

âœ… ì¤‘ë³µ ID ì—†ìŒ
âœ… ëŒ€ë¶€ë¶„ì˜ ì»¬ëŸ¼ì—ì„œ < 5% ê²°ì¸¡ì¹˜
âš ï¸ ì¼ë¶€ ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ì—ì„œ ë†’ì€ ê²°ì¸¡ë¥ 

## ê¶Œì¥ì‚¬í•­

1. ìƒ˜í”Œ ë°ì´í„°ë¡œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
2. ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìˆ˜ì§‘
3. ë²”ì£¼í˜• ë³€ìˆ˜ëŠ” ì¸ì½”ë”© í•„ìš”
```

### âœ… ì™„ë£Œ ê¸°ì¤€

- [ ] `explore_data.py` ì‹¤í–‰ ì™„ë£Œ
- [ ] `eda_analysis.py` ì‹¤í–‰ ì™„ë£Œ
- [ ] 3ê°œì˜ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ
- [ ] `data_stats.json` íŒŒì¼ ìƒì„±
- [ ] `eda_report.md` ì‘ì„± ì™„ë£Œ

### ğŸ“Š ì‚°ì¶œë¬¼

```
data/
â”œâ”€â”€ train.gz (ì›ë³¸)
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ train_sample_1k.csv
â”‚   â”œâ”€â”€ train_sample_10k.csv
â”‚   â”œâ”€â”€ train_sample_50k.csv
â”‚   â””â”€â”€ data_stats.json
docs/
â””â”€â”€ eda_report.md
```

---

## ğŸ“Œ Day 2 (í™”): Kafka + Schema Registry êµ¬ì¶• (2ì‹œê°„)

### ëª©í‘œ
- Kafkaì™€ Schema Registry ì™„ì „íˆ ì‘ë™í•˜ê²Œ ì„¤ì •
- Avro ìŠ¤í‚¤ë§ˆ ì •ì˜ ë° ë“±ë¡
- ë¡œì»¬ Docker í™˜ê²½ì—ì„œ ê²€ì¦

### ğŸ“‹ í• ë‹¹ ì‹œê°„
| ì‘ì—… | ì‹œê°„ |
|------|------|
| Docker Compose ì‘ì„± | 30ë¶„ |
| ì‹¤í–‰ ë° ê²€ì¦ | 30ë¶„ |
| Avro ìŠ¤í‚¤ë§ˆ ì •ì˜ | 40ë¶„ |
| í†µí•© í…ŒìŠ¤íŠ¸ | 20ë¶„ |

### ğŸ› ï¸ ì‹¤ìŠµ ë‚´ìš©

#### 2-1. Docker Compose íŒŒì¼ ì‘ì„± (20ë¶„)

**íŒŒì¼:** `docker-compose.yml` (í”„ë¡œì íŠ¸ ë£¨íŠ¸)

```yaml
version: '3.8'

services:
  # Zookeeper: Kafka í´ëŸ¬ìŠ¤í„° ê´€ë¦¬
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SYNC_LIMIT: 5
      ZOOKEEPER_INIT_LIMIT: 10
    ports:
      - "2181:2181"
    networks:
      - kafka-network
    healthcheck:
      test: echo stat | nc localhost 2181
      interval: 10s
      timeout: 5s
      retries: 5

  # Kafka: ë©”ì‹œì§€ ë¸Œë¡œì»¤
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"      # ì™¸ë¶€ í´ë¼ì´ì–¸íŠ¸
      - "29092:29092"    # ë‚´ë¶€ í†µì‹ 
      - "9101:9101"      # JMX
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: kafka
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9101 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false
    networks:
      - kafka-network
    healthcheck:
      test: kafka-broker-api-versions --bootstrap-server localhost:9092
      interval: 10s
      timeout: 10s
      retries: 5

  # Schema Registry: ìŠ¤í‚¤ë§ˆ ê´€ë¦¬
  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    container_name: schema-registry
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092
      SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL: PLAINTEXT
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_DEBUG: "false"
    networks:
      - kafka-network
    healthcheck:
      test: curl -f http://localhost:8081/subjects || exit 1
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis: ìºì‹œ (ì„ íƒ)
  redis:
    image: redis:7.2-alpine
    container_name: redis
    ports:
      - "6379:6379"
    networks:
      - kafka-network
    healthcheck:
      test: redis-cli ping
      interval: 10s
      timeout: 5s
      retries: 5

networks:
  kafka-network:
    driver: bridge
```

**ì €ì¥ ìœ„ì¹˜:** í”„ë¡œì íŠ¸ ë£¨íŠ¸

#### 2-2. Docker Compose ì‹¤í–‰ (20ë¶„)

```bash
# ì„œë¹„ìŠ¤ ì‹œì‘
docker-compose up -d

# ì§„í–‰ ìƒí™© í™•ì¸ (ëŒ€ê¸°: 1~2ë¶„)
docker-compose logs -f

# ê° ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬
docker-compose ps

# ì˜ˆìƒ ì¶œë ¥:
# NAME                COMMAND                  SERVICE             STATUS      PORTS
# zookeeper           "sh -c '/etc/confluentâ€¦   zookeeper           Up 30s      0.0.0.0:2181->2181/tcp
# kafka               "sh -c '/etc/confluentâ€¦   kafka               Up 25s      0.0.0.0:9092->9092/tcp
# schema-registry     "sh -c '/etc/confluentâ€¦   schema-registry     Up 15s      0.0.0.0:8081->8081/tcp
# redis               "docker-entrypoint.sâ€¦"   redis               Up 35s      0.0.0.0:6379->6379/tcp
```

**ì—°ê²° í…ŒìŠ¤íŠ¸:**

```bash
# Kafka ì—°ê²° í™•ì¸
docker-compose exec kafka kafka-broker-api-versions --bootstrap-server kafka:29092

# Schema Registry í™•ì¸
curl http://localhost:8081/subjects

# Redis í™•ì¸
docker-compose exec redis redis-cli ping
# ì‘ë‹µ: PONG
```

#### 2-3. Avro ìŠ¤í‚¤ë§ˆ ì •ì˜ (30ë¶„)

**íŒŒì¼:** `schemas/ad_event.avsc`

```json
{
  "type": "record",
  "name": "AdEvent",
  "namespace": "com.marketing_roas.avazu",
  "doc": "Avazu advertisement event",
  "fields": [
    {
      "name": "id",
      "type": "string",
      "doc": "Unique event ID"
    },
    {
      "name": "click",
      "type": "int",
      "doc": "1 if click, 0 if no click"
    },
    {
      "name": "hour",
      "type": "int",
      "doc": "Hour of the event (YYMMDDH format)"
    },
    {
      "name": "C1",
      "type": ["null", "int"],
      "default": null,
      "doc": "Anonymous categorical feature"
    },
    {
      "name": "banner_pos",
      "type": ["null", "int"],
      "default": null,
      "doc": "Position of the banner"
    },
    {
      "name": "site_id",
      "type": ["null", "string"],
      "default": null,
      "doc": "ID of the website"
    },
    {
      "name": "site_domain",
      "type": ["null", "string"],
      "default": null,
      "doc": "Domain of the website"
    },
    {
      "name": "site_category",
      "type": ["null", "string"],
      "default": null,
      "doc": "Category of the website"
    },
    {
      "name": "app_id",
      "type": ["null", "string"],
      "default": null,
      "doc": "ID of the application"
    },
    {
      "name": "app_domain",
      "type": ["null", "string"],
      "default": null,
      "doc": "Domain of the application"
    },
    {
      "name": "app_category",
      "type": ["null", "string"],
      "default": null,
      "doc": "Category of the application"
    },
    {
      "name": "device_id",
      "type": ["null", "string"],
      "default": null,
      "doc": "Device ID"
    },
    {
      "name": "device_ip",
      "type": ["null", "string"],
      "default": null,
      "doc": "Device IP address"
    },
    {
      "name": "device_model",
      "type": ["null", "string"],
      "default": null,
      "doc": "Device model"
    },
    {
      "name": "device_type",
      "type": ["null", "int"],
      "default": null,
      "doc": "Device type"
    },
    {
      "name": "device_conn_type",
      "type": ["null", "int"],
      "default": null,
      "doc": "Device connection type"
    },
    {
      "name": "C14",
      "type": ["null", "int"],
      "default": null,
      "doc": "Anonymous feature 14"
    },
    {
      "name": "C15",
      "type": ["null", "int"],
      "default": null,
      "doc": "Anonymous feature 15"
    },
    {
      "name": "C16",
      "type": ["null", "int"],
      "default": null,
      "doc": "Anonymous feature 16"
    },
    {
      "name": "C17",
      "type": ["null", "int"],
      "default": null,
      "doc": "Anonymous feature 17"
    },
    {
      "name": "C18",
      "type": ["null", "int"],
      "default": null,
      "doc": "Anonymous feature 18"
    },
    {
      "name": "C19",
      "type": ["null", "int"],
      "default": null,
      "doc": "Anonymous feature 19"
    },
    {
      "name": "C20",
      "type": ["null", "int"],
      "default": null,
      "doc": "Anonymous feature 20"
    },
    {
      "name": "C21",
      "type": ["null", "int"],
      "default": null,
      "doc": "Anonymous feature 21"
    },
    {
      "name": "timestamp",
      "type": "long",
      "doc": "Event timestamp in milliseconds"
    }
  ]
}
```

**Schema Registryì— ë“±ë¡:**

```bash
# ìŠ¤í‚¤ë§ˆ íŒŒì¼ì„ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜
python3 << 'EOF'
import json

# ìŠ¤í‚¤ë§ˆ ë¡œë“œ
with open('schemas/ad_event.avsc', 'r') as f:
    schema = json.load(f)

# Schema Registry ë“±ë¡ìš© JSON
payload = {
    "schema": json.dumps(schema),
    "schemaType": "AVRO"
}

# ì €ì¥
with open('schemas/register_schema.json', 'w') as f:
    json.dump(payload, f)

print("âœ… Schema preparation complete")
EOF

# Schema Registryì— ë“±ë¡
curl -X POST http://localhost:8081/subjects/ad_events_raw-value/versions \
  -H "Content-Type: application/vnd.schemaregistry.v1+json" \
  -d @schemas/register_schema.json

# ë“±ë¡ í™•ì¸
curl http://localhost:8081/subjects/ad_events_raw-value/versions
```

**ì˜ˆìƒ ì‘ë‹µ:**
```json
[1]  // ë²„ì „ 1ì´ ë“±ë¡ë¨
```

#### 2-4. í†µí•© í…ŒìŠ¤íŠ¸ (10ë¶„)

**íŒŒì¼:** `src/kafka/test_connection.py`

```python
#!/usr/bin/env python3

import requests
import json
from kafka import KafkaProducer, KafkaConsumer
import time

print("=" * 60)
print("KAFKA + SCHEMA REGISTRY í†µí•© í…ŒìŠ¤íŠ¸")
print("=" * 60)

# 1. Schema Registry ì—°ê²° í…ŒìŠ¤íŠ¸
print("\n1ï¸âƒ£  Schema Registry ì—°ê²° í…ŒìŠ¤íŠ¸...")
try:
    response = requests.get('http://localhost:8081/subjects')
    subjects = response.json()
    print(f"   âœ… Schema Registry ì •ìƒ (subjects: {subjects})")
except Exception as e:
    print(f"   âŒ Schema Registry ì—°ê²° ì‹¤íŒ¨: {e}")

# 2. Kafka ì—°ê²° í…ŒìŠ¤íŠ¸
print("\n2ï¸âƒ£  Kafka ì—°ê²° í…ŒìŠ¤íŠ¸...")
try:
    producer = KafkaProducer(bootstrap_servers=['localhost:9092'])
    producer.close()
    print(f"   âœ… Kafka Producer ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"   âŒ Kafka ì—°ê²° ì‹¤íŒ¨: {e}")

# 3. í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ë°œí–‰
print("\n3ï¸âƒ£  í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ë°œí–‰...")
try:
    producer = KafkaProducer(
        bootstrap_servers=['localhost:9092'],
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )

    test_event = {
        'id': 'test_001',
        'click': 1,
        'hour': 140102,
        'C1': 1005,
        'device_type': 1,
        'timestamp': int(time.time() * 1000)
    }

    future = producer.send('ad_events_raw', value=test_event)
    record_metadata = future.get(timeout=10)

    print(f"   âœ… ë©”ì‹œì§€ ë°œí–‰ ì„±ê³µ")
    print(f"      Topic: {record_metadata.topic}")
    print(f"      Partition: {record_metadata.partition}")
    print(f"      Offset: {record_metadata.offset}")

    producer.close()
except Exception as e:
    print(f"   âŒ ë©”ì‹œì§€ ë°œí–‰ ì‹¤íŒ¨: {e}")

# 4. ë©”ì‹œì§€ ìˆ˜ì‹  í™•ì¸
print("\n4ï¸âƒ£  ë©”ì‹œì§€ ìˆ˜ì‹  í™•ì¸...")
try:
    consumer = KafkaConsumer(
        'ad_events_raw',
        bootstrap_servers=['localhost:9092'],
        value_deserializer=lambda m: json.loads(m.decode('utf-8')),
        auto_offset_reset='earliest',
        consumer_timeout_ms=5000
    )

    messages_received = 0
    for message in consumer:
        print(f"   âœ… ë©”ì‹œì§€ ìˆ˜ì‹ : {message.value}")
        messages_received += 1
        if messages_received >= 1:
            break

    consumer.close()
except Exception as e:
    print(f"   âŒ ë©”ì‹œì§€ ìˆ˜ì‹  ì‹¤íŒ¨: {e}")

print("\n" + "=" * 60)
print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
print("=" * 60)
```

**ì‹¤í–‰:**
```bash
python src/kafka/test_connection.py
```

**ì˜ˆìƒ ì¶œë ¥:**
```
============================================================
KAFKA + SCHEMA REGISTRY í†µí•© í…ŒìŠ¤íŠ¸
============================================================

1ï¸âƒ£  Schema Registry ì—°ê²° í…ŒìŠ¤íŠ¸...
   âœ… Schema Registry ì •ìƒ (subjects: ['ad_events_raw-value'])

2ï¸âƒ£  Kafka ì—°ê²° í…ŒìŠ¤íŠ¸...
   âœ… Kafka Producer ì—°ê²° ì„±ê³µ

3ï¸âƒ£  í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ë°œí–‰...
   âœ… ë©”ì‹œì§€ ë°œí–‰ ì„±ê³µ
      Topic: ad_events_raw
      Partition: 0
      Offset: 0

4ï¸âƒ£  ë©”ì‹œì§€ ìˆ˜ì‹  í™•ì¸...
   âœ… ë©”ì‹œì§€ ìˆ˜ì‹ : {'id': 'test_001', 'click': 1, ...}

============================================================
âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!
============================================================
```

### âœ… ì™„ë£Œ ê¸°ì¤€

- [ ] Docker Compose íŒŒì¼ ì‘ì„± ì™„ë£Œ
- [ ] ëª¨ë“  ì„œë¹„ìŠ¤ ì •ìƒ ì‹¤í–‰ (`docker-compose ps` í™•ì¸)
- [ ] Schema Registryì— Ad Event ìŠ¤í‚¤ë§ˆ ë“±ë¡
- [ ] `test_connection.py` ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] Kafka Topic `ad_events_raw` ìƒì„± í™•ì¸

### ğŸ“Š ì‚°ì¶œë¬¼

```
docker-compose.yml
schemas/
â”œâ”€â”€ ad_event.avsc
â””â”€â”€ register_schema.json
src/kafka/
â””â”€â”€ test_connection.py
```

---

## ğŸ“Œ Day 3 (ìˆ˜): Kafka Topic ìƒì„± & ì„¤ì • (2ì‹œê°„)

### ëª©í‘œ
- í”„ë¡œë•ì…˜ í™˜ê²½ì— ë§ëŠ” Kafka Topic êµ¬ì„±
- Topic ì„¤ì • ìµœì í™” (íŒŒí‹°ì…˜, ë ˆí”Œë¦¬ì¹´ ë“±)
- JMX Exporterë¡œ ëª¨ë‹ˆí„°ë§ ê¸°ì´ˆ ì„¤ì •
- ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

### ğŸ“‹ í• ë‹¹ ì‹œê°„
| ì‘ì—… | ì‹œê°„ |
|------|------|
| Topic ìƒì„± | 30ë¶„ |
| ì„¤ì • ê²€ì¦ | 30ë¶„ |
| JMX ì„¤ì • | 40ë¶„ |
| ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ | 20ë¶„ |

### ğŸ› ï¸ ì‹¤ìŠµ ë‚´ìš©

#### 3-1. í”„ë¡œë•ì…˜ Kafka Topics ìƒì„± (20ë¶„)

**íŒŒì¼:** `scripts/create_topics.sh`

```bash
#!/bin/bash

# Topic ìƒì„± í•¨ìˆ˜
create_topic() {
    TOPIC_NAME=$1
    PARTITIONS=$2
    REPLICATION=$3

    echo "Creating topic: $TOPIC_NAME (partitions: $PARTITIONS, replication: $REPLICATION)"

    docker-compose exec kafka kafka-topics \
        --create \
        --bootstrap-server kafka:29092 \
        --topic $TOPIC_NAME \
        --partitions $PARTITIONS \
        --replication-factor $REPLICATION \
        --config retention.ms=86400000 \
        --config compression.type=snappy \
        --config min.insync.replicas=1 \
        --config cleanup.policy=delete \
        2>&1 || echo "âš ï¸  Topic $TOPIC_NAME already exists"
}

echo "================================"
echo "Creating Kafka Topics..."
echo "================================"

# 1. ë©”ì¸ í† í”½: ê´‘ê³  ì´ë²¤íŠ¸ (ì‹¤ì‹œê°„ ì²˜ë¦¬)
create_topic "ad_events_raw" 3 1

# 2. DLQ: ì²˜ë¦¬ ì‹¤íŒ¨í•œ ë©”ì‹œì§€
create_topic "ad_events_error" 1 1

# 3. ë¦¬íŠ¸ë¼ì´: ì¬ì²˜ë¦¬ ëŒ€ê¸°
create_topic "ad_events_retry" 1 1

echo ""
echo "âœ… Topic creation completed"
```

**ì‹¤í–‰:**
```bash
chmod +x scripts/create_topics.sh
bash scripts/create_topics.sh
```

**Topic í™•ì¸:**

```bash
# ìƒì„±ëœ Topic í™•ì¸
docker-compose exec kafka kafka-topics \
    --list \
    --bootstrap-server kafka:29092

# Topic ìƒì„¸ ì •ë³´
docker-compose exec kafka kafka-topics \
    --describe \
    --bootstrap-server kafka:29092 \
    --topic ad_events_raw

# ì˜ˆìƒ ì¶œë ¥:
# Topic: ad_events_raw    TopicId: xxx    PartitionCount: 3    ReplicationFactor: 1
# Topic: ad_events_raw    Partition: 0    Leader: 1   Replicas: [1]    Isr: [1]
# Topic: ad_events_raw    Partition: 1    Leader: 1   Replicas: [1]    Isr: [1]
# Topic: ad_events_raw    Partition: 2    Leader: 1   Replicas: [1]    Isr: [1]
```

#### 3-2. Topic ì„¤ì • ìµœì í™” (20ë¶„)

**íŒŒì¼:** `config/kafka_topics_config.json`

```json
{
  "topics": [
    {
      "name": "ad_events_raw",
      "description": "Real-time advertisement events from Avazu",
      "partitions": 3,
      "replication_factor": 1,
      "config": {
        "retention.ms": 86400000,
        "retention.bytes": 10737418240,
        "compression.type": "snappy",
        "min.insync.replicas": 1,
        "cleanup.policy": "delete",
        "flush.messages": 10000,
        "flush.ms": 30000,
        "segment.ms": 3600000,
        "max.message.bytes": 1048576
      },
      "notes": "3 partitions for high throughput, snappy compression for efficiency"
    },
    {
      "name": "ad_events_error",
      "description": "Failed messages (DLQ)",
      "partitions": 1,
      "replication_factor": 1,
      "config": {
        "retention.ms": 604800000,
        "compression.type": "gzip",
        "cleanup.policy": "delete"
      },
      "notes": "DLQ with 7 days retention for debugging"
    },
    {
      "name": "ad_events_retry",
      "description": "Messages pending retry",
      "partitions": 2,
      "replication_factor": 1,
      "config": {
        "retention.ms": 3600000,
        "compression.type": "none",
        "priority.processor": "true"
      },
      "notes": "Retry queue with 1 hour TTL"
    }
  ],
  "performance_targets": {
    "throughput_msg_per_sec": 50000,
    "latency_p99_ms": 1000,
    "availability_percent": 99.9
  }
}
```

**ì„¤ì • ì ìš©:**

```python
# config/apply_topic_configs.py
import subprocess
import json

with open('config/kafka_topics_config.json', 'r') as f:
    config = json.load(f)

for topic in config['topics']:
    topic_name = topic['name']
    configs = topic['config']

    for key, value in configs.items():
        cmd = [
            'docker-compose', 'exec', 'kafka',
            'kafka-configs',
            '--bootstrap-server', 'kafka:29092',
            '--entity-type', 'topics',
            '--entity-name', topic_name,
            '--alter',
            '--add-config', f'{key}={value}'
        ]

        subprocess.run(cmd)
        print(f"âœ… Applied {key}={value} to {topic_name}")

print("\nâœ… All configurations applied")
```

**ì‹¤í–‰:**
```bash
python config/apply_topic_configs.py
```

#### 3-3. JMX ëª¨ë‹ˆí„°ë§ ì„¤ì • (30ë¶„)

**íŒŒì¼:** `config/jmx_exporter_config.yml`

```yaml
# JMX Exporter Configuration for Kafka

lowercaseOutputName: true
lowercaseOutputLabelNames: true

rules:
  # Broker Metrics
  - pattern: "kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=([0-9]+)><value>(.+)"
    name: "kafka_server_$1_$2"
    labels:
      clientId: "$3"
      topic: "$4"
      partition: "$5"
    value: "$6"
    type: GAUGE

  - pattern: "kafka.server<type=(.+), name=(.+), clientId=(.+), brokerHost=(.+), brokerPort=(.+)><value>(.+)"
    name: "kafka_server_$1_$2"
    labels:
      clientId: "$3"
      broker: "$4:$5"
    value: "$6"

  - pattern: "kafka.server<type=(.+), name=(.+)><value>(.+)"
    name: "kafka_server_$1_$2"
    value: "$3"

  # ReplicaManager
  - pattern: "kafka.server<type=ReplicaManager, name=(.+), topic=(.+), partition=([0-9]+)><value>(.+)"
    name: "kafka_server_replica_manager_$1"
    labels:
      topic: "$2"
      partition: "$3"
    value: "$4"

  # Controller Metrics
  - pattern: "kafka.controller<type=(.+), name=(.+)><value>(.+)"
    name: "kafka_controller_$1_$2"
    value: "$3"

  # Network Metrics
  - pattern: "kafka.network<type=(.+), name=(.+)><value>(.+)"
    name: "kafka_network_$1_$2"
    value: "$3"

  # Group Coordinator
  - pattern: "kafka.coordinator.group<type=(.+), name=(.+)><value>(.+)"
    name: "kafka_coordinator_group_$1_$2"
    value: "$3"
```

**JMX Exporter Container ì¶”ê°€ (docker-compose.yml):**

```yaml
  jmx-exporter:
    image: sscaling/jmx-exporter:latest
    container_name: jmx-exporter
    ports:
      - "5556:5556"
    volumes:
      - ./config/jmx_exporter_config.yml:/etc/jmx_exporter/config.yml:ro
    command:
      - "5556"
      - "/etc/jmx_exporter/config.yml"
    depends_on:
      - kafka
    networks:
      - kafka-network
```

**ì¬ì‹œì‘:**
```bash
docker-compose up -d jmx-exporter
docker-compose logs -f jmx-exporter
```

#### 3-4. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (20ë¶„)

**íŒŒì¼:** `src/kafka/performance_test.py`

```python
#!/usr/bin/env python3

import json
import time
from kafka import KafkaProducer, KafkaConsumer
from kafka.errors import KafkaError
import statistics

print("=" * 70)
print("KAFKA ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
print("=" * 70)

# í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ìƒì„±
test_messages = []
for i in range(10000):
    test_messages.append({
        'id': f'test_{i:06d}',
        'click': i % 100 < 16,  # 16% CTR
        'hour': 140102,
        'device_type': i % 10,
        'timestamp': int(time.time() * 1000)
    })

# 1ï¸âƒ£  Producer ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
print("\n1ï¸âƒ£  Producer ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (10,000 messages)...")

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8'),
    acks='all',
    retries=3,
    batch_size=16384,
    linger_ms=10
)

latencies = []
start_time = time.time()

for i, msg in enumerate(test_messages):
    try:
        future = producer.send('ad_events_raw', value=msg)
        record_metadata = future.get(timeout=10)

        latency = (time.time() - start_time) * 1000 / (i + 1)
        latencies.append(latency)

        if (i + 1) % 1000 == 0:
            print(f"   ğŸ“¤ {i + 1:,} messages sent")
    except KafkaError as e:
        print(f"   âŒ Error: {e}")

producer.flush()
producer.close()

elapsed = time.time() - start_time
throughput = len(test_messages) / elapsed

print(f"\n   ê²°ê³¼:")
print(f"   - ì´ ë©”ì‹œì§€: {len(test_messages):,}")
print(f"   - ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
print(f"   - ì²˜ë¦¬ëŸ‰: {throughput:.0f} msg/sec")
print(f"   - í‰ê·  ë ˆì´í„´ì‹œ: {statistics.mean(latencies):.2f}ms")
print(f"   - ìµœëŒ€ ë ˆì´í„´ì‹œ: {max(latencies):.2f}ms")
print(f"   - P99 ë ˆì´í„´ì‹œ: {sorted(latencies)[int(len(latencies)*0.99)]:.2f}ms")

# 2ï¸âƒ£  Consumer ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
print("\n2ï¸âƒ£  Consumer ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")

consumer = KafkaConsumer(
    'ad_events_raw',
    bootstrap_servers=['localhost:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8')),
    auto_offset_reset='earliest',
    consumer_timeout_ms=30000,
    fetch_max_bytes=52428800,
    max_poll_records=500
)

messages_received = 0
start_time = time.time()

for message in consumer:
    messages_received += 1
    if messages_received % 1000 == 0:
        print(f"   ğŸ“¥ {messages_received:,} messages received")

elapsed_consume = time.time() - start_time
throughput_consume = messages_received / elapsed_consume

print(f"\n   ê²°ê³¼:")
print(f"   - ìˆ˜ì‹  ë©”ì‹œì§€: {messages_received:,}")
print(f"   - ì†Œìš” ì‹œê°„: {elapsed_consume:.2f}ì´ˆ")
print(f"   - ì²˜ë¦¬ëŸ‰: {throughput_consume:.0f} msg/sec")

consumer.close()

# 3ï¸âƒ£  ìµœì¢… ê²°ê³¼
print("\n" + "=" * 70)
print("ì„±ëŠ¥ ìš”ì•½")
print("=" * 70)
print(f"Producer ì²˜ë¦¬ëŸ‰:  {throughput:>10.0f} msg/sec")
print(f"Consumer ì²˜ë¦¬ëŸ‰:  {throughput_consume:>10.0f} msg/sec")
print(f"P99 ë ˆì´í„´ì‹œ:     {sorted(latencies)[int(len(latencies)*0.99)]:>10.2f} ms")
print(f"ëª©í‘œ ë‹¬ì„±:        {'âœ…' if throughput > 50000 else 'âš ï¸'}")
print("=" * 70)
```

**ì‹¤í–‰:**
```bash
python src/kafka/performance_test.py
```

### âœ… ì™„ë£Œ ê¸°ì¤€

- [ ] 3ê°œ Topic ìƒì„± ì™„ë£Œ (`ad_events_raw`, `ad_events_error`, `ad_events_retry`)
- [ ] Topic ì„¤ì • íŒŒì¼ ì‘ì„± ë° ì ìš©
- [ ] JMX Exporter ì‹¤í–‰ ì¤‘
- [ ] Performance í…ŒìŠ¤íŠ¸ > 50,000 msg/sec
- [ ] P99 ë ˆì´í„´ì‹œ < 1,000ms

### ğŸ“Š ì‚°ì¶œë¬¼

```
scripts/
â””â”€â”€ create_topics.sh
config/
â”œâ”€â”€ jmx_exporter_config.yml
â””â”€â”€ kafka_topics_config.json
src/kafka/
â””â”€â”€ performance_test.py
```

---

## ğŸ“Œ Day 4 (ëª©): Python Kafka Producer ê°œë°œ (2ì‹œê°„)

### ëª©í‘œ
- Avazu CSV ë°ì´í„°ë¥¼ Kafkaë¡œ ì „ì†¡í•˜ëŠ” ê³ ì„±ëŠ¥ Producer ì‘ì„±
- ì—ëŸ¬ ì²˜ë¦¬ ë° DLQ ë¼ìš°íŒ…
- ë°°ì¹˜ ì²˜ë¦¬ ë° í”„ë¡œì„¸ì‹± ìµœì í™”
- í”„ë¡œë•ì…˜ ë ˆë”” ì½”ë“œ

### ğŸ“‹ í• ë‹¹ ì‹œê°„
| ì‘ì—… | ì‹œê°„ |
|------|------|
| ë°ì´í„° ë³€í™˜ ë¡œì§ | 30ë¶„ |
| Producer êµ¬í˜„ | 50ë¶„ |
| ì—ëŸ¬ ì²˜ë¦¬ | 20ë¶„ |
| í…ŒìŠ¤íŠ¸ | 20ë¶„ |

### ğŸ› ï¸ ì‹¤ìŠµ ë‚´ìš©

#### 4-1. ë°ì´í„° ë³€í™˜ ëª¨ë“ˆ (20ë¶„)

**íŒŒì¼:** `src/kafka/data_transformer.py`

```python
"""
Avazu CSV ë°ì´í„°ë¥¼ JSON ì´ë²¤íŠ¸ë¡œ ë³€í™˜
"""

import logging
from typing import Dict, Any, Optional
from datetime import datetime

logger = logging.getLogger(__name__)

class AvazuDataTransformer:
    """Avazu ë°ì´í„° ë³€í™˜ê¸°"""

    # ì»¬ëŸ¼ íƒ€ì… ì •ì˜
    INT_COLUMNS = {
        'click', 'hour', 'C1', 'banner_pos', 'device_type',
        'device_conn_type', 'C14', 'C15', 'C16', 'C17',
        'C18', 'C19', 'C20', 'C21'
    }

    STRING_COLUMNS = {
        'id', 'site_id', 'site_domain', 'site_category',
        'app_id', 'app_domain', 'app_category',
        'device_id', 'device_ip', 'device_model'
    }

    def __init__(self):
        self.processed_count = 0
        self.error_count = 0
        self.transformation_errors = []

    @staticmethod
    def parse_csv_line(header: list, values: list) -> Dict[str, Any]:
        """
        CSV ë¼ì¸ì„ íŒŒì‹±í•˜ì—¬ Python dictë¡œ ë³€í™˜

        Args:
            header: CSV í—¤ë” ë¦¬ìŠ¤íŠ¸
            values: CSV ê°’ ë¦¬ìŠ¤íŠ¸

        Returns:
            íŒŒì‹±ëœ dict, ë˜ëŠ” None if error
        """
        if len(header) != len(values):
            logger.error(f"Header/values mismatch: {len(header)} vs {len(values)}")
            return None

        try:
            result = {}

            for col, val in zip(header, values):
                if not val or val == '':
                    # ë¹ˆ ê°’ì€ nullë¡œ
                    result[col] = None
                elif col in AvazuDataTransformer.INT_COLUMNS:
                    try:
                        result[col] = int(val)
                    except ValueError:
                        logger.warning(f"Cannot convert {col}={val} to int, setting to None")
                        result[col] = None
                else:
                    # ë¬¸ìì—´ë¡œ ì²˜ë¦¬
                    result[col] = str(val)

            return result

        except Exception as e:
            logger.error(f"Parse error: {e}")
            return None

    def transform(self, csv_line: str, line_number: int) -> Optional[Dict[str, Any]]:
        """
        CSV ë¼ì¸ì„ Kafka ì´ë²¤íŠ¸ë¡œ ë³€í™˜

        Args:
            csv_line: CSV ë¼ì¸ ë¬¸ìì—´
            line_number: ë¼ì¸ ë²ˆí˜¸ (ì²« ì¤„ì€ 0)

        Returns:
            ë³€í™˜ëœ ì´ë²¤íŠ¸ dict, ë˜ëŠ” None if error
        """
        try:
            values = csv_line.strip().split(',')

            # í—¤ë”ëŠ” ì²« ë²ˆì§¸ ë¼ì¸ (line_number == 0)
            if line_number == 0:
                self.header = values
                return None

            # ë°ì´í„° ë³€í™˜
            event = self.parse_csv_line(self.header, values)

            if event is None:
                self.error_count += 1
                return None

            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            event['_source'] = 'avazu'
            event['_processed_at'] = datetime.utcnow().isoformat()
            event['_line_number'] = line_number

            self.processed_count += 1
            return event

        except Exception as e:
            logger.error(f"Transformation error at line {line_number}: {e}")
            self.error_count += 1
            self.transformation_errors.append({
                'line_number': line_number,
                'error': str(e)
            })
            return None

    def get_statistics(self) -> Dict[str, Any]:
        """ë³€í™˜ í†µê³„"""
        return {
            'processed': self.processed_count,
            'errors': self.error_count,
            'success_rate': (
                self.processed_count / (self.processed_count + self.error_count) * 100
                if (self.processed_count + self.error_count) > 0 else 0
            )
        }


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    transformer = AvazuDataTransformer()

    # í—¤ë” ì²˜ë¦¬
    header = "id,click,hour,C1,banner_pos,site_id,site_category,device_type"
    transformer.transform(header, 0)

    # ë°ì´í„° ì²˜ë¦¬
    data = "1000009418151094273,0,14102100,1005,0,1fbe01fe,28905ebd,1"
    event = transformer.transform(data, 1)

    print("Transformed event:")
    import json
    print(json.dumps(event, indent=2))
    print(f"\nStatistics: {transformer.get_statistics()}")
```

#### 4-2. Kafka Producer êµ¬í˜„ (40ë¶„)

**íŒŒì¼:** `src/kafka/producer.py`

```python
"""
Avazu ë°ì´í„°ë¥¼ Kafkaë¡œ ë°œí–‰í•˜ëŠ” Producer
"""

import gzip
import json
import time
import logging
from typing import Optional, Callable
from kafka import KafkaProducer
from kafka.errors import KafkaError

from data_transformer import AvazuDataTransformer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class AvazuKafkaProducer:
    """Avazu ë°ì´í„°ë¥¼ Kafkaë¡œ ë°œí–‰í•˜ëŠ” Producer"""

    def __init__(
        self,
        bootstrap_servers: str = 'localhost:9092',
        topic_success: str = 'ad_events_raw',
        topic_error: str = 'ad_events_error',
        batch_size: int = 100,
        linger_ms: int = 10
    ):
        """
        Args:
            bootstrap_servers: Kafka bootstrap servers
            topic_success: ì„±ê³µí•œ ë©”ì‹œì§€ í† í”½
            topic_error: ì‹¤íŒ¨í•œ ë©”ì‹œì§€ í† í”½
            batch_size: ë°°ì¹˜ í¬ê¸°
            linger_ms: ë°°ì¹˜ ëŒ€ê¸° ì‹œê°„ (ms)
        """
        self.bootstrap_servers = bootstrap_servers
        self.topic_success = topic_success
        self.topic_error = topic_error
        self.batch_size = batch_size
        self.linger_ms = linger_ms

        # í†µê³„
        self.stats = {
            'sent': 0,
            'errors': 0,
            'dlq': 0,
            'total_time': 0,
            'start_time': None,
            'end_time': None
        }

        # Producer ì´ˆê¸°í™”
        self.producer = self._init_producer()
        self.transformer = AvazuDataTransformer()

    def _init_producer(self) -> KafkaProducer:
        """Kafka Producer ì´ˆê¸°í™”"""
        try:
            producer = KafkaProducer(
                bootstrap_servers=self.bootstrap_servers,
                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                acks='all',  # ëª¨ë“  replicasì˜ í™•ì¸ ëŒ€ê¸°
                retries=3,  # ì¬ì‹œë„ íšŸìˆ˜
                batch_size=self.batch_size,
                linger_ms=self.linger_ms,
                buffer_memory=33554432,  # 32MB
                max_in_flight_requests_per_connection=5,
                compression_type='snappy'
            )
            logger.info("âœ… Kafka Producer initialized")
            return producer
        except Exception as e:
            logger.error(f"âŒ Failed to initialize producer: {e}")
            raise

    def _on_send_success(self, record_metadata):
        """ë©”ì‹œì§€ ë°œí–‰ ì„±ê³µ ì½œë°±"""
        self.stats['sent'] += 1
        if self.stats['sent'] % 1000 == 0:
            logger.info(f"âœ… {self.stats['sent']:,} messages sent")

    def _on_send_error(self, exc):
        """ë©”ì‹œì§€ ë°œí–‰ ì‹¤íŒ¨ ì½œë°±"""
        self.stats['errors'] += 1
        logger.error(f"âŒ Send error: {exc}")

    def produce_from_gz(
        self,
        gz_file_path: str,
        max_records: Optional[int] = None,
        progress_callback: Optional[Callable] = None
    ) -> dict:
        """
        Gzip íŒŒì¼ì—ì„œ ë©”ì‹œì§€ë¥¼ ì½ì–´ Kafkaë¡œ ë°œí–‰

        Args:
            gz_file_path: Gzip íŒŒì¼ ê²½ë¡œ
            max_records: ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜ (None: ì „ì²´)
            progress_callback: ì§„í–‰ë„ ì½œë°± í•¨ìˆ˜

        Returns:
            í†µê³„ dict
        """
        self.stats['start_time'] = time.time()

        try:
            with gzip.open(gz_file_path, 'rt', encoding='utf-8') as f:
                line_number = 0

                for line in f:
                    # ìµœëŒ€ ë ˆì½”ë“œ ì œí•œ
                    if max_records and line_number >= max_records + 1:  # +1 for header
                        break

                    # ë°ì´í„° ë³€í™˜
                    event = self.transformer.transform(line, line_number)

                    # í—¤ë”ëŠ” ë°œí–‰í•˜ì§€ ì•ŠìŒ
                    if line_number == 0:
                        line_number += 1
                        continue

                    # ë³€í™˜ ì‹¤íŒ¨ì‹œ DLQë¡œ ì „ì†¡
                    if event is None:
                        try:
                            error_event = {
                                'line_number': line_number,
                                'raw_data': line.strip(),
                                'error': 'Transformation failed',
                                'timestamp': int(time.time() * 1000)
                            }
                            self.producer.send(
                                self.topic_error,
                                value=error_event
                            ).get(timeout=10)
                            self.stats['dlq'] += 1
                        except Exception as e:
                            logger.error(f"Failed to send to DLQ: {e}")

                        line_number += 1
                        continue

                    # ì„±ê³µ í† í”½ìœ¼ë¡œ ë°œí–‰
                    try:
                        self.producer.send(
                            self.topic_success,
                            value=event
                        ).add_callback(self._on_send_success) \
                         .add_errback(self._on_send_error)

                    except Exception as e:
                        logger.error(f"Failed to send message: {e}")
                        self.stats['errors'] += 1

                    # ì§„í–‰ë„ ì½œë°±
                    if progress_callback:
                        progress_callback(line_number)

                    line_number += 1

        except Exception as e:
            logger.error(f"âŒ Error reading file: {e}")
            raise

        finally:
            # Flush ë° ì •ë¦¬
            self.producer.flush(timeout=30)
            self.stats['end_time'] = time.time()
            self.stats['total_time'] = self.stats['end_time'] - self.stats['start_time']

        return self.get_statistics()

    def get_statistics(self) -> dict:
        """í†µê³„ ë°˜í™˜"""
        stats = self.stats.copy()
        stats['transformer'] = self.transformer.get_statistics()

        if stats['total_time'] > 0:
            stats['throughput_msg_per_sec'] = (
                (stats['sent'] + stats['dlq']) / stats['total_time']
            )

        return stats

    def close(self):
        """Producer ì¢…ë£Œ"""
        self.producer.close()
        logger.info("âœ… Producer closed")


def print_statistics(stats: dict):
    """í†µê³„ ì¶œë ¥"""
    print("\n" + "=" * 70)
    print("PRODUCTION STATISTICS")
    print("=" * 70)
    print(f"Messages sent:          {stats['sent']:>15,}")
    print(f"DLQ (errors):           {stats['dlq']:>15,}")
    print(f"Send errors:            {stats['errors']:>15,}")
    print(f"Total time:             {stats['total_time']:>15.2f} sec")
    print(f"Throughput:             {stats.get('throughput_msg_per_sec', 0):>15.0f} msg/sec")
    print(f"Transformer success:    {stats['transformer']['success_rate']:>14.2f}%")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    import sys

    # íŒŒë¼ë¯¸í„°
    gz_file = '../../data/train.gz'
    max_records = 10000  # í…ŒìŠ¤íŠ¸ìš© 10,000ê±´

    if len(sys.argv) > 1:
        max_records = int(sys.argv[1])

    # Producer ì‹¤í–‰
    producer = AvazuKafkaProducer()

    try:
        logger.info(f"Starting production from {gz_file} (max {max_records:,} records)")

        stats = producer.produce_from_gz(
            gz_file,
            max_records=max_records,
            progress_callback=lambda line: (
                print(f"Processing line {line:,}...")
                if line % 2000 == 0 else None
            )
        )

        print_statistics(stats)
        logger.info("âœ… Production completed successfully")

    except KeyboardInterrupt:
        logger.info("âš ï¸  Production interrupted by user")
        print_statistics(producer.get_statistics())

    except Exception as e:
        logger.error(f"âŒ Production failed: {e}")
        print_statistics(producer.get_statistics())

    finally:
        producer.close()
```

#### 4-3. ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ (15ë¶„)

**íŒŒì¼:** `src/kafka/error_handler.py`

```python
"""
DLQ ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§
"""

import json
import logging
from kafka import KafkaConsumer, KafkaProducer
from kafka.errors import KafkaError

logger = logging.getLogger(__name__)


class DLQHandler:
    """Dead Letter Queue ì²˜ë¦¬"""

    def __init__(
        self,
        bootstrap_servers: str = 'localhost:9092',
        dlq_topic: str = 'ad_events_error',
        retry_topic: str = 'ad_events_retry'
    ):
        self.bootstrap_servers = bootstrap_servers
        self.dlq_topic = dlq_topic
        self.retry_topic = retry_topic

        self.consumer = KafkaConsumer(
            dlq_topic,
            bootstrap_servers=bootstrap_servers,
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            auto_offset_reset='earliest',
            consumer_timeout_ms=10000
        )

        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )

        self.stats = {
            'processed': 0,
            'retried': 0,
            'skipped': 0
        }

    def process_dlq(self):
        """DLQ ë©”ì‹œì§€ ì²˜ë¦¬"""
        logger.info(f"Processing DLQ topic: {self.dlq_topic}")

        for message in self.consumer:
            try:
                error_event = message.value
                line_number = error_event.get('line_number')
                error_reason = error_event.get('error', 'Unknown')

                logger.warning(f"DLQ message at line {line_number}: {error_reason}")

                # ì¬ì‹œë„ ì—¬ë¶€ íŒë‹¨
                if self._should_retry(error_event):
                    # Retry í† í”½ìœ¼ë¡œ ì „ì†¡
                    self.producer.send(
                        self.retry_topic,
                        value=error_event
                    )
                    self.stats['retried'] += 1
                    logger.info(f"Message sent to retry topic: {line_number}")
                else:
                    logger.error(f"Skipping message: {line_number}")
                    self.stats['skipped'] += 1

                self.stats['processed'] += 1

            except Exception as e:
                logger.error(f"Error processing DLQ message: {e}")

        self.producer.flush()
        logger.info(f"DLQ processing completed: {self.stats}")

    def _should_retry(self, error_event: dict) -> bool:
        """ì¬ì‹œë„ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨"""
        retry_count = error_event.get('retry_count', 0)
        max_retries = 3

        # íƒ€ì„ì•„ì›ƒ, ì¼ì‹œì  ì˜¤ë¥˜ëŠ” ì¬ì‹œë„
        error_reason = error_event.get('error', '')
        retriable_errors = [
            'Transformation failed',
            'Timeout',
            'Connection error'
        ]

        return (
            retry_count < max_retries and
            any(err in error_reason for err in retriable_errors)
        )

    def close(self):
        self.consumer.close()
        self.producer.close()


if __name__ == "__main__":
    handler = DLQHandler()
    handler.process_dlq()
    handler.close()
```

#### 4-4. í†µí•© í…ŒìŠ¤íŠ¸ (20ë¶„)

**íŒŒì¼:** `src/kafka/test_producer.py`

```python
"""
Producer í†µí•© í…ŒìŠ¤íŠ¸
"""

import subprocess
import time
import logging
from producer import AvazuKafkaProducer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_producer():
    """Producer í…ŒìŠ¤íŠ¸"""

    print("\n" + "=" * 70)
    print("KAFKA PRODUCER TEST")
    print("=" * 70)

    # 1ï¸âƒ£  Small datasetìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    print("\n1ï¸âƒ£  í…ŒìŠ¤íŠ¸: 1,000ê±´ ë©”ì‹œì§€ ë°œí–‰...")
    producer = AvazuKafkaProducer()

    try:
        stats = producer.produce_from_gz(
            '../../data/train.gz',
            max_records=1000
        )

        assert stats['sent'] > 900, f"Expected > 900 messages, got {stats['sent']}"
        assert stats['dlq'] < 100, f"Expected < 100 DLQ, got {stats['dlq']}"
        print("   âœ… Test passed")

    except AssertionError as e:
        print(f"   âŒ Test failed: {e}")
        return False

    finally:
        producer.close()

    # 2ï¸âƒ£  Producer ì¬ì‹œì‘ í›„ ë©”ì‹œì§€ ìˆ˜ì‹  í™•ì¸
    print("\n2ï¸âƒ£  ë©”ì‹œì§€ ìˆ˜ì‹  í™•ì¸...")

    subprocess.run([
        'bash', '-c',
        'docker-compose exec kafka kafka-console-consumer '
        '--bootstrap-server kafka:29092 '
        '--topic ad_events_raw '
        '--from-beginning '
        '--max-messages 1 | head -1'
    ])

    print("   âœ… Message received")

    # 3ï¸âƒ£  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    print("\n3ï¸âƒ£  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸: 10,000ê±´...")

    producer = AvazuKafkaProducer()
    try:
        stats = producer.produce_from_gz(
            '../../data/train.gz',
            max_records=10000
        )

        throughput = stats.get('throughput_msg_per_sec', 0)
        assert throughput > 1000, f"Throughput too low: {throughput}"

        print(f"   âœ… Throughput: {throughput:.0f} msg/sec")

    finally:
        producer.close()

    print("\n" + "=" * 70)
    print("âœ… ALL TESTS PASSED")
    print("=" * 70)

    return True


if __name__ == "__main__":
    test_producer()
```

**ì‹¤í–‰:**
```bash
cd src/kafka
python producer.py 10000
```

### âœ… ì™„ë£Œ ê¸°ì¤€

- [ ] `data_transformer.py` ì‘ë™ í™•ì¸ (CSV â†’ JSON ë³€í™˜)
- [ ] `producer.py` ì‹¤í–‰ ì™„ë£Œ (10,000ê±´ ì´ìƒ ë°œí–‰)
- [ ] ë°œí–‰ ì²˜ë¦¬ëŸ‰ > 1,000 msg/sec
- [ ] Kafka Topicì—ì„œ ë©”ì‹œì§€ ìˆ˜ì‹  í™•ì¸
- [ ] DLQë¡œ ì—ëŸ¬ ë©”ì‹œì§€ ì „ë‹¬
- [ ] ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼

### ğŸ“Š ì‚°ì¶œë¬¼

```
src/kafka/
â”œâ”€â”€ data_transformer.py
â”œâ”€â”€ producer.py
â”œâ”€â”€ error_handler.py
â””â”€â”€ test_producer.py
```

---

## ğŸ“Œ Day 5 (ê¸ˆ): ëª¨ë‹ˆí„°ë§ & í†µí•© í…ŒìŠ¤íŠ¸ (2ì‹œê°„)

### ëª©í‘œ
- ì „ì²´ íŒŒì´í”„ë¼ì¸ ì—”ë“œ-íˆ¬-ì—”ë“œ í…ŒìŠ¤íŠ¸
- Prometheus/JMX ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í™•ì¸
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
- Week 1 ë§ˆë¬´ë¦¬ ë° Week 2 ì¤€ë¹„

### ğŸ“‹ í• ë‹¹ ì‹œê°„
| ì‘ì—… | ì‹œê°„ |
|------|------|
| Prometheus ì„¤ì • | 30ë¶„ |
| ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê²€ì¦ | 30ë¶„ |
| E2E í…ŒìŠ¤íŠ¸ | 40ë¶„ |
| ë¬¸ì„œí™” | 20ë¶„ |

### ğŸ› ï¸ ì‹¤ìŠµ ë‚´ìš©

#### 5-1. Prometheus ì„¤ì • (20ë¶„)

**íŒŒì¼:** `config/prometheus.yml`

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    monitor: 'kafka-monitoring'

alerting:
  alertmanagers:
    - static_configs:
        - targets: []

rule_files: []

scrape_configs:
  # Kafka JMX Metrics
  - job_name: 'kafka'
    static_configs:
      - targets: ['localhost:5556']
        labels:
          instance: 'kafka-broker-1'

  # Prometheus self-monitoring
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
```

**Docker Composeì— Prometheus ì¶”ê°€:**

```yaml
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    depends_on:
      - jmx-exporter
    networks:
      - kafka-network

volumes:
  prometheus_data:
```

**ì‹¤í–‰:**
```bash
docker-compose up -d prometheus
docker-compose logs -f prometheus
```

**ì ‘ì†:** http://localhost:9090

#### 5-2. ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê²€ì¦ (30ë¶„)

**íŒŒì¼:** `src/monitoring/verify_metrics.py`

```python
"""
Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê²€ì¦
"""

import requests
import json
import logging
from typing import List, Dict

logger = logging.getLogger(__name__)


class PrometheusClient:
    """Prometheus í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, url: str = 'http://localhost:9090'):
        self.url = url

    def query(self, query: str) -> Dict:
        """PromQL ì¿¼ë¦¬ ì‹¤í–‰"""
        try:
            response = requests.get(
                f'{self.url}/api/v1/query',
                params={'query': query},
                timeout=10
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"Query failed: {e}")
            return {'status': 'error'}

    def verify_kafka_metrics(self) -> bool:
        """Kafka ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í™•ì¸"""

        print("\n" + "=" * 70)
        print("PROMETHEUS METRICS VERIFICATION")
        print("=" * 70)

        # 1ï¸âƒ£  Kafka metrics í™•ì¸
        print("\n1ï¸âƒ£  Kafka Broker Metrics...")
        metrics = [
            ('kafka_server_replica_manager_isr_shrinks_total', 'ë³€ê²½ëœ ISR'),
            ('kafka_server_broker_topic_metrics_messages_in_total', 'ë©”ì‹œì§€ ìˆ˜ì‹ '),
            ('kafka_server_broker_topic_metrics_bytes_in_total', 'ìˆ˜ì‹  ë°”ì´íŠ¸'),
        ]

        for metric_name, description in metrics:
            result = self.query(metric_name)
            status = 'OK' if result['status'] == 'success' else 'MISSING'
            print(f"   {status}: {metric_name} ({description})")

        # 2ï¸âƒ£  Topicë³„ ë©”íŠ¸ë¦­
        print("\n2ï¸âƒ£  Topic Metrics...")
        result = self.query(
            'kafka_server_broker_topic_metrics_messages_in_total{topic="ad_events_raw"}'
        )

        if result['status'] == 'success' and result['data']['result']:
            value = result['data']['result'][0]['value'][1]
            print(f"   âœ… ad_events_raw: {value} messages")
        else:
            print(f"   âš ï¸  ad_events_raw: No data yet")

        # 3ï¸âƒ£  ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
        print("\n3ï¸âƒ£  System Metrics...")
        result = self.query('kafka_server_request_handler_avg_idle_percent')

        if result['status'] == 'success' and result['data']['result']:
            value = result['data']['result'][0]['value'][1]
            print(f"   âœ… Request handler idle: {value}%")

        print("\n" + "=" * 70)
        return True


def verify_all():
    """ì „ì²´ ê²€ì¦"""

    print("\nPrometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í™•ì¸...")

    client = PrometheusClient()

    # Prometheus ì—°ê²° í™•ì¸
    try:
        response = requests.get('http://localhost:9090/-/healthy', timeout=5)
        if response.status_code == 200:
            print("âœ… Prometheus is healthy")
        else:
            print("âŒ Prometheus health check failed")
            return False
    except Exception as e:
        print(f"âŒ Cannot connect to Prometheus: {e}")
        return False

    # ë©”íŠ¸ë¦­ ê²€ì¦
    return client.verify_kafka_metrics()


if __name__ == "__main__":
    verify_all()
```

#### 5-3. E2E í†µí•© í…ŒìŠ¤íŠ¸ (30ë¶„)

**íŒŒì¼:** `scripts/e2e_test.sh`

```bash
#!/bin/bash

set -e

echo "=========================================="
echo "E2E TEST: Week 1 Pipeline Validation"
echo "=========================================="

# 1ï¸âƒ£  Docker ì„œë¹„ìŠ¤ í™•ì¸
echo ""
echo "1ï¸âƒ£  Checking Docker services..."
docker-compose ps

# 2ï¸âƒ£  Topics í™•ì¸
echo ""
echo "2ï¸âƒ£  Checking Kafka Topics..."
docker-compose exec kafka kafka-topics \
    --list \
    --bootstrap-server kafka:29092

# 3ï¸âƒ£  Schema Registry í™•ì¸
echo ""
echo "3ï¸âƒ£  Checking Schema Registry..."
curl -s http://localhost:8081/subjects | jq .

# 4ï¸âƒ£  Producer ì‹¤í–‰
echo ""
echo "4ï¸âƒ£  Running Producer (5,000 messages)..."
cd src/kafka
python producer.py 5000
cd ../../

# 5ï¸âƒ£  ë©”ì‹œì§€ ìˆ˜ì‹  í™•ì¸
echo ""
echo "5ï¸âƒ£  Verifying messages received..."
MESSAGE_COUNT=$(docker-compose exec kafka kafka-console-consumer \
    --bootstrap-server kafka:29092 \
    --topic ad_events_raw \
    --from-beginning \
    --timeout-ms 5000 \
    2>/dev/null | wc -l)

echo "Messages received: $MESSAGE_COUNT"

if [ $MESSAGE_COUNT -gt 4000 ]; then
    echo "âœ… E2E test passed!"
else
    echo "âŒ E2E test failed! Expected > 4000, got $MESSAGE_COUNT"
    exit 1
fi

# 6ï¸âƒ£  Prometheus ë©”íŠ¸ë¦­ í™•ì¸
echo ""
echo "6ï¸âƒ£  Checking Prometheus metrics..."
python src/monitoring/verify_metrics.py

echo ""
echo "=========================================="
echo "âœ… WEEK 1 VALIDATION COMPLETE"
echo "=========================================="
```

**ì‹¤í–‰:**
```bash
chmod +x scripts/e2e_test.sh
bash scripts/e2e_test.sh
```

#### 5-4. ìµœì¢… ë³´ê³ ì„œ (20ë¶„)

**íŒŒì¼:** `docs/week1_summary.md`

```markdown
# Week 1 ì™„ë£Œ ë³´ê³ ì„œ: ë°ì´í„° ìˆ˜ì§‘ & ìŠ¤íŠ¸ë¦¬ë° ê¸°ì´ˆ

## ğŸ“‹ ê°œìš”

**ê¸°ê°„:** 5ì¼ (ì›”~ê¸ˆ)
**ëª©í‘œ:** Kafka í´ëŸ¬ìŠ¤í„° êµ¬ì¶• ë° Avazu ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì™„ì„±
**ìƒíƒœ:** âœ… ì™„ë£Œ

## ğŸ¯ ì£¼ìš” ì„±ê³¼

### 1. Avazu ë°ì´í„° ë¶„ì„ ì™„ë£Œ
- ë°ì´í„° í¬ê¸°: 7.82 GB (40M í–‰)
- ì»¬ëŸ¼ ìˆ˜: 24ê°œ
- í´ë¦­ë¥ : 16.6%
- 3ê°œì˜ ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„± (1K, 10K, 50K)

### 2. Kafka + Schema Registry êµ¬ì¶•
- Zookeeper + Kafka + Schema Registry ì •ìƒ ì‘ë™
- 3ê°œ Topic ìƒì„± (ad_events_raw, ad_events_error, ad_events_retry)
- Avro Schema ë“±ë¡ ë° ê²€ì¦

### 3. Python Kafka Producer ê°œë°œ
- CSV â†’ JSON ë°ì´í„° ë³€í™˜
- ë°°ì¹˜ ì²˜ë¦¬ ë° ì••ì¶• (Snappy)
- ì—ëŸ¬ ì²˜ë¦¬ ë° DLQ ë¼ìš°íŒ…
- **ì²˜ë¦¬ëŸ‰:** 1,000+ msg/sec

### 4. ëª¨ë‹ˆí„°ë§ ê¸°ì´ˆ ì„¤ì •
- JMX Exporter ì—°ë™
- Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- ê¸°ë³¸ ëŒ€ì‹œë³´ë“œ êµ¬ì„±

## ğŸ“Š ìµœì¢… ë©”íŠ¸ë¦­

| í•­ëª© | ëª©í‘œ | ì‹¤ì œ | ìƒíƒœ |
|------|------|------|------|
| Producer ì²˜ë¦¬ëŸ‰ | > 1,000 msg/sec | 1,200+ | âœ… |
| Kafka ê°€ìš©ì„± | 99.9% | 100% | âœ… |
| Schema Registry ì‘ë‹µ | < 100ms | 50ms | âœ… |
| ë©”ì‹œì§€ ì†ì‹¤ë¥  | 0% | 0% | âœ… |
| ë°ì´í„° ë³€í™˜ ì„±ê³µë¥  | > 99% | 99.8% | âœ… |

## ğŸ“ ì‚°ì¶œë¬¼

### ì½”ë“œ
- `src/kafka/producer.py` - Kafka Producer
- `src/kafka/data_transformer.py` - ë°ì´í„° ë³€í™˜
- `src/kafka/error_handler.py` - DLQ ì²˜ë¦¬
- `src/analysis/eda_analysis.py` - EDA ìŠ¤í¬ë¦½íŠ¸

### ì„¤ì •
- `docker-compose.yml` - Docker ì„œë¹„ìŠ¤
- `schemas/ad_event.avsc` - Avro ìŠ¤í‚¤ë§ˆ
- `config/prometheus.yml` - Prometheus ì„¤ì •
- `config/jmx_exporter_config.yml` - JMX ì„¤ì •

### ë¬¸ì„œ
- `docs/eda_report.md` - EDA ë³´ê³ ì„œ
- `docs/week1_summary.md` - ì£¼ê°„ ë³´ê³ ì„œ
- `data/data_stats.json` - ë°ì´í„° í†µê³„

## ğŸ”„ ì „ì²´ ë°ì´í„° íë¦„

```
[Avazu train.gz]
       â†“
[Producer: CSV â†’ JSON]
       â†“
[Kafka Topic: ad_events_raw] â† [JMX Metrics]
       â†“                              â†“
  [âœ… Success]          [Prometheus] â†’ [Grafana (í–¥í›„)]
  [âŒ Error] â†’ [DLQ Topic: ad_events_error]
```

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [x] Avazu ë°ì´í„° ë¶„ì„ ì™„ë£Œ
- [x] Docker í™˜ê²½ êµ¬ì„±
- [x] Kafka + Schema Registry ì‹¤í–‰
- [x] 3ê°œ Topic ìƒì„±
- [x] Producer ê°œë°œ ì™„ë£Œ
- [x] 10,000+ ë©”ì‹œì§€ ë°œí–‰ ì„±ê³µ
- [x] JMX/Prometheus ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [x] E2E í…ŒìŠ¤íŠ¸ í†µê³¼

## ğŸš€ Week 2 ì¤€ë¹„

- [ ] PyFlink ê°œë°œ í™˜ê²½ ì„¤ì •
- [ ] PostgreSQL Docker ì´ë¯¸ì§€ ì¤€ë¹„
- [ ] Streamlit ê°œë°œ í™˜ê²½
- [ ] Redis Docker ì´ë¯¸ì§€ ì¤€ë¹„

## ğŸ“ ì£¼ìš” ë°°ìš´ ì 

1. **Kafka ì•„í‚¤í…ì²˜**: Partitioning, replication, leader election
2. **ìŠ¤í‚¤ë§ˆ ê´€ë¦¬**: Avro, Schema Registry, ë²„ì „ ê´€ë¦¬
3. **Data Transformation**: CSV íŒŒì‹±, íƒ€ì… ë³€í™˜, ì—ëŸ¬ ì²˜ë¦¬
4. **ê³ ì„±ëŠ¥ Producer**: Batching, compression, async callbacks

## âš ï¸ ë¬¸ì œ & í•´ê²°

### ë¬¸ì œ 1: ì´ˆê¸° Producer ì—°ê²° ì‹¤íŒ¨
**ì›ì¸:** Kafka ì´ˆê¸°í™” ì§€ì—°
**í•´ê²°:** docker-compose healthcheck ì¶”ê°€

### ë¬¸ì œ 2: ë°ì´í„° ì†ì‹¤
**ì›ì¸:** ë°°ì¹˜ ì²˜ë¦¬ ì „ Producer ì¢…ë£Œ
**í•´ê²°:** producer.flush() ì¶”ê°€

### ë¬¸ì œ 3: ë©”ëª¨ë¦¬ ë¶€ì¡±
**ì›ì¸:** ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
**í•´ê²°:** ìƒ˜í”Œ ë°ì´í„°ì…‹ ì‚¬ìš©

## ğŸ¯ ë‹¤ìŒ ì£¼ ëª©í‘œ

- PyFlinkë¡œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
- 1ë¶„/5ë¶„ Tumbling Window êµ¬í˜„
- Redis ìºì‹œ êµ¬ì¶•
- Streamlit ëŒ€ì‹œë³´ë“œ ê°œë°œ
- PostgreSQL ë°ì´í„° ì ì¬

---

**ì‘ì„± ì¼ì‹œ:** 2025-12-13
**ë‹´ë‹¹ì:** Engineering Team
**ìƒíƒœ:** âœ… ì™„ë£Œ & Week 2 ì¤€ë¹„ ì™„ë£Œ
```

### âœ… ì™„ë£Œ ê¸°ì¤€

- [ ] ëª¨ë“  Docker ì„œë¹„ìŠ¤ ì •ìƒ ì‹¤í–‰
- [ ] Kafka Topics ì •ìƒ ì‘ë™
- [ ] Producer 10,000+ ë©”ì‹œì§€ ë°œí–‰
- [ ] Prometheusì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í™•ì¸
- [ ] E2E í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] Week 1 ì™„ë£Œ ë³´ê³ ì„œ ì‘ì„±

### ğŸ“Š ìµœì¢… ì‚°ì¶œë¬¼

```
Week 1 ì™„ë£Œ
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â”œâ”€â”€ producer.py
â”‚   â”‚   â”œâ”€â”€ data_transformer.py
â”‚   â”‚   â”œâ”€â”€ error_handler.py
â”‚   â”‚   â””â”€â”€ test_producer.py
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ explore_data.py
â”‚   â”‚   â””â”€â”€ eda_analysis.py
â”‚   â””â”€â”€ monitoring/
â”‚       â””â”€â”€ verify_metrics.py
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ ad_event.avsc
â”‚   â””â”€â”€ register_schema.json
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â””â”€â”€ jmx_exporter_config.yml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create_topics.sh
â”‚   â””â”€â”€ e2e_test.sh
â””â”€â”€ docs/
    â”œâ”€â”€ eda_report.md
    â””â”€â”€ week1_summary.md
```

---

## ğŸ“ Week 1 í•µì‹¬ í•™ìŠµ ë‚´ìš©

### ê¸°ìˆ 
âœ… Kafka ì•„í‚¤í…ì²˜ ë° ì„¤ì •
âœ… Schema Registry ë° Avro
âœ… Python ê³ ì„±ëŠ¥ Producer
âœ… ëª¨ë‹ˆí„°ë§ ë° ë©”íŠ¸ë¦­

### ìŠ¤í‚¬
âœ… Docker Compose
âœ… Bash ìŠ¤í¬ë¦½íŒ…
âœ… ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì„¤ê³„
âœ… ì„±ëŠ¥ ìµœì í™”

### ì‹œìŠ¤í…œ ì„¤ê³„
âœ… ë©”ì‹œì§€ ë¸Œë¡œì»¤ ì´í•´
âœ… ìŠ¤ì¼€ì¼ì„± ê³ ë ¤
âœ… ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬
âœ… ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

---

**Week 1 ì™„ë£Œ!** ğŸ‰
**ë‹¤ìŒ ì£¼ ëª©í‘œ:** ì‹¤ì‹œê°„ ì²˜ë¦¬ (Flink) & ìºì‹± (Redis)
