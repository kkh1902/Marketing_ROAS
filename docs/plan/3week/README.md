# Week 3: ë°°ì¹˜ ì²˜ë¦¬ & ëª¨ë‹ˆí„°ë§ & ë¶„ì„ ëŒ€ì‹œë³´ë“œ

**ëª©í‘œ:** Airflow ë°°ì¹˜ ìŠ¤ì¼€ì¤„ë§, dbt ë°ì´í„° ë³€í™˜, Streamlit ë¶„ì„ ëŒ€ì‹œë³´ë“œ, Grafana ëª¨ë‹ˆí„°ë§ì„ êµ¬ì¶•í•˜ì—¬ ì™„ì „í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì™„ì„±

**ê¸°ê°„:** 5ì¼ (ì›”~ê¸ˆ)
**ì¼ì¼ ë¶„ëŸ‰:** 2ì‹œê°„
**ì´ ì‹œê°„:** 10ì‹œê°„

---

## ğŸ“… ì£¼ê°„ ì¼ì •í‘œ

| ë‹¨ê³„ | ì£¼ì œ | ì‹œê°„ | ëˆ„ì  |
|------|------|------|------|
| **ì›”** | PostgreSQL + ë°°ì¹˜ DB ì„¤ê³„ | 2h | 2h |
| **í™”** | Airflow DAG ë° ìŠ¤ì¼€ì¤„ë§ | 2h | 4h |
| **ìˆ˜** | dbt ëª¨ë¸ ê°œë°œ ë° í…ŒìŠ¤íŠ¸ | 2h | 6h |
| **ëª©** | Streamlit ëŒ€ì‹œë³´ë“œ ê°œë°œ | 2h | 8h |
| **ê¸ˆ** | Grafana ëª¨ë‹ˆí„°ë§ + ìµœì¢… í†µí•© | 2h | 10h |

---

## ğŸ“Œ Day 1 (ì›”): PostgreSQL + ë°°ì¹˜ DB ì„¤ê³„ (2ì‹œê°„)

### ëª©í‘œ
- PostgreSQL Docker í™˜ê²½ êµ¬ì¶•
- ë°°ì¹˜ ì²˜ë¦¬ìš© ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ê³„
- ì‹¤ì‹œê°„/ë°°ì¹˜ ë°ì´í„° ì €ì¥ í…Œì´ë¸” ìƒì„±
- ì„±ëŠ¥ ìµœì í™” (ì¸ë±ìŠ¤, íŒŒí‹°ì…”ë‹)

### ğŸ“‹ í• ë‹¹ ì‹œê°„
| ì‘ì—… | ì‹œê°„ |
|------|------|
| PostgreSQL Docker êµ¬ì„± | 30ë¶„ |
| ìŠ¤í‚¤ë§ˆ ì„¤ê³„ | 40ë¶„ |
| DDL ì‘ì„± ë° ì‹¤í–‰ | 40ë¶„ |
| ê²€ì¦ ë° ë¬¸ì„œí™” | 10ë¶„ |

### ğŸ› ï¸ ì‹¤ìŠµ ë‚´ìš©

#### 1-1. PostgreSQL Docker ì„¤ì • (15ë¶„)

**íŒŒì¼:** `docker-compose.yml` (ì—…ë°ì´íŠ¸)

```yaml
  postgres:
    image: postgres:15-alpine
    container_name: postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: marketing_roas
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_postgres.sql:/docker-entrypoint-initdb.d/init.sql:ro
    ports:
      - "5432:5432"
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  postgres_data:
```

**ì‹¤í–‰:**
```bash
docker-compose up -d postgres
docker-compose logs -f postgres
```

#### 1-2. ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ê³„ (25ë¶„)

**íŒŒì¼:** `scripts/init_postgres.sql`

```sql
-- ============================================================
-- DATABASE & SCHEMA CREATION
-- ============================================================

-- Realtime ìŠ¤í‚¤ë§ˆ (ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­)
CREATE SCHEMA IF NOT EXISTS realtime;

-- Analytics ìŠ¤í‚¤ë§ˆ (ë¶„ì„ìš© ë§ˆíŠ¸)
CREATE SCHEMA IF NOT EXISTS analytics;

-- Errors ìŠ¤í‚¤ë§ˆ (DLQ ë©”ì‹œì§€)
CREATE SCHEMA IF NOT EXISTS errors;

-- ============================================================
-- REALTIME SCHEMA TABLES (ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­)
-- ============================================================

-- 1. ê´‘ê³  ì´ë²¤íŠ¸ í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS realtime.ad_events (
    id BIGINT PRIMARY KEY,
    click SMALLINT NOT NULL,
    hour INTEGER NOT NULL,
    banner_pos SMALLINT,
    site_id VARCHAR(100),
    site_domain VARCHAR(100),
    site_category VARCHAR(100),
    app_id VARCHAR(100),
    app_domain VARCHAR(100),
    app_category VARCHAR(100),
    device_id VARCHAR(100),
    device_ip VARCHAR(50),
    device_model VARCHAR(100),
    device_type SMALLINT,
    device_conn_type SMALLINT,
    c1 SMALLINT,
    c14 SMALLINT,
    c15 SMALLINT,
    c16 SMALLINT,
    c17 SMALLINT,
    c18 SMALLINT,
    c19 SMALLINT,
    c20 SMALLINT,
    c21 SMALLINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ì¸ë±ìŠ¤
CREATE INDEX idx_ad_events_hour ON realtime.ad_events(hour);
CREATE INDEX idx_ad_events_device_type ON realtime.ad_events(device_type);
CREATE INDEX idx_ad_events_created_at ON realtime.ad_events(created_at);

-- 2. ì‹œê°„ë³„ ì§‘ê³„ í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS realtime.hourly_stats (
    hour INTEGER PRIMARY KEY,
    impression_count BIGINT NOT NULL,
    click_count BIGINT NOT NULL,
    ctr DECIMAL(5, 2),
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ============================================================
-- ANALYTICS SCHEMA TABLES (ë°°ì¹˜ ì²˜ë¦¬ìš© ë§ˆíŠ¸)
-- ============================================================

-- 1. Fact: ì¼ì¼ ì§‘ê³„
CREATE TABLE IF NOT EXISTS analytics.fact_daily_agg (
    date DATE,
    device_type SMALLINT,
    impressions BIGINT,
    clicks BIGINT,
    ctr DECIMAL(5, 2),
    PRIMARY KEY (date, device_type)
);

-- ============================================================
-- ERRORS SCHEMA TABLES (DLQ ì—ëŸ¬ ì¶”ì )
-- ============================================================

CREATE TABLE IF NOT EXISTS errors.dlq_messages (
    id SERIAL PRIMARY KEY,
    message_id VARCHAR(100),
    topic VARCHAR(100),
    partition INTEGER,
    offset BIGINT,
    message_content TEXT,
    error_reason VARCHAR(500),
    received_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processed_at TIMESTAMP
);

CREATE INDEX idx_dlq_messages_received_at ON errors.dlq_messages(received_at);
```

**ì‹¤í–‰:**
```bash
docker-compose exec postgres psql -U postgres -d marketing_roas < scripts/init_postgres.sql
```

#### 1-3. ìŠ¤í‚¤ë§ˆ ê²€ì¦ (10ë¶„)

```bash
docker-compose exec postgres psql -U postgres -d marketing_roas -c "\dt realtime.*"
docker-compose exec postgres psql -U postgres -d marketing_roas -c "\dt analytics.*"
```

### âœ… ì™„ë£Œ ê¸°ì¤€

- [ ] PostgreSQL ì»¨í…Œì´ë„ˆ ì •ìƒ ì‹¤í–‰
- [ ] 3ê°œ ìŠ¤í‚¤ë§ˆ ìƒì„± (realtime, analytics, errors)
- [ ] ëª¨ë“  í…Œì´ë¸” ìƒì„± ì™„ë£Œ
- [ ] ë°ì´í„° ì‚½ì… í…ŒìŠ¤íŠ¸ ì„±ê³µ

### ğŸ“Š ì‚°ì¶œë¬¼

```
scripts/
â””â”€â”€ init_postgres.sql

docker-compose.yml (ì—…ë°ì´íŠ¸)
```

---

## ğŸ“Œ Day 2 (í™”): Airflow DAG ë° ìŠ¤ì¼€ì¤„ë§ (2ì‹œê°„)

### ëª©í‘œ
- Airflow Docker í™˜ê²½ êµ¬ì¶•
- DAG (Directed Acyclic Graph) ì •ì˜
- ì¼ì¼ ë°°ì¹˜ ìŠ¤ì¼€ì¤„ë§ ì„¤ì •
- ì˜ì¡´ì„± ê´€ë¦¬

### ğŸ“‹ í• ë‹¹ ì‹œê°„
| ì‘ì—… | ì‹œê°„ |
|------|------|
| Airflow Docker êµ¬ì„± | 30ë¶„ |
| DAG ì‘ì„± | 60ë¶„ |
| ìŠ¤ì¼€ì¤„ë§ ì„¤ì • | 20ë¶„ |
| í…ŒìŠ¤íŠ¸ | 10ë¶„ |

### ğŸ› ï¸ ì‹¤ìŠµ ë‚´ìš©

#### 2-1. Airflow Docker ì„¤ì • (20ë¶„)

**íŒŒì¼:** `docker-compose.yml` (ì—…ë°ì´íŠ¸)

```yaml
  airflow-postgres:
    image: postgres:15-alpine
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_postgres_data:/var/lib/postgresql/data
    networks:
      - kafka-network

  airflow-webserver:
    image: apache/airflow:2.7.3-python3.11
    container_name: airflow-webserver
    environment:
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags:rw
    depends_on:
      airflow-postgres:
        condition: service_started
    networks:
      - kafka-network
    command: webserver

  airflow-scheduler:
    image: apache/airflow:2.7.3-python3.11
    container_name: airflow-scheduler
    environment:
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
    volumes:
      - ./dags:/opt/airflow/dags:rw
    depends_on:
      airflow-postgres:
        condition: service_started
    networks:
      - kafka-network
    command: scheduler

volumes:
  airflow_postgres_data:
```

**ì´ˆê¸°í™”:**
```bash
docker-compose exec airflow-webserver airflow db init
docker-compose exec airflow-webserver airflow users create \
    --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin
```

#### 2-2. DAG ì‘ì„± (40ë¶„)

**íŒŒì¼:** `dags/dag_daily_etl.py`

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.postgres_operator import PostgresOperator

default_args = {
    'owner': 'data-team',
    'start_date': datetime(2025, 1, 1),
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'dag_daily_etl',
    default_args=default_args,
    description='Daily ETL: Data validation',
    schedule_interval='0 2 * * *',
    catchup=False,
)

def check_data_quality(**context):
    from sqlalchemy import create_engine
    engine = create_engine('postgresql://postgres:postgres@postgres:5432/marketing_roas')
    result = engine.execute("SELECT COUNT(*) FROM realtime.ad_events WHERE created_at >= CURRENT_DATE")
    print(f"Today's records: {result}")

check_quality = PythonOperator(
    task_id='check_data_quality',
    python_callable=check_data_quality,
    dag=dag,
)
```

**íŒŒì¼:** `dags/dag_dbt_run.py`

```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator

default_args = {
    'owner': 'data-team',
    'start_date': datetime(2025, 1, 1),
    'retries': 2,
}

dag = DAG(
    'dag_dbt_run',
    default_args=default_args,
    schedule_interval='0 3 * * *',
    catchup=False,
)

dbt_run = BashOperator(
    task_id='dbt_run',
    bash_command='cd /opt/dbt && dbt run',
    dag=dag,
)

dbt_test = BashOperator(
    task_id='dbt_test',
    bash_command='cd /opt/dbt && dbt test',
    dag=dag,
)

dbt_run >> dbt_test
```

### âœ… ì™„ë£Œ ê¸°ì¤€

- [ ] Airflow Webserver ì •ìƒ ì‹¤í–‰ (localhost:8080)
- [ ] 2ê°œ DAG ìƒì„± (dag_daily_etl, dag_dbt_run)
- [ ] DAG ìŠ¤ì¼€ì¤„ë§ ì„¤ì • í™•ì¸
- [ ] ì˜¤ë¥˜ ì—†ìŒ

### ğŸ“Š ì‚°ì¶œë¬¼

```
dags/
â”œâ”€â”€ dag_daily_etl.py
â””â”€â”€ dag_dbt_run.py

docker-compose.yml (ì—…ë°ì´íŠ¸)
```

---

## ğŸ“Œ Day 3 (ìˆ˜): dbt ëª¨ë¸ ê°œë°œ ë° í…ŒìŠ¤íŠ¸ (2ì‹œê°„)

### ëª©í‘œ
- dbt í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
- Staging ëª¨ë¸ ì‘ì„± (ë°ì´í„° ì •ì œ)
- Mart ëª¨ë¸ ì‘ì„± (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)
- í…ŒìŠ¤íŠ¸ ì‹¤í–‰

### ğŸ“‹ í• ë‹¹ ì‹œê°„
| ì‘ì—… | ì‹œê°„ |
|------|------|
| dbt í”„ë¡œì íŠ¸ ì„¤ì • | 20ë¶„ |
| Staging ëª¨ë¸ | 40ë¶„ |
| Mart ëª¨ë¸ | 40ë¶„ |
| í…ŒìŠ¤íŠ¸ | 20ë¶„ |

### ğŸ› ï¸ ì‹¤ìŠµ ë‚´ìš©

#### 3-1. dbt í”„ë¡œì íŠ¸ ì„¤ì • (15ë¶„)

```bash
dbt init marketing_roas_dbt -d postgres
```

**íŒŒì¼:** `dbt/dbt_project.yml`

```yaml
name: 'marketing_roas'
version: '1.0.0'
profile: 'marketing_roas'
model-paths: ["models"]
```

**íŒŒì¼:** `dbt/profiles.yml`

```yaml
marketing_roas:
  target: dev
  outputs:
    dev:
      type: postgres
      host: postgres
      user: postgres
      password: postgres
      dbname: marketing_roas
      schema: analytics
      threads: 4
```

#### 3-2. Staging ëª¨ë¸ (25ë¶„)

**íŒŒì¼:** `dbt/models/staging/stg_ad_events.sql`

```sql
SELECT
    id,
    click,
    hour,
    device_type,
    created_at
FROM {{ source('realtime', 'ad_events') }}
WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'
```

#### 3-3. Mart ëª¨ë¸ (25ë¶„)

**íŒŒì¼:** `dbt/models/marts/fct_daily_agg.sql`

```sql
SELECT
    CURRENT_DATE as date,
    device_type,
    COUNT(*) as impressions,
    SUM(CASE WHEN click = 1 THEN 1 ELSE 0 END) as clicks,
    ROUND(SUM(CASE WHEN click = 1 THEN 1 ELSE 0 END)::float / COUNT(*) * 100, 2) as ctr
FROM {{ ref('stg_ad_events') }}
WHERE DATE(created_at) = CURRENT_DATE
GROUP BY device_type
```

**ì‹¤í–‰:**
```bash
cd dbt
dbt run
dbt test
```

### âœ… ì™„ë£Œ ê¸°ì¤€

- [ ] dbt í”„ë¡œì íŠ¸ ìƒì„±
- [ ] Staging ëª¨ë¸ ì‹¤í–‰ ì„±ê³µ
- [ ] Mart ëª¨ë¸ ì‹¤í–‰ ì„±ê³µ
- [ ] PostgreSQLì—ì„œ í…Œì´ë¸” í™•ì¸

### ğŸ“Š ì‚°ì¶œë¬¼

```
dbt/
â”œâ”€â”€ dbt_project.yml
â”œâ”€â”€ profiles.yml
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â””â”€â”€ stg_ad_events.sql
â”‚   â””â”€â”€ marts/
â”‚       â””â”€â”€ fct_daily_agg.sql
â””â”€â”€ tests/
```

---

## ğŸ“Œ Day 4 (ëª©): Streamlit ëŒ€ì‹œë³´ë“œ ê°œë°œ (2ì‹œê°„)

### ëª©í‘œ
- Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸° ì„¤ì •
- PostgreSQL ë°ì´í„° ì‹œê°í™”
- ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ
- ì¸í„°ë™í‹°ë¸Œ í•„í„°

### ğŸ“‹ í• ë‹¹ ì‹œê°„
| ì‘ì—… | ì‹œê°„ |
|------|------|
| Streamlit êµ¬ì¡° ì„¤ê³„ | 20ë¶„ |
| DB ì—°ê²° ëª¨ë“ˆ | 30ë¶„ |
| ëŒ€ì‹œë³´ë“œ UI | 50ë¶„ |
| ë°°í¬ ì„¤ì • | 20ë¶„ |

### ğŸ› ï¸ ì‹¤ìŠµ ë‚´ìš©

#### 4-1. Streamlit í”„ë¡œì íŠ¸ (10ë¶„)

```bash
mkdir -p streamlit && cd streamlit
python -m venv venv
source venv/bin/activate
pip install streamlit pandas plotly sqlalchemy psycopg2-binary
```

**í´ë” êµ¬ì¡°:**
```
streamlit/
â”œâ”€â”€ app.py
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ 01_Overview.py
â”‚   â””â”€â”€ 02_Realtime.py
â”œâ”€â”€ components/
â”‚   â””â”€â”€ metrics.py
â””â”€â”€ requirements.txt
```

#### 4-2. ë©”ì¸ ì•± (30ë¶„)

**íŒŒì¼:** `streamlit/app.py`

```python
import streamlit as st
import pandas as pd
from sqlalchemy import create_engine
import plotly.express as px

st.set_page_config(page_title="Marketing ROAS", layout="wide")

@st.cache_resource
def get_db_engine():
    return create_engine('postgresql://postgres:postgres@postgres:5432/marketing_roas')

st.title("ğŸ“Š Marketing ROAS Dashboard")

engine = get_db_engine()
result = pd.read_sql("""
SELECT COUNT(*) as total, SUM(CASE WHEN click = 1 THEN 1 ELSE 0 END) as clicks,
       ROUND(SUM(CASE WHEN click = 1 THEN 1 ELSE 0 END)::float / COUNT(*) * 100, 2) as ctr
FROM realtime.ad_events WHERE created_at >= CURRENT_DATE
""", engine)

col1, col2, col3 = st.columns(3)
col1.metric("Total Events", f"{result['total'][0]:,}")
col2.metric("Clicks", f"{result['clicks'][0]:,}")
col3.metric("CTR", f"{result['ctr'][0]:.2f}%")

st.subheader("Hourly Statistics")
hourly_data = pd.read_sql("""
SELECT hour, impression_count, click_count, ctr
FROM realtime.hourly_stats ORDER BY hour DESC LIMIT 24
""", engine)

fig = px.line(hourly_data, x='hour', y='ctr', title='CTR by Hour')
st.plotly_chart(fig, use_container_width=True)
```

#### 4-3. Docker êµ¬ì„± (10ë¶„)

**docker-compose.yml ì¶”ê°€:**
```yaml
  streamlit:
    image: python:3.11-slim
    container_name: streamlit
    working_dir: /app
    volumes:
      - ./streamlit:/app
    ports:
      - "8501:8501"
    command: bash -c "pip install -r requirements.txt && streamlit run app.py"
    depends_on:
      - postgres
    networks:
      - kafka-network
```

**ì‹¤í–‰:**
```bash
docker-compose up -d streamlit
# http://localhost:8501
```

### âœ… ì™„ë£Œ ê¸°ì¤€

- [ ] Streamlit ì•± ì‹¤í–‰ (localhost:8501)
- [ ] PostgreSQL ì—°ê²° ì„±ê³µ
- [ ] ë©”íŠ¸ë¦­ í‘œì‹œ í™•ì¸
- [ ] ì°¨íŠ¸ ë Œë”ë§ í™•ì¸

### ğŸ“Š ì‚°ì¶œë¬¼

```
streamlit/
â”œâ”€â”€ app.py
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ 01_Overview.py
â”‚   â””â”€â”€ 02_Realtime.py
â””â”€â”€ requirements.txt

docker-compose.yml (ì—…ë°ì´íŠ¸)
```

---

## ğŸ“Œ Day 5 (ê¸ˆ): Grafana ëª¨ë‹ˆí„°ë§ + ìµœì¢… í†µí•© (2ì‹œê°„)

### ëª©í‘œ
- Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì„±
- Prometheus ë©”íŠ¸ë¦­ ì—°ë™
- Kafka/PostgreSQL ëª¨ë‹ˆí„°ë§
- ì „ì²´ E2E í…ŒìŠ¤íŠ¸

### ğŸ“‹ í• ë‹¹ ì‹œê°„
| ì‘ì—… | ì‹œê°„ |
|------|------|
| Grafana Docker êµ¬ì„± | 20ë¶„ |
| ëŒ€ì‹œë³´ë“œ êµ¬ì„± | 40ë¶„ |
| ì•ŒëŒ ê·œì¹™ | 20ë¶„ |
| E2E í…ŒìŠ¤íŠ¸ | 40ë¶„ |

### ğŸ› ï¸ ì‹¤ìŠµ ë‚´ìš©

#### 5-1. Grafana Docker ì„¤ì • (15ë¶„)

**docker-compose.yml ì¶”ê°€:**
```yaml
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_USERS_ALLOW_SIGN_UP: 'false'
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - kafka-network

volumes:
  grafana_data:
```

**ì‹¤í–‰:**
```bash
docker-compose up -d grafana
# http://localhost:3000 (admin/admin)
```

#### 5-2. Prometheus ë°ì´í„°ì†ŒìŠ¤ (15ë¶„)

Grafanaì—ì„œ:
1. Configuration > Data Sources
2. Add Prometheus
3. URL: http://prometheus:9090
4. Save & test

#### 5-3. Kafka ëŒ€ì‹œë³´ë“œ (25ë¶„)

**Grafana UIì—ì„œ ìƒˆ ëŒ€ì‹œë³´ë“œ ìƒì„±:**
- ë©”ì‹œì§€ ì²˜ë¦¬ëŸ‰
- í† í”½ë³„ ë©”ì‹œì§€ ìˆ˜
- Producer/Consumer lag

#### 5-4. E2E í†µí•© í…ŒìŠ¤íŠ¸ (20ë¶„)

**íŒŒì¼:** `scripts/e2e_test.sh`

```bash
#!/bin/bash

echo "=========================================="
echo "WEEK 3 E2E TEST"
echo "=========================================="

echo "1. Checking services..."
docker-compose ps

echo "2. Checking PostgreSQL..."
docker-compose exec postgres psql -U postgres -d marketing_roas -c "SELECT COUNT(*) FROM realtime.ad_events;"

echo "3. Running dbt..."
docker-compose exec dbt dbt run

echo "4. Checking Airflow..."
docker-compose exec airflow-webserver airflow dags list

echo "5. Testing Streamlit..."
curl http://localhost:8501

echo "6. Testing Grafana..."
curl http://localhost:3000

echo "=========================================="
echo "âœ… WEEK 3 E2E TEST COMPLETE"
echo "=========================================="
```

### âœ… ì™„ë£Œ ê¸°ì¤€

- [ ] Grafana ì ‘ì† ê°€ëŠ¥ (localhost:3000)
- [ ] Prometheus ë°ì´í„°ì†ŒìŠ¤ ì—°ê²°
- [ ] Kafka ëŒ€ì‹œë³´ë“œ í‘œì‹œ
- [ ] PostgreSQL ë°ì´í„° í™•ì¸
- [ ] dbt ëª¨ë¸ ìƒì„± ì™„ë£Œ
- [ ] Airflow DAG ì •ìƒ ì‘ë™
- [ ] Streamlit ë°ì´í„° ë¡œë“œ ì™„ë£Œ
- [ ] E2E í…ŒìŠ¤íŠ¸ í†µê³¼

### ğŸ“Š ì‚°ì¶œë¬¼

```
docker-compose.yml (ìµœì¢…)

scripts/
â”œâ”€â”€ init_postgres.sql
â””â”€â”€ e2e_test.sh
```

---

## ğŸ“ Week 3 í•µì‹¬ í•™ìŠµ ë‚´ìš©

### ê¸°ìˆ 
âœ… PostgreSQL ìŠ¤í‚¤ë§ˆ ì„¤ê³„
âœ… Airflow DAG ë° ìŠ¤ì¼€ì¤„ë§
âœ… dbt ë°ì´í„° ë³€í™˜
âœ… Streamlit ëŒ€ì‹œë³´ë“œ
âœ… Grafana ëª¨ë‹ˆí„°ë§

### ì‹œìŠ¤í…œ
âœ… ë°°ì¹˜ ì²˜ë¦¬ ì•„í‚¤í…ì²˜
âœ… ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì„¤ê³„
âœ… ì‹¤ì‹œê°„ + ë°°ì¹˜ í†µí•©
âœ… ëª¨ë‹ˆí„°ë§ ë° ì•ŒëŒ

---

## âœ… ì „ì²´ ì²´í¬ë¦¬ìŠ¤íŠ¸

### PostgreSQL
- [ ] Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰
- [ ] 3ê°œ ìŠ¤í‚¤ë§ˆ ìƒì„±
- [ ] í…Œì´ë¸” ìƒì„±
- [ ] ì¸ë±ìŠ¤ ìƒì„±

### Airflow
- [ ] Webserver ì‹¤í–‰ (localhost:8080)
- [ ] 2ê°œ DAG ìƒì„±
- [ ] ìŠ¤ì¼€ì¤„ë§ í™•ì¸

### dbt
- [ ] í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
- [ ] Staging ëª¨ë¸ ì™„ì„±
- [ ] Mart ëª¨ë¸ ì™„ì„±

### Streamlit
- [ ] ì•± ì‹¤í–‰ (localhost:8501)
- [ ] PostgreSQL ì—°ê²°
- [ ] ë©”íŠ¸ë¦­ í‘œì‹œ

### Grafana
- [ ] ì•± ì‹¤í–‰ (localhost:3000)
- [ ] Prometheus ì—°ë™
- [ ] ëŒ€ì‹œë³´ë“œ ìƒì„±

---

## ğŸ“Š Week 3 ìµœì¢… ì‚°ì¶œë¬¼

```
Week 3 Complete
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_postgres.sql
â”‚   â””â”€â”€ e2e_test.sh
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ dag_daily_etl.py
â”‚   â””â”€â”€ dag_dbt_run.py
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml
â”‚   â””â”€â”€ models/
â””â”€â”€ streamlit/
    â”œâ”€â”€ app.py
    â””â”€â”€ requirements.txt
```

---

**ì‘ì„± ì¼ì‹œ:** 2025-12-13
**ë‹´ë‹¹ì:** Data Engineering Team
**ìƒíƒœ:** âœ… ìµœì¢… ì„¤ê³„ ì™„ë£Œ

**ë‹¤ìŒ ë‹¨ê³„:** í”„ë¡œë•ì…˜ ë°°í¬ & ìë™í™”
