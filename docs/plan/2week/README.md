# Week 2: ì‹¤ì‹œê°„ ì²˜ë¦¬ & ìºì‹±

**ëª©í‘œ:** PyFlinkë¡œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ë¥¼ êµ¬í˜„í•˜ê³ , Redis ìºì‹œì™€ PostgreSQLì„ ì—°ë™í•©ë‹ˆë‹¤.

**ê¸°ê°„:** 5ì¼ (ì›”~ê¸ˆ)
**ì¼ì¼ ë¶„ëŸ‰:** 2ì‹œê°„
**ì´ ì‹œê°„:** 10ì‹œê°„

---

## ğŸ“… ì£¼ê°„ ì¼ì •í‘œ

| ë‹¨ê³„ | ì£¼ì œ | ì‹œê°„ | ëˆ„ì  |
|------|------|------|------|
| **ì›”** | PyFlink ê°œë°œí™˜ê²½ êµ¬ì¶• | 2h | 2h |
| **í™”** | PyFlink ìŠ¤íŠ¸ë¦¬ë° ì‘ì—… (1/3) | 2h | 4h |
| **ìˆ˜** | PyFlink ìŠ¤íŠ¸ë¦¬ë° ì‘ì—… (2/3) | 2h | 6h |
| **ëª©** | Redis + PostgreSQL êµ¬ì¶• | 2h | 8h |
| **ê¸ˆ** | Streamlit ëŒ€ì‹œë³´ë“œ + í†µí•© í…ŒìŠ¤íŠ¸ | 2h | 10h |

---

## ğŸ“Œ Day 1 (ì›”): PyFlink ê°œë°œí™˜ê²½ êµ¬ì¶• (2ì‹œê°„)

### ëª©í‘œ
- PyFlink ë¡œì»¬ í™˜ê²½ ì„¤ì •
- Flink Job/Task Manager Docker êµ¬ì„±
- Kafka ì†ŒìŠ¤ ì—°ë™ í…ŒìŠ¤íŠ¸
- ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ê²€ì¦

### ğŸ“‹ í• ë‹¹ ì‹œê°„
| ì‘ì—… | ì‹œê°„ |
|------|------|
| PyFlink ì„¤ì¹˜ ë° ì„¤ì • | 40ë¶„ |
| Docker êµ¬ì„± | 40ë¶„ |
| Kafka ì—°ë™ í…ŒìŠ¤íŠ¸ | 30ë¶„ |
| ë¬¸ì„œí™” | 10ë¶„ |

### ğŸ› ï¸ ì‹¤ìŠµ ë‚´ìš©

#### 1-1. PyFlink ì„¤ì¹˜ ë° ì˜ì¡´ì„± (20ë¶„)

**íŒŒì¼:** `requirements.txt` (ì—…ë°ì´íŠ¸)

```
# ê¸°ì¡´ ì˜ì¡´ì„±...

# PyFlink (Week 2)
apache-flink==1.18.1
pyflink==1.18.1

# PyFlink ì¶”ê°€ ì˜ì¡´ì„±
avro-python3==1.11.0

# Real-time processing
redis==5.0.1
sqlalchemy==2.0.23
psycopg2-binary==2.9.9

# Streamlit (Week 2)
streamlit==1.28.1
plotly==5.17.0
pandas==2.1.3

# Monitoring
prometheus-client==0.19.0
```

**ì„¤ì¹˜:**
```bash
pip install -r requirements.txt
```

#### 1-2. Flink ì„¤ì • íŒŒì¼ ì‘ì„± (20ë¶„)

**íŒŒì¼:** `src/flink/config.py`

```python
"""
PyFlink ì„¤ì • íŒŒì¼
"""

import os
from dotenv import load_dotenv

load_dotenv()

class FlinkConfig:
    """Flink ì„¤ì •"""

    # Flink í´ëŸ¬ìŠ¤í„° ì„¤ì •
    JOB_MANAGER_RPC_ADDRESS = os.getenv('FLINK_JOBMANAGER_RPC_ADDRESS', 'localhost')
    JOB_MANAGER_RPC_PORT = int(os.getenv('FLINK_JOBMANAGER_RPC_PORT', 6123))
    TASK_MANAGER_RPC_PORT = int(os.getenv('FLINK_TASKMANAGER_RPC_PORT', 6124))

    # ë³‘ë ¬í™” ì„¤ì •
    PARALLELISM = 4  # 4ê°œ task manager slot
    CHECKPOINT_INTERVAL = 60000  # 60ì´ˆ
    CHECKPOINT_TIMEOUT = 600000  # 10ë¶„

    # Kafka ì„¤ì •
    KAFKA_BOOTSTRAP_SERVERS = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092')
    KAFKA_TOPIC = 'ad_events_raw'
    KAFKA_ERROR_TOPIC = 'ad_events_error'

    # Schema Registry
    SCHEMA_REGISTRY_URL = os.getenv('SCHEMA_REGISTRY_URL', 'http://localhost:8081')
    SCHEMA_SUBJECT = 'ad_events_raw-value'

    # ìƒíƒœ ì €ì¥ì†Œ (State Backend)
    CHECKPOINT_DIR = './data/checkpoints'
    STATE_BACKEND = 'rocksdb'  # or 'filesystem'

    # ìœˆë„ìš° ì„¤ì •
    WINDOW_1MIN = 60  # 1ë¶„ (ì´ˆ)
    WINDOW_5MIN = 300  # 5ë¶„ (ì´ˆ)
    ALLOWED_LATENESS = 10  # 10ì´ˆ ì§€ê° í—ˆìš©

    # ë©”ëª¨ë¦¬ ì„¤ì •
    JVM_MEMORY = '1024m'
    TASK_MEMORY = '512m'

    @classmethod
    def get_env_variables(cls):
        """Flink í™˜ê²½ ë³€ìˆ˜ ë”•ì…”ë„ˆë¦¬"""
        return {
            'jobmanager.rpc.address': cls.JOB_MANAGER_RPC_ADDRESS,
            'jobmanager.rpc.port': cls.JOB_MANAGER_RPC_PORT,
            'taskmanager.rpc.port': cls.TASK_MANAGER_RPC_PORT,
            'parallelism.default': cls.PARALLELISM,
            'state.checkpoints.dir': f'file://{cls.CHECKPOINT_DIR}',
            'state.backend': cls.STATE_BACKEND,
            'execution.checkpointing.interval': cls.CHECKPOINT_INTERVAL,
            'execution.checkpointing.timeout': cls.CHECKPOINT_TIMEOUT,
        }

    @classmethod
    def validate(cls):
        """ì„¤ì • ê²€ì¦"""
        os.makedirs(cls.CHECKPOINT_DIR, exist_ok=True)
        print(f"âœ… Flink config validated")
        print(f"   Parallelism: {cls.PARALLELISM}")
        print(f"   Checkpoint Dir: {cls.CHECKPOINT_DIR}")
```

#### 1-3. Docker Compose ì—…ë°ì´íŠ¸ (30ë¶„)

**íŒŒì¼:** `docker-compose.yml` (Flink ì„œë¹„ìŠ¤ ì¶”ê°€)

```yaml
  # Flink JobManager
  flink-jobmanager:
    image: flink:1.18.1-scala_2.12
    container_name: flink-jobmanager
    command: jobmanager
    ports:
      - "6123:6123"
      - "8081:8081"  # Web UI
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - FLINK_PROPERTIES=jobmanager.rpc.address:flink-jobmanager
    volumes:
      - ./data/checkpoints:/flink/checkpoints
      - ./src/flink:/flink/jobs
    networks:
      - kafka-network
    healthcheck:
      test: curl -f http://localhost:8081/overview || exit 1
      interval: 10s
      timeout: 5s
      retries: 5

  # Flink TaskManager (ìµœì†Œ 2ê°œ)
  flink-taskmanager-1:
    image: flink:1.18.1-scala_2.12
    container_name: flink-taskmanager-1
    command: taskmanager
    depends_on:
      flink-jobmanager:
        condition: service_healthy
    ports:
      - "6124:6124"
      - "9081:8081"
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - FLINK_PROPERTIES=jobmanager.rpc.address:flink-jobmanager
        taskmanager.rpc.port:6124
    volumes:
      - ./data/checkpoints:/flink/checkpoints
      - ./src/flink:/flink/jobs
    networks:
      - kafka-network

  flink-taskmanager-2:
    image: flink:1.18.1-scala_2.12
    container_name: flink-taskmanager-2
    command: taskmanager
    depends_on:
      flink-jobmanager:
        condition: service_healthy
    ports:
      - "6125:6124"
      - "9082:8081"
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - FLINK_PROPERTIES=jobmanager.rpc.address:flink-jobmanager
        taskmanager.rpc.port:6124
    volumes:
      - ./data/checkpoints:/flink/checkpoints
      - ./src/flink:/flink/jobs
    networks:
      - kafka-network
```

**ì‹¤í–‰:**
```bash
docker-compose up -d flink-jobmanager flink-taskmanager-1 flink-taskmanager-2

# Flink Web UI ì ‘ì†
# http://localhost:8081

# ë¡œê·¸ í™•ì¸
docker-compose logs -f flink-jobmanager
docker-compose logs -f flink-taskmanager-1
```

#### 1-4. ê¸°ë³¸ PyFlink íŒŒì´í”„ë¼ì¸ (30ë¶„)

**íŒŒì¼:** `src/flink/simple_pipeline.py`

```python
"""
ê¸°ë³¸ PyFlink íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
Kafka â†’ Print Sink
"""

from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors.kafka import KafkaSource, KafkaOffsetsInitializer
from pyflink.common.serialization import SimpleStringSchema
from pyflink.common.typeinfo import Types
import json
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class KafkaAdEventSource:
    """Kafka ì†ŒìŠ¤ ìƒì„±"""

    @staticmethod
    def create_kafka_source(bootstrap_servers='kafka:29092', topic='ad_events_raw'):
        """KafkaSource ìƒì„±"""
        kafka_source = KafkaSource.builder() \
            .set_bootstrap_servers(bootstrap_servers) \
            .set_topics(topic) \
            .set_group_id('flink-consumer-group') \
            .set_starting_offsets(KafkaOffsetsInitializer.earliest()) \
            .set_value_only_deserializer(SimpleStringSchema()) \
            .build()

        return kafka_source


def parse_event(json_str):
    """JSON ë¬¸ìì—´ì„ íŒŒì‹±"""
    try:
        event = json.loads(json_str)
        return event
    except Exception as e:
        logger.error(f"Parse error: {e}")
        return None


def process_event(event):
    """ì´ë²¤íŠ¸ ì²˜ë¦¬"""
    if event is None:
        return None

    # í•„ë“œ ì¶”ì¶œ
    event_id = event.get('id', '')
    click = event.get('click', 0)
    hour = event.get('hour', 0)

    return {
        'id': event_id,
        'click': click,
        'hour': hour,
        'processed_at': int(__import__('time').time() * 1000)
    }


def main():
    """ë©”ì¸ íŒŒì´í”„ë¼ì¸"""

    env = StreamExecutionEnvironment.get_execution_environment()

    # ë³‘ë ¬í™” ì„¤ì •
    env.set_parallelism(2)

    # Checkpoint ì„¤ì •
    env.enable_checkpointing(60000)  # 60ì´ˆ

    try:
        # Kafka ì†ŒìŠ¤ ìƒì„±
        logger.info("Creating Kafka source...")
        kafka_source = KafkaAdEventSource.create_kafka_source(
            bootstrap_servers='kafka:29092',
            topic='ad_events_raw'
        )

        # ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ìƒì„±
        data_stream = env.add_source(kafka_source)

        # ì´ë²¤íŠ¸ íŒŒì‹± ë° ì²˜ë¦¬
        processed_stream = data_stream \
            .map(lambda x: parse_event(x), output_type=Types.MAP(Types.STRING, Types.OBJECT)) \
            .filter(lambda x: x is not None) \
            .map(lambda x: process_event(x), output_type=Types.MAP(Types.STRING, Types.OBJECT))

        # ì½˜ì†” ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        processed_stream.print()

        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        logger.info("Starting Flink pipeline...")
        env.execute("Simple Kafka Pipeline")

    except Exception as e:
        logger.error(f"Pipeline error: {e}")
        raise


if __name__ == "__main__":
    main()
```

**ì‹¤í–‰:**
```bash
cd src/flink
python simple_pipeline.py
```

#### 1-5. ì—°ë™ í…ŒìŠ¤íŠ¸ (20ë¶„)

**íŒŒì¼:** `scripts/test_flink_kafka.sh`

```bash
#!/bin/bash

echo "================================"
echo "Flink + Kafka ì—°ë™ í…ŒìŠ¤íŠ¸"
echo "================================"

# 1ï¸âƒ£  Flink ìƒíƒœ í™•ì¸
echo ""
echo "1ï¸âƒ£  Flink ì„œë¹„ìŠ¤ ìƒíƒœ..."
docker-compose ps | grep flink

# 2ï¸âƒ£  Kafka í† í”½ í™•ì¸
echo ""
echo "2ï¸âƒ£  Kafka í† í”½ í™•ì¸..."
docker-compose exec kafka kafka-topics \
    --list \
    --bootstrap-server kafka:29092

# 3ï¸âƒ£  Flink Web UI ì ‘ê·¼ì„± í™•ì¸
echo ""
echo "3ï¸âƒ£  Flink Web UI ì ‘ê·¼ì„± í™•ì¸..."
curl -s http://localhost:8081/overview | jq '.["taskmanagers"]' || echo "Flink UI ì¤€ë¹„ ì¤‘..."

# 4ï¸âƒ£  ë©”ì‹œì§€ ë°œí–‰ (í…ŒìŠ¤íŠ¸ìš©)
echo ""
echo "4ï¸âƒ£  í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ë°œí–‰..."
python src/kafka/producer.py 100

# 5ï¸âƒ£  í† í”½ ë©”ì‹œì§€ í™•ì¸
echo ""
echo "5ï¸âƒ£  í† í”½ ë©”ì‹œì§€ í™•ì¸..."
docker-compose exec kafka kafka-console-consumer \
    --bootstrap-server kafka:29092 \
    --topic ad_events_raw \
    --from-beginning \
    --max-messages 1 \
    --timeout-ms 5000 2>/dev/null || echo "ë©”ì‹œì§€ ëŒ€ê¸° ì¤‘..."

echo ""
echo "âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ"
```

**ì‹¤í–‰:**
```bash
chmod +x scripts/test_flink_kafka.sh
bash scripts/test_flink_kafka.sh
```

### âœ… ì™„ë£Œ ê¸°ì¤€

- [ ] PyFlink ì„¤ì¹˜ ì™„ë£Œ
- [ ] Flink í™˜ê²½ ì„¤ì • íŒŒì¼ ì‘ì„±
- [ ] docker-compose.ymlì— Flink ì„œë¹„ìŠ¤ ì¶”ê°€
- [ ] ëª¨ë“  Flink ì»¨í…Œì´ë„ˆ ì •ìƒ ì‹¤í–‰
- [ ] Flink Web UI (http://localhost:8081) ì ‘ê·¼ ê°€ëŠ¥
- [ ] Kafka â†’ Flink ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ

### ğŸ“Š ì‚°ì¶œë¬¼

```
src/flink/
â”œâ”€â”€ config.py (Flink ì„¤ì •)
â””â”€â”€ simple_pipeline.py (ê¸°ë³¸ íŒŒì´í”„ë¼ì¸)

docker-compose.yml (Flink ì„œë¹„ìŠ¤ ì¶”ê°€)

scripts/
â””â”€â”€ test_flink_kafka.sh (í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸)
```

---

## ğŸ“Œ Day 2-3 (í™”-ìˆ˜): PyFlink ìŠ¤íŠ¸ë¦¬ë° ì‘ì—… êµ¬í˜„ (4ì‹œê°„)

### ëª©í‘œ
- KafkaSource ì™„ì „ êµ¬í˜„ (Schema Registry ì—°ë™)
- Event Time ë° Watermark ì„¤ì •
- Tumbling Window (1ë¶„, 5ë¶„) êµ¬í˜„
- CTR ê³„ì‚° ë¡œì§ êµ¬í˜„
- State Management ë° Checkpoint

### ğŸ“‹ í• ë‹¹ ì‹œê°„
| ì‘ì—… | ì‹œê°„ |
|------|------|
| KafkaSource + ìŠ¤í‚¤ë§ˆ | 50ë¶„ |
| Window ë° ì§‘ê³„ | 60ë¶„ |
| State ê´€ë¦¬ | 50ë¶„ |
| í†µí•© í…ŒìŠ¤íŠ¸ | 40ë¶„ |

### ğŸ› ï¸ ì‹¤ìŠµ ë‚´ìš©

#### 2-1. KafkaSource with Schema Registry (30ë¶„)

**íŒŒì¼:** `src/flink/kafka_source.py`

```python
"""
Schema Registryì™€ ì—°ë™í•˜ëŠ” KafkaSource
"""

import json
import logging
from typing import Dict, Any
from confluent_kafka.schema_registry import SchemaRegistryClient
from confluent_kafka.schema_registry.json_schema import JSONDeserializer
from confluent_kafka.schema_registry.avro import AvroDeserializer
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors.kafka import KafkaSource, KafkaOffsetsInitializer
from pyflink.common.serialization import DeserializationSchema
from pyflink.common.typeinfo import Types

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AvroDeserializationSchema(DeserializationSchema):
    """Avro ì—­ì§ë ¬í™” ìŠ¤í‚¤ë§ˆ"""

    def __init__(self, schema_registry_url: str, subject: str):
        self.schema_registry_url = schema_registry_url
        self.subject = subject
        self.deserializer = None
        self._init_deserializer()

    def _init_deserializer(self):
        """Avro ì—­ì§ë ¬í™”ê¸° ì´ˆê¸°í™”"""
        try:
            sr_client = SchemaRegistryClient({'url': self.schema_registry_url})
            schema = sr_client.get_latest_version(self.subject)
            self.deserializer = AvroDeserializer(sr_client, schema.schema)
            logger.info(f"âœ… Avro deserializer initialized for {self.subject}")
        except Exception as e:
            logger.error(f"Failed to init deserializer: {e}")
            raise

    def deserialize(self, message: bytes) -> Dict[str, Any]:
        """ë°”ì´íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì—­ì§ë ¬í™”"""
        try:
            return self.deserializer(message, ctx=None)
        except Exception as e:
            logger.error(f"Deserialization error: {e}")
            return None

    def is_end_of_stream(self, next_element: Dict) -> bool:
        return next_element is None

    def get_produced_type(self):
        return Types.MAP(Types.STRING, Types.OBJECT)


class AdEventKafkaSource:
    """ê´‘ê³  ì´ë²¤íŠ¸ Kafka ì†ŒìŠ¤"""

    @staticmethod
    def create_source(
        bootstrap_servers: str = 'kafka:29092',
        topic: str = 'ad_events_raw',
        schema_registry_url: str = 'http://schema-registry:8081',
        group_id: str = 'flink-ad-events'
    ):
        """
        Kafka ì†ŒìŠ¤ ìƒì„±

        Args:
            bootstrap_servers: Kafka ë¶€íŠ¸ìŠ¤íŠ¸ë© ì„œë²„
            topic: í† í”½ëª…
            schema_registry_url: Schema Registry URL
            group_id: ì»¨ìŠˆë¨¸ ê·¸ë£¹

        Returns:
            KafkaSource
        """

        # Avro ì—­ì§ë ¬í™” ìŠ¤í‚¤ë§ˆ
        avro_schema = AvroDeserializationSchema(
            schema_registry_url=schema_registry_url,
            subject=f'{topic}-value'
        )

        # KafkaSource ë¹Œë”
        kafka_source = KafkaSource.builder() \
            .set_bootstrap_servers(bootstrap_servers) \
            .set_topics(topic) \
            .set_group_id(group_id) \
            .set_starting_offsets(KafkaOffsetsInitializer.earliest()) \
            .set_value_only_deserializer(avro_schema) \
            .set_property('isolation.level', 'read_committed') \
            .build()

        logger.info(f"âœ… KafkaSource created: {topic}")
        return kafka_source


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    source = AdEventKafkaSource.create_source()
    print("âœ… Source created successfully")
```

#### 2-2. ì§‘ê³„ í•¨ìˆ˜ ë° Window (40ë¶„)

**íŒŒì¼:** `src/flink/aggregations.py`

```python
"""
ì‹¤ì‹œê°„ ì§‘ê³„ í•¨ìˆ˜ ë° Window ì—°ì‚°
"""

import logging
from typing import Dict, Any, Tuple
from datetime import datetime
from pyflink.datastream.window import TumblingEventTimeWindow
from pyflink.common.time import Time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AdEventAggregator:
    """ê´‘ê³  ì´ë²¤íŠ¸ ì§‘ê³„ê¸°"""

    @staticmethod
    def extract_timestamp(event: Dict[str, Any]) -> int:
        """ì´ë²¤íŠ¸ì—ì„œ íƒ€ì„ìŠ¤íƒí”„ ì¶”ì¶œ"""
        try:
            # event['timestamp']ëŠ” ë°€ë¦¬ì´ˆ ë‹¨ìœ„
            return int(event.get('timestamp', 0))
        except Exception as e:
            logger.error(f"Timestamp extraction error: {e}")
            return 0

    @staticmethod
    def create_window_key(event: Dict[str, Any]) -> str:
        """ìœˆë„ìš° í‚¤ ìƒì„± (íŒŒí‹°ì…”ë‹ìš©)"""
        # ì˜ˆ: hourë³„, device_typeë³„ë¡œ ë¶„í• 
        hour = event.get('hour', 'unknown')
        device_type = event.get('device_type', 'unknown')
        return f"{hour}_{device_type}"

    @staticmethod
    def aggregate_1min(
        events: list,
        window_start: int,
        window_end: int
    ) -> Dict[str, Any]:
        """1ë¶„ ë‹¨ìœ„ ì§‘ê³„"""

        if not events:
            return None

        total_events = len(events)
        clicks = sum(1 for e in events if e.get('click', 0) == 1)
        impressions = total_events
        ctr = (clicks / impressions * 100) if impressions > 0 else 0

        # ì¹´í…Œê³ ë¦¬ë³„ CTR
        category_ctr = {}
        for event in events:
            cat = event.get('site_category', 'unknown')
            if cat not in category_ctr:
                category_ctr[cat] = {'clicks': 0, 'impressions': 0}
            category_ctr[cat]['impressions'] += 1
            if event.get('click') == 1:
                category_ctr[cat]['clicks'] += 1

        result = {
            'window_start': window_start,
            'window_end': window_end,
            'window_type': '1min',
            'total_impressions': impressions,
            'total_clicks': clicks,
            'ctr': round(ctr, 2),
            'category_ctr': {
                cat: round(
                    v['clicks'] / v['impressions'] * 100, 2
                ) for cat, v in category_ctr.items()
            },
            'timestamp': int(__import__('time').time() * 1000)
        }

        logger.info(f"1min aggregation: {impressions} events, CTR={ctr:.2f}%")
        return result

    @staticmethod
    def aggregate_5min(
        events: list,
        window_start: int,
        window_end: int
    ) -> Dict[str, Any]:
        """5ë¶„ ë‹¨ìœ„ ì§‘ê³„"""

        if not events:
            return None

        total_events = len(events)
        clicks = sum(1 for e in events if e.get('click', 0) == 1)
        impressions = total_events
        ctr = (clicks / impressions * 100) if impressions > 0 else 0

        # ë””ë°”ì´ìŠ¤ íƒ€ì…ë³„ CTR
        device_ctr = {}
        for event in events:
            dev = event.get('device_type', 'unknown')
            if dev not in device_ctr:
                device_ctr[dev] = {'clicks': 0, 'impressions': 0}
            device_ctr[dev]['impressions'] += 1
            if event.get('click') == 1:
                device_ctr[dev]['clicks'] += 1

        result = {
            'window_start': window_start,
            'window_end': window_end,
            'window_type': '5min',
            'total_impressions': impressions,
            'total_clicks': clicks,
            'ctr': round(ctr, 2),
            'device_ctr': {
                dev: round(
                    v['clicks'] / v['impressions'] * 100, 2
                ) for dev, v in device_ctr.items()
            },
            'timestamp': int(__import__('time').time() * 1000)
        }

        logger.info(f"5min aggregation: {impressions} events, CTR={ctr:.2f}%")
        return result


class WindowConfig:
    """Window ì„¤ì •"""

    # Window í¬ê¸°
    WINDOW_1MIN = Time.milliseconds(60 * 1000)  # 1ë¶„
    WINDOW_5MIN = Time.milliseconds(5 * 60 * 1000)  # 5ë¶„

    # Watermark ì„¤ì •
    WATERMARK_ALLOWED_LATENESS = Time.seconds(10)  # 10ì´ˆ ì§€ê° í—ˆìš©

    @staticmethod
    def get_1min_window():
        """1ë¶„ Window ìƒì„±"""
        return TumblingEventTimeWindow(WindowConfig.WINDOW_1MIN)

    @staticmethod
    def get_5min_window():
        """5ë¶„ Window ìƒì„±"""
        return TumblingEventTimeWindow(WindowConfig.WINDOW_5MIN)
```

#### 2-3. ì™„ì „í•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ì—… (60ë¶„)

**íŒŒì¼:** `src/flink/streaming_job.py`

```python
"""
ì™„ì „í•œ PyFlink ìŠ¤íŠ¸ë¦¬ë° ì‘ì—…
Kafka â†’ 1min/5min Window â†’ Redis/PostgreSQL
"""

import logging
import json
import time
from typing import Dict, Any

from pyflink.datastream import StreamExecutionEnvironment, DataStream
from pyflink.datastream.window import TumblingEventTimeWindow
from pyflink.datastream.functions import WindowFunction, AggregateFunction
from pyflink.common.time import Time
from pyflink.common.typeinfo import Types

from kafka_source import AdEventKafkaSource
from aggregations import AdEventAggregator, WindowConfig
from config import FlinkConfig

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AdEventWindowFunction(WindowFunction):
    """Window í•¨ìˆ˜"""

    def __init__(self, window_size: int):
        self.window_size = window_size
        self.aggregator = AdEventAggregator()

    def apply(self, key, window, inputs):
        """Window í•¨ìˆ˜ ì‹¤í–‰"""
        events = list(inputs)
        window_start = window.start
        window_end = window.end

        if self.window_size == 60:
            # 1ë¶„ window
            result = self.aggregator.aggregate_1min(
                events, window_start, window_end
            )
        else:
            # 5ë¶„ window
            result = self.aggregator.aggregate_5min(
                events, window_start, window_end
            )

        if result:
            yield result


class SimpleAggregator(AggregateFunction):
    """ê°„ë‹¨í•œ ì§‘ê³„ í•¨ìˆ˜"""

    def create_accumulator(self):
        """ëˆ„ì ê¸° ìƒì„±"""
        return {
            'count': 0,
            'clicks': 0,
            'total_click': 0
        }

    def add(self, value, accumulator):
        """ê°’ ì¶”ê°€"""
        accumulator['count'] += 1
        if value.get('click') == 1:
            accumulator['clicks'] += 1
            accumulator['total_click'] += 1
        return accumulator

    def get_result(self, accumulator):
        """ê²°ê³¼ ë°˜í™˜"""
        ctr = (
            accumulator['clicks'] / accumulator['count'] * 100
            if accumulator['count'] > 0 else 0
        )
        return {
            'total': accumulator['count'],
            'clicks': accumulator['clicks'],
            'ctr': round(ctr, 2),
            'timestamp': int(time.time() * 1000)
        }

    def merge(self, a, b):
        """ëˆ„ì ê¸° ë³‘í•©"""
        a['count'] += b['count']
        a['clicks'] += b['clicks']
        return a


class PrintSink:
    """ì½˜ì†” ì¶œë ¥ Sink (ë””ë²„ê¹…)"""

    @staticmethod
    def print_result(data: Dict[str, Any]):
        """ê²°ê³¼ ì¶œë ¥"""
        logger.info(f"Window Result: {json.dumps(data, indent=2)}")
        return data


def create_streaming_job(env: StreamExecutionEnvironment):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ì—… ìƒì„±"""

    logger.info("=" * 60)
    logger.info("Starting Flink Streaming Job")
    logger.info("=" * 60)

    # 1ï¸âƒ£  Kafka ì†ŒìŠ¤ ìƒì„±
    logger.info("Creating Kafka source...")
    kafka_source = AdEventKafkaSource.create_source(
        bootstrap_servers=FlinkConfig.KAFKA_BOOTSTRAP_SERVERS,
        topic=FlinkConfig.KAFKA_TOPIC,
        schema_registry_url=FlinkConfig.SCHEMA_REGISTRY_URL
    )

    # 2ï¸âƒ£  ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ìƒì„±
    data_stream = env.add_source(kafka_source)

    # 3ï¸âƒ£  Timestamp ë° Watermark ì„¤ì •
    logger.info("Setting watermark strategy...")
    data_stream = data_stream.assign_timestamps_and_watermarks(
        WatermarkStrategy.for_bounded_out_of_orderness(
            Time.seconds(FlinkConfig.ALLOWED_LATENESS)
        ).with_timestamp_selector(
            lambda event: AdEventAggregator.extract_timestamp(event)
        )
    )

    # 4ï¸âƒ£  1ë¶„ Window ì ìš©
    logger.info("Creating 1-minute window aggregation...")
    window_1min = data_stream \
        .key_by(lambda e: AdEventAggregator.create_window_key(e)) \
        .window(TumblingEventTimeWindow(Time.milliseconds(60 * 1000))) \
        .apply(AdEventWindowFunction(window_size=60))

    # 5ï¸âƒ£  5ë¶„ Window ì ìš©
    logger.info("Creating 5-minute window aggregation...")
    window_5min = data_stream \
        .key_by(lambda e: AdEventAggregator.create_window_key(e)) \
        .window(TumblingEventTimeWindow(Time.milliseconds(5 * 60 * 1000))) \
        .apply(AdEventWindowFunction(window_size=300))

    # 6ï¸âƒ£  ë””ë²„ê¹… ì¶œë ¥
    window_1min.map(PrintSink.print_result).print()
    window_5min.map(PrintSink.print_result).print()

    return window_1min, window_5min


def main():
    """ë©”ì¸ í•¨ìˆ˜"""

    # Flink í™˜ê²½ ì„¤ì • ê²€ì¦
    FlinkConfig.validate()

    # Flink í™˜ê²½ ìƒì„±
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_parallelism(FlinkConfig.PARALLELISM)

    # Checkpoint í™œì„±í™”
    env.enable_checkpointing(FlinkConfig.CHECKPOINT_INTERVAL)
    env.get_checkpoint_config().set_checkpointing_timeout(
        FlinkConfig.CHECKPOINT_TIMEOUT
    )

    try:
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ì—… ìƒì„±
        window_1min, window_5min = create_streaming_job(env)

        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        logger.info("Starting job execution...")
        env.execute("Ad Event Streaming Job")

    except Exception as e:
        logger.error(f"Job execution failed: {e}")
        raise
    finally:
        logger.info("Job completed")


if __name__ == "__main__":
    main()
```

**ì‹¤í–‰:**
```bash
cd src/flink
python streaming_job.py
```

### âœ… ì™„ë£Œ ê¸°ì¤€

- [ ] KafkaSource with Schema Registry êµ¬í˜„
- [ ] Timestamp ë° Watermark ì„¤ì • ì™„ë£Œ
- [ ] 1ë¶„ Window ì§‘ê³„ ì‘ë™
- [ ] 5ë¶„ Window ì§‘ê³„ ì‘ë™
- [ ] CTR ê³„ì‚° ì •í™•ë„ ê²€ì¦
- [ ] Checkpoint ì €ì¥ í™•ì¸
- [ ] ì½˜ì†” ë¡œê·¸ì—ì„œ ê²°ê³¼ ì¶œë ¥ í™•ì¸

### ğŸ“Š ì‚°ì¶œë¬¼

```
src/flink/
â”œâ”€â”€ kafka_source.py (Schema Registry ì—°ë™)
â”œâ”€â”€ aggregations.py (ì§‘ê³„ í•¨ìˆ˜)
â””â”€â”€ streaming_job.py (ë©”ì¸ ìŠ¤íŠ¸ë¦¬ë° ì‘ì—…)

data/
â””â”€â”€ checkpoints/ (Checkpoint íŒŒì¼)
```

---

## ğŸ“Œ Day 4 (ëª©): Redis + PostgreSQL êµ¬ì¶• (2ì‹œê°„)

### ëª©í‘œ
- Redis Docker ì„¤ì • ë° ìºì‹œ êµ¬ì¡° ì„¤ê³„
- PostgreSQL ìŠ¤í‚¤ë§ˆ ìƒì„±
- Flink â†’ Redis/PostgreSQL Sink êµ¬í˜„
- ë°ì´í„° ì—°ê²°ì„± í…ŒìŠ¤íŠ¸

### ğŸ“‹ í• ë‹¹ ì‹œê°„
| ì‘ì—… | ì‹œê°„ |
|------|------|
| Redis ìºì‹œ ì„¤ê³„ ë° êµ¬í˜„ | 40ë¶„ |
| PostgreSQL ìŠ¤í‚¤ë§ˆ | 40ë¶„ |
| Sink êµ¬í˜„ | 30ë¶„ |
| í†µí•© í…ŒìŠ¤íŠ¸ | 10ë¶„ |

### ğŸ› ï¸ ì‹¤ìŠµ ë‚´ìš©

#### 4-1. Redis ìºì‹œ ê´€ë¦¬ì (25ë¶„)

**íŒŒì¼:** `src/redis/cache_manager.py`

```python
"""
Redis ìºì‹œ ê´€ë¦¬ì
"""

import redis
import json
import logging
from typing import Dict, Any, Optional
from datetime import datetime, timedelta

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RedisCacheManager:
    """Redis ìºì‹œ ê´€ë¦¬"""

    def __init__(
        self,
        host: str = 'redis',
        port: int = 6379,
        db: int = 0,
        ttl_minutes: int = 5
    ):
        """
        ì´ˆê¸°í™”

        Args:
            host: Redis í˜¸ìŠ¤íŠ¸
            port: Redis í¬íŠ¸
            db: ë°ì´í„°ë² ì´ìŠ¤ ë²ˆí˜¸
            ttl_minutes: TTL (ë¶„)
        """
        self.host = host
        self.port = port
        self.db = db
        self.ttl = ttl_minutes * 60  # ì´ˆ ë‹¨ìœ„

        try:
            self.client = redis.Redis(
                host=host,
                port=port,
                db=db,
                decode_responses=True,
                socket_keepalive=True
            )
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            self.client.ping()
            logger.info(f"âœ… Connected to Redis {host}:{port}")
        except Exception as e:
            logger.error(f"Failed to connect to Redis: {e}")
            raise

    def set_1min_metrics(self, window_id: str, metrics: Dict[str, Any]):
        """1ë¶„ ë©”íŠ¸ë¦­ ì €ì¥"""
        key = f"metrics:1min:{window_id}"
        try:
            self.client.setex(
                key,
                self.ttl,
                json.dumps(metrics)
            )
            logger.info(f"Cached 1min metrics: {key}")
        except Exception as e:
            logger.error(f"Failed to cache: {e}")

    def set_5min_metrics(self, window_id: str, metrics: Dict[str, Any]):
        """5ë¶„ ë©”íŠ¸ë¦­ ì €ì¥"""
        key = f"metrics:5min:{window_id}"
        try:
            self.client.setex(
                key,
                self.ttl * 5,  # 25ë¶„ TTL
                json.dumps(metrics)
            )
            logger.info(f"Cached 5min metrics: {key}")
        except Exception as e:
            logger.error(f"Failed to cache: {e}")

    def get_1min_metrics(self, window_id: str) -> Optional[Dict]:
        """1ë¶„ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        key = f"metrics:1min:{window_id}"
        try:
            data = self.client.get(key)
            if data:
                return json.loads(data)
            return None
        except Exception as e:
            logger.error(f"Failed to get metric: {e}")
            return None

    def get_all_current_metrics(self) -> Dict[str, Any]:
        """í˜„ì¬ ëª¨ë“  ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        try:
            keys_1min = self.client.keys("metrics:1min:*")
            keys_5min = self.client.keys("metrics:5min:*")

            result = {
                '1min': {},
                '5min': {},
                'timestamp': datetime.now().isoformat()
            }

            for key in keys_1min:
                data = self.client.get(key)
                if data:
                    result['1min'][key] = json.loads(data)

            for key in keys_5min:
                data = self.client.get(key)
                if data:
                    result['5min'][key] = json.loads(data)

            return result
        except Exception as e:
            logger.error(f"Failed to get all metrics: {e}")
            return {}

    def health_check(self) -> bool:
        """ìƒíƒœ í™•ì¸"""
        try:
            self.client.ping()
            info = self.client.info()
            logger.info(
                f"Redis health: "
                f"used_memory={info.get('used_memory_human', 'N/A')}, "
                f"connected_clients={info.get('connected_clients', 'N/A')}"
            )
            return True
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return False

    def flush_all(self):
        """ëª¨ë“  ë°ì´í„° ì‚­ì œ (í…ŒìŠ¤íŠ¸ìš©)"""
        try:
            self.client.flushdb()
            logger.info("Redis database flushed")
        except Exception as e:
            logger.error(f"Flush failed: {e}")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    cache = RedisCacheManager()

    # ë©”íŠ¸ë¦­ ì €ì¥
    metrics_1min = {
        'window_id': 'test_1',
        'ctr': 16.5,
        'impressions': 1000,
        'clicks': 165
    }
    cache.set_1min_metrics('test_1', metrics_1min)

    # ë©”íŠ¸ë¦­ ì¡°íšŒ
    result = cache.get_1min_metrics('test_1')
    print(f"Retrieved: {result}")

    # ìƒíƒœ í™•ì¸
    cache.health_check()
```

#### 4-2. PostgreSQL ìŠ¤í‚¤ë§ˆ (25ë¶„)

**íŒŒì¼:** `src/postgres/schema.sql`

```sql
-- realtime ìŠ¤í‚¤ë§ˆ ìƒì„±
CREATE SCHEMA IF NOT EXISTS realtime;

-- 1ë¶„ ì§‘ê³„ í…Œì´ë¸”
CREATE TABLE realtime.metrics_1min (
    id SERIAL PRIMARY KEY,
    window_start BIGINT NOT NULL,
    window_end BIGINT NOT NULL,
    total_impressions INTEGER NOT NULL,
    total_clicks INTEGER NOT NULL,
    ctr NUMERIC(5, 2) NOT NULL,
    category_ctr JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_window_time (window_start, window_end)
);

-- 5ë¶„ ì§‘ê³„ í…Œì´ë¸”
CREATE TABLE realtime.metrics_5min (
    id SERIAL PRIMARY KEY,
    window_start BIGINT NOT NULL,
    window_end BIGINT NOT NULL,
    total_impressions INTEGER NOT NULL,
    total_clicks INTEGER NOT NULL,
    ctr NUMERIC(5, 2) NOT NULL,
    device_ctr JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_window_time (window_start, window_end)
);

-- ì›ë³¸ ì´ë²¤íŠ¸ í…Œì´ë¸”
CREATE TABLE realtime.raw_events (
    id VARCHAR(255) PRIMARY KEY,
    click INTEGER,
    hour INTEGER,
    C1 INTEGER,
    banner_pos INTEGER,
    site_id VARCHAR(255),
    site_category VARCHAR(255),
    device_type INTEGER,
    device_ip VARCHAR(255),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_hour (hour),
    INDEX idx_device_type (device_type)
);

-- ì„±ëŠ¥ ìµœì í™” ì¸ë±ìŠ¤
CREATE INDEX idx_metrics_1min_ctr ON realtime.metrics_1min(ctr DESC);
CREATE INDEX idx_metrics_5min_ctr ON realtime.metrics_5min(ctr DESC);
CREATE INDEX idx_events_created ON realtime.raw_events(created_at DESC);
```

#### 4-3. PostgreSQL ì—°ê²° ê´€ë¦¬ì (25ë¶„)

**íŒŒì¼:** `src/postgres/db_connector.py`

```python
"""
PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ì“°ê¸°
"""

import logging
import json
from typing import Dict, Any
from sqlalchemy import create_engine, Column, Integer, Float, String, DateTime, JSON, BIGINT
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

Base = declarative_base()


class Metrics1Min(Base):
    """1ë¶„ ë©”íŠ¸ë¦­ ëª¨ë¸"""
    __tablename__ = 'metrics_1min'
    __table_args__ = {'schema': 'realtime'}

    id = Column(Integer, primary_key=True, autoincrement=True)
    window_start = Column(BIGINT, nullable=False, index=True)
    window_end = Column(BIGINT, nullable=False, index=True)
    total_impressions = Column(Integer, nullable=False)
    total_clicks = Column(Integer, nullable=False)
    ctr = Column(Float, nullable=False, index=True)
    category_ctr = Column(JSON)
    created_at = Column(DateTime, default=datetime.utcnow, index=True)


class Metrics5Min(Base):
    """5ë¶„ ë©”íŠ¸ë¦­ ëª¨ë¸"""
    __tablename__ = 'metrics_5min'
    __table_args__ = {'schema': 'realtime'}

    id = Column(Integer, primary_key=True, autoincrement=True)
    window_start = Column(BIGINT, nullable=False, index=True)
    window_end = Column(BIGINT, nullable=False, index=True)
    total_impressions = Column(Integer, nullable=False)
    total_clicks = Column(Integer, nullable=False)
    ctr = Column(Float, nullable=False, index=True)
    device_ctr = Column(JSON)
    created_at = Column(DateTime, default=datetime.utcnow, index=True)


class RawEvent(Base):
    """ì›ë³¸ ì´ë²¤íŠ¸ ëª¨ë¸"""
    __tablename__ = 'raw_events'
    __table_args__ = {'schema': 'realtime'}

    id = Column(String(255), primary_key=True)
    click = Column(Integer)
    hour = Column(Integer, index=True)
    c1 = Column(Integer)
    banner_pos = Column(Integer)
    site_id = Column(String(255))
    site_category = Column(String(255))
    device_type = Column(Integer, index=True)
    device_ip = Column(String(255))
    created_at = Column(DateTime, default=datetime.utcnow, index=True)


class PostgreSQLConnector:
    """PostgreSQL ì»¤ë„¥í„°"""

    def __init__(
        self,
        host: str = 'postgres',
        port: int = 5432,
        database: str = 'marketing_roas',
        user: str = 'postgres',
        password: str = 'postgres'
    ):
        """ì´ˆê¸°í™”"""
        self.connection_string = (
            f'postgresql://{user}:{password}@{host}:{port}/{database}'
        )

        try:
            self.engine = create_engine(self.connection_string, echo=False)
            self.SessionLocal = sessionmaker(bind=self.engine)

            # í…Œì´ë¸” ìƒì„±
            Base.metadata.create_all(self.engine)
            logger.info(f"âœ… Connected to PostgreSQL {host}:{port}/{database}")
        except Exception as e:
            logger.error(f"Failed to connect: {e}")
            raise

    def insert_1min_metrics(self, metrics: Dict[str, Any]) -> bool:
        """1ë¶„ ë©”íŠ¸ë¦­ ì €ì¥"""
        session = self.SessionLocal()
        try:
            record = Metrics1Min(
                window_start=metrics['window_start'],
                window_end=metrics['window_end'],
                total_impressions=metrics['total_impressions'],
                total_clicks=metrics['total_clicks'],
                ctr=metrics['ctr'],
                category_ctr=json.dumps(metrics.get('category_ctr', {}))
            )
            session.add(record)
            session.commit()
            logger.info(f"Inserted 1min metrics")
            return True
        except Exception as e:
            logger.error(f"Insert error: {e}")
            session.rollback()
            return False
        finally:
            session.close()

    def insert_5min_metrics(self, metrics: Dict[str, Any]) -> bool:
        """5ë¶„ ë©”íŠ¸ë¦­ ì €ì¥"""
        session = self.SessionLocal()
        try:
            record = Metrics5Min(
                window_start=metrics['window_start'],
                window_end=metrics['window_end'],
                total_impressions=metrics['total_impressions'],
                total_clicks=metrics['total_clicks'],
                ctr=metrics['ctr'],
                device_ctr=json.dumps(metrics.get('device_ctr', {}))
            )
            session.add(record)
            session.commit()
            logger.info(f"Inserted 5min metrics")
            return True
        except Exception as e:
            logger.error(f"Insert error: {e}")
            session.rollback()
            return False
        finally:
            session.close()

    def insert_raw_event(self, event: Dict[str, Any]) -> bool:
        """ì›ë³¸ ì´ë²¤íŠ¸ ì €ì¥"""
        session = self.SessionLocal()
        try:
            record = RawEvent(
                id=event['id'],
                click=event.get('click'),
                hour=event.get('hour'),
                c1=event.get('C1'),
                banner_pos=event.get('banner_pos'),
                site_id=event.get('site_id'),
                site_category=event.get('site_category'),
                device_type=event.get('device_type'),
                device_ip=event.get('device_ip')
            )
            session.add(record)
            session.commit()
            return True
        except Exception as e:
            logger.debug(f"Insert error: {e}")
            session.rollback()
            return False
        finally:
            session.close()

    def health_check(self) -> bool:
        """ìƒíƒœ í™•ì¸"""
        session = self.SessionLocal()
        try:
            session.execute('SELECT 1')
            logger.info("âœ… PostgreSQL health check passed")
            return True
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return False
        finally:
            session.close()


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    db = PostgreSQLConnector()

    # ë©”íŠ¸ë¦­ ì €ì¥
    metrics_1min = {
        'window_start': int(__import__('time').time() * 1000),
        'window_end': int(__import__('time').time() * 1000) + 60000,
        'total_impressions': 1000,
        'total_clicks': 165,
        'ctr': 16.5,
        'category_ctr': {'news': 15.5, 'sports': 17.2}
    }
    db.insert_1min_metrics(metrics_1min)

    # ìƒíƒœ í™•ì¸
    db.health_check()
```

#### 4-4. PostgreSQL & Redis Docker ì¶”ê°€ (20ë¶„)

**íŒŒì¼:** `docker-compose.yml` (ì—…ë°ì´íŠ¸)

```yaml
  # PostgreSQL
  postgres:
    image: postgres:15-alpine
    container_name: postgres
    environment:
      POSTGRES_DB: marketing_roas
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./src/postgres/schema.sql:/docker-entrypoint-initdb.d/01-schema.sql:ro
    networks:
      - kafka-network
    healthcheck:
      test: pg_isready -U postgres
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis (ì´ë¯¸ Week 1ì— ì¶”ê°€ë¨)
  redis:
    image: redis:7.2-alpine
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - kafka-network
    healthcheck:
      test: redis-cli ping
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  postgres_data:
  redis_data:
```

**ì‹¤í–‰:**
```bash
docker-compose up -d postgres redis

# ìƒíƒœ í™•ì¸
docker-compose ps

# PostgreSQL ì—°ê²° í…ŒìŠ¤íŠ¸
docker-compose exec postgres psql -U postgres -d marketing_roas -c "SELECT schema_name FROM information_schema.schemata;"

# Redis ì—°ê²° í…ŒìŠ¤íŠ¸
docker-compose exec redis redis-cli PING
```

### âœ… ì™„ë£Œ ê¸°ì¤€

- [ ] Redis ìºì‹œ ë§¤ë‹ˆì € êµ¬í˜„ ì™„ë£Œ
- [ ] PostgreSQL ìŠ¤í‚¤ë§ˆ ìƒì„± ì™„ë£Œ
- [ ] DB ì»¤ë„¥í„° êµ¬í˜„ ì™„ë£Œ
- [ ] docker-compose.yml ì—…ë°ì´íŠ¸ ì™„ë£Œ
- [ ] PostgreSQL í—¬ìŠ¤ì²´í¬ í†µê³¼
- [ ] Redis í—¬ìŠ¤ì²´í¬ í†µê³¼
- [ ] ë°ì´í„° ì €ì¥/ì¡°íšŒ í…ŒìŠ¤íŠ¸ ì„±ê³µ

### ğŸ“Š ì‚°ì¶œë¬¼

```
src/redis/
â”œâ”€â”€ cache_manager.py

src/postgres/
â”œâ”€â”€ schema.sql
â””â”€â”€ db_connector.py

docker-compose.yml (PostgreSQL, Redis ì¶”ê°€)
```

---

## ğŸ“Œ Day 5 (ê¸ˆ): Streamlit ëŒ€ì‹œë³´ë“œ & í†µí•© í…ŒìŠ¤íŠ¸ (2ì‹œê°„)

### ëª©í‘œ
- Streamlit ëŒ€ì‹œë³´ë“œ êµ¬ì¶•
- Redis ì‹¤ì‹œê°„ ë°ì´í„° ì¡°íšŒ ë° ì‹œê°í™”
- ì°¨íŠ¸ ë° ì§€í‘œ êµ¬í˜„
- E2E í†µí•© í…ŒìŠ¤íŠ¸

### ğŸ“‹ í• ë‹¹ ì‹œê°„
| ì‘ì—… | ì‹œê°„ |
|------|------|
| Streamlit ê¸°ë³¸ êµ¬ì¡° | 30ë¶„ |
| ì°¨íŠ¸ ë° ì§€í‘œ | 40ë¶„ |
| ì‹¤ì‹œê°„ ë°ì´í„° ì—°ë™ | 30ë¶„ |
| E2E í…ŒìŠ¤íŠ¸ | 20ë¶„ |

### ğŸ› ï¸ ì‹¤ìŠµ ë‚´ìš©

#### 5-1. Streamlit ëŒ€ì‹œë³´ë“œ (40ë¶„)

**íŒŒì¼:** `src/streamlit/dashboard.py`

```python
"""
ì‹¤ì‹œê°„ CTR ë¶„ì„ ëŒ€ì‹œë³´ë“œ
"""

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
import sys
import os

# ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from redis.cache_manager import RedisCacheManager
from postgres.db_connector import PostgreSQLConnector

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Ad CTR Dashboard",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ìŠ¤íƒ€ì¼
st.markdown("""
    <style>
    .metric-card {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
        margin: 10px 0;
    }
    </style>
""", unsafe_allow_html=True)


class Dashboard:
    """ëŒ€ì‹œë³´ë“œ í´ë˜ìŠ¤"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        try:
            self.redis = RedisCacheManager(
                host=os.getenv('REDIS_HOST', 'localhost'),
                port=int(os.getenv('REDIS_PORT', 6379))
            )
            self.db = PostgreSQLConnector()
        except Exception as e:
            st.error(f"Connection error: {e}")
            self.redis = None
            self.db = None

    def get_current_metrics(self):
        """í˜„ì¬ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        if not self.redis:
            return {}
        return self.redis.get_all_current_metrics()

    def render_header(self):
        """í—¤ë” ë Œë”ë§"""
        st.title("ğŸ“Š ì‹¤ì‹œê°„ ê´‘ê³  CTR ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
        st.markdown("---")

        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric(
                label="í˜„ì¬ ìƒíƒœ",
                value="ğŸŸ¢ LIVE",
                delta="ì‹¤ì‹œê°„"
            )

        with col2:
            st.metric(
                label="ì—…ë°ì´íŠ¸",
                value=datetime.now().strftime("%H:%M:%S"),
                delta_color="off"
            )

        with col3:
            st.metric(
                label="ë°ì´í„° ì†ŒìŠ¤",
                value="Kafka + Flink",
                delta_color="off"
            )

    def render_1min_metrics(self):
        """1ë¶„ ë©”íŠ¸ë¦­ ë Œë”ë§"""
        st.subheader("â±ï¸  1ë¶„ ë‹¨ìœ„ ë©”íŠ¸ë¦­")

        metrics = self.get_current_metrics()
        metrics_1min = metrics.get('1min', {})

        if not metrics_1min:
            st.info("ì•„ì§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ë°ì´í„° ì¤€ë¹„
        data = []
        for key, value in metrics_1min.items():
            data.append({
                'Window': key,
                'CTR': value.get('ctr', 0),
                'Impressions': value.get('total_impressions', 0),
                'Clicks': value.get('total_clicks', 0)
            })

        df = pd.DataFrame(data)

        col1, col2, col3 = st.columns(3)

        with col1:
            avg_ctr = df['CTR'].mean() if len(df) > 0 else 0
            st.metric("í‰ê·  CTR", f"{avg_ctr:.2f}%")

        with col2:
            total_impressions = df['Impressions'].sum()
            st.metric("ì´ ë…¸ì¶œìˆ˜", f"{total_impressions:,}")

        with col3:
            total_clicks = df['Clicks'].sum()
            st.metric("ì´ í´ë¦­ìˆ˜", f"{total_clicks:,}")

        # í‘œ ì¶œë ¥
        st.dataframe(df, use_container_width=True)

        # ì°¨íŠ¸
        if len(df) > 0:
            fig = px.bar(df, x='Window', y='CTR', title='Windowë³„ CTR')
            st.plotly_chart(fig, use_container_width=True)

    def render_5min_metrics(self):
        """5ë¶„ ë©”íŠ¸ë¦­ ë Œë”ë§"""
        st.subheader("ğŸ“Š 5ë¶„ ë‹¨ìœ„ ë©”íŠ¸ë¦­")

        metrics = self.get_current_metrics()
        metrics_5min = metrics.get('5min', {})

        if not metrics_5min:
            st.info("ì•„ì§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ë°ì´í„° ì¤€ë¹„
        data = []
        for key, value in metrics_5min.items():
            data.append({
                'Window': key,
                'CTR': value.get('ctr', 0),
                'Impressions': value.get('total_impressions', 0),
                'Clicks': value.get('total_clicks', 0)
            })

        df = pd.DataFrame(data)

        col1, col2, col3 = st.columns(3)

        with col1:
            avg_ctr = df['CTR'].mean() if len(df) > 0 else 0
            st.metric("í‰ê·  CTR", f"{avg_ctr:.2f}%")

        with col2:
            total_impressions = df['Impressions'].sum()
            st.metric("ì´ ë…¸ì¶œìˆ˜", f"{total_impressions:,}")

        with col3:
            total_clicks = df['Clicks'].sum()
            st.metric("ì´ í´ë¦­ìˆ˜", f"{total_clicks:,}")

        # í‘œ ì¶œë ¥
        st.dataframe(df, use_container_width=True)

        # ì°¨íŠ¸
        if len(df) > 0:
            fig = px.line(df, x='Window', y='CTR', title='ì‹œê°„ëŒ€ë³„ CTR ì¶”ì´', markers=True)
            st.plotly_chart(fig, use_container_width=True)

    def render_comparision(self):
        """1ë¶„ vs 5ë¶„ ë¹„êµ"""
        st.subheader("ğŸ” 1ë¶„ vs 5ë¶„ ë¹„êµ")

        metrics = self.get_current_metrics()

        col1, col2 = st.columns(2)

        with col1:
            metrics_1min = metrics.get('1min', {})
            if metrics_1min:
                ctr_1min = [v.get('ctr', 0) for v in metrics_1min.values()]
                st.metric("í‰ê·  1ë¶„ CTR", f"{sum(ctr_1min)/len(ctr_1min):.2f}%" if ctr_1min else "N/A")

        with col2:
            metrics_5min = metrics.get('5min', {})
            if metrics_5min:
                ctr_5min = [v.get('ctr', 0) for v in metrics_5min.values()]
                st.metric("í‰ê·  5ë¶„ CTR", f"{sum(ctr_5min)/len(ctr_5min):.2f}%" if ctr_5min else "N/A")

    def run(self):
        """ëŒ€ì‹œë³´ë“œ ì‹¤í–‰"""
        self.render_header()

        tab1, tab2, tab3, tab4 = st.tabs(["1ë¶„ ë©”íŠ¸ë¦­", "5ë¶„ ë©”íŠ¸ë¦­", "ë¹„êµ", "ì„¤ì •"])

        with tab1:
            self.render_1min_metrics()

        with tab2:
            self.render_5min_metrics()

        with tab3:
            self.render_comparision()

        with tab4:
            st.subheader("âš™ï¸  ì„¤ì •")
            st.write("**Redis ì—°ê²° ìƒíƒœ:**")
            if self.redis and self.redis.health_check():
                st.success("âœ… ì •ìƒ")
            else:
                st.error("âŒ ì‹¤íŒ¨")

            st.write("**PostgreSQL ì—°ê²° ìƒíƒœ:**")
            if self.db and self.db.health_check():
                st.success("âœ… ì •ìƒ")
            else:
                st.error("âŒ ì‹¤íŒ¨")

        # ìë™ ìƒˆë¡œê³ ì¹¨
        st.write("---")
        col1, col2 = st.columns(2)
        with col1:
            refresh_interval = st.slider("ìƒˆë¡œê³ ì¹¨ ê°„ê²© (ì´ˆ)", 5, 60, 10)
        with col2:
            if st.button("ğŸ”„ ì§€ê¸ˆ ìƒˆë¡œê³ ì¹¨"):
                st.rerun()


if __name__ == "__main__":
    dashboard = Dashboard()
    dashboard.run()
```

**ì‹¤í–‰:**
```bash
streamlit run src/streamlit/dashboard.py

# ë˜ëŠ” íŠ¹ì • í¬íŠ¸ ì§€ì •
streamlit run src/streamlit/dashboard.py --server.port 8501
```

#### 5-2. Streamlit ì„¤ì • (10ë¶„)

**íŒŒì¼:** `src/streamlit/.streamlit/config.toml`

```toml
[theme]
primaryColor = "#FF6B6B"
backgroundColor = "#FFFFFF"
secondaryBackgroundColor = "#F0F2F6"
textColor = "#262730"
font = "sans serif"

[client]
toolbarMode = "minimal"
showErrorDetails = true

[server]
port = 8501
headless = true
runOnSave = true
maxUploadSize = 200
```

#### 5-3. E2E í†µí•© í…ŒìŠ¤íŠ¸ (40ë¶„)

**íŒŒì¼:** `scripts/week2_e2e_test.sh`

```bash
#!/bin/bash

set -e

echo "=========================================="
echo "E2E TEST: Week 2 Pipeline Validation"
echo "=========================================="

# 1ï¸âƒ£  ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
echo ""
echo "1ï¸âƒ£  Checking all services..."
docker-compose ps

# 2ï¸âƒ£  Redis í—¬ìŠ¤ ì²´í¬
echo ""
echo "2ï¸âƒ£  Redis health check..."
docker-compose exec redis redis-cli PING || echo "Redis not ready"

# 3ï¸âƒ£  PostgreSQL í—¬ìŠ¤ ì²´í¬
echo ""
echo "3ï¸âƒ£  PostgreSQL health check..."
docker-compose exec postgres psql -U postgres -d marketing_roas -c "SELECT 1;" || echo "PostgreSQL not ready"

# 4ï¸âƒ£  Flink ìƒíƒœ í™•ì¸
echo ""
echo "4ï¸âƒ£  Flink JobManager status..."
curl -s http://localhost:8081/overview | jq '.["taskmanagers"]' || echo "Flink not ready"

# 5ï¸âƒ£  Producer ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ ë°ì´í„°)
echo ""
echo "5ï¸âƒ£  Running producer (1,000 messages)..."
cd src/kafka
timeout 60 python producer.py 1000 || true
cd ../../

# 6ï¸âƒ£  Redis ë°ì´í„° í™•ì¸
echo ""
echo "6ï¸âƒ£  Checking Redis data..."
docker-compose exec redis redis-cli KEYS "metrics:*" | head -5 || echo "No metrics yet"

# 7ï¸âƒ£  PostgreSQL ë°ì´í„° í™•ì¸
echo ""
echo "7ï¸âƒ£  Checking PostgreSQL data..."
docker-compose exec postgres psql -U postgres -d marketing_roas -c "SELECT COUNT(*) FROM realtime.metrics_1min;" || echo "No data yet"

echo ""
echo "=========================================="
echo "âœ… E2E TEST COMPLETE"
echo "=========================================="
echo ""
echo "ëŒ€ì‹œë³´ë“œ ì ‘ì†: http://localhost:8501"
echo "Flink UI: http://localhost:8081"
echo "Prometheus: http://localhost:9090"
```

**ì‹¤í–‰:**
```bash
chmod +x scripts/week2_e2e_test.sh
bash scripts/week2_e2e_test.sh
```

### âœ… ì™„ë£Œ ê¸°ì¤€

- [ ] Streamlit ëŒ€ì‹œë³´ë“œ êµ¬ì¶• ì™„ë£Œ
- [ ] Redis ì‹¤ì‹œê°„ ë°ì´í„° ì—°ë™ ì™„ë£Œ
- [ ] PostgreSQL ë°ì´í„° ì¡°íšŒ ì™„ë£Œ
- [ ] ì°¨íŠ¸ ë° ì§€í‘œ í‘œì‹œ ì™„ë£Œ
- [ ] http://localhost:8501 ì ‘ê·¼ ê°€ëŠ¥
- [ ] E2E í…ŒìŠ¤íŠ¸ í†µê³¼

### ğŸ“Š ì‚°ì¶œë¬¼

```
src/streamlit/
â”œâ”€â”€ dashboard.py (ë©”ì¸ ëŒ€ì‹œë³´ë“œ)
â””â”€â”€ .streamlit/
    â””â”€â”€ config.toml (ì„¤ì •)

scripts/
â””â”€â”€ week2_e2e_test.sh (í†µí•© í…ŒìŠ¤íŠ¸)
```

---

## ì£¼ê°„ ë§ˆì¼ìŠ¤í†¤

- âœ… PyFlink ê°œë°œí™˜ê²½ êµ¬ì¶• ì™„ë£Œ
- âœ… Kafka â†’ Flink ìŠ¤íŠ¸ë¦¬ë° ì—°ê²° ì™„ë£Œ
- âœ… 1ë¶„, 5ë¶„ Window CTR ê³„ì‚° ì™„ë£Œ
- âœ… Redis ìºì‹œ ì €ì¥ ì™„ë£Œ
- âœ… PostgreSQL ë°ì´í„° ì ì¬ ì™„ë£Œ
- âœ… Streamlit ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ êµ¬ì¶• ì™„ë£Œ

## ì„±ëŠ¥ ì§€í‘œ

| ì§€í‘œ | ëª©í‘œ | ìƒíƒœ |
|------|------|------|
| ë©”ì‹œì§€ ì²˜ë¦¬ ë ˆì´í„´ì‹œ | < 1ì´ˆ | - |
| ìºì‹œ ì¡°íšŒ ì‘ë‹µì‹œê°„ | < 100ms | - |
| ëŒ€ì‹œë³´ë“œ ê°±ì‹  ì£¼ê¸° | 10ì´ˆ | - |
| Window ë©”íŠ¸ë¦­ ì •í™•ë„ | > 99% | - |

## ìœ„í—˜ìš”ì†Œ ë° í•´ê²°ì±…

| ìœ„í—˜ìš”ì†Œ | ì˜í–¥ | í•´ê²°ì±… |
|---------|------|--------|
| ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ (Flink) | ë†’ìŒ | Checkpoint ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ |
| Redis ë©”ëª¨ë¦¬ ë¶€ì¡± | ë†’ìŒ | TTL ì •ì±… ê°•í™” |
| ëŒ€ì‹œë³´ë“œ ëŠë¦° ì‘ë‹µ | ì¤‘ê°„ | ìºì‹œ ìµœì í™” |
| ë°ì´í„° ì†ì‹¤ | ë†’ìŒ | Checkpoint ë° ë°±ì—… |

## ë‹¤ìŒ ì£¼ ì¤€ë¹„ì‚¬í•­

- [ ] Airflow í™˜ê²½ êµ¬ì¶•
- [ ] dbt ëª¨ë¸ ì„¤ê³„
- [ ] DLQ ì¬ì²˜ë¦¬ ë¡œì§ ì¤€ë¹„
- [ ] Monitoring/Alerting ê°•í™”

---

**Week 2 ì™„ë£Œ!** ğŸ‰
**ë‹¤ìŒ ì£¼ ëª©í‘œ:** Airflow ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ & dbt ë°ì´í„° ëª¨ë¸ë§
