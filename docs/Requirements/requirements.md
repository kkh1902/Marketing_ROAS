# ğŸ“„ **requirements.md (ì´ˆì•ˆ)**

ì•„ë˜ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ë³µë¶™í•´ì„œ ì €ì¥í•˜ë©´ ëœë‹¤.
ì¶”ê°€í•˜ê³  ì‹¶ì€ ìš”êµ¬ì‚¬í•­ ìˆìœ¼ë©´ ë°”ë¡œ í™•ì¥í•´ì¤„ê²Œ.

---

# **Requirements: Ad Events Realtime & Batch Data Pipeline**

## **1. í”„ë¡œì íŠ¸ ê°œìš”**

ê´‘ê³  í”Œë«í¼ì—ì„œ ë°œìƒí•˜ëŠ” ëŒ€ê·œëª¨ ê´‘ê³  ì´ë²¤íŠ¸ ë¡œê·¸(40M rows / 10ì¼)ë¥¼ ì‹¤ì‹œê°„Â·ë°°ì¹˜ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬
CTR/CPA/ROAS ë“± í•µì‹¬ ë©”íŠ¸ë¦­ì„ ë¹ ë¥´ê²Œ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•œë‹¤.

---

## **2. ë°ì´í„°**

### **2.1 Source Data**

* Dataset: **Avazu Click-Through Prediction**
* Format: `.gz` ì••ì¶• CSV
* Volume: **ì•½ 40M rows (10ì¼ì¹˜)**
* Fields: `click`, `hour`, `device_id`, `site_domain`, `app_id`, â€¦ (ì•½ 20+ ì»¬ëŸ¼)

---

## **3. ì‹œìŠ¤í…œ êµ¬ì„± ìš”êµ¬ì‚¬í•­**

## **3.1 Ingestion Layer**

* Python Kafka ProducerëŠ” row ë‹¨ìœ„ë¡œ JSON ì´ë²¤íŠ¸ ìƒì„±í•˜ì—¬ Kafka topicì— ë°œí–‰í•œë‹¤.
* ë ˆì½”ë“œ ê²€ì¦ ì‹¤íŒ¨ ì‹œ **DLQ Topic(ad_events_error)**ë¡œ ì „ì†¡í•œë‹¤.
* Schema Registryë¥¼ ì‚¬ìš©í•´ JSON schemaë¥¼ ê´€ë¦¬í•œë‹¤.

---

## **3.2 Streaming Layer (Kafka)**

* ì£¼ìš” Topic:

  * `ad_events_raw` (partition=3)
  * `ad_events_error` (DLQ)
* Kafka JMX Exporter í™œì„±í™” (ë©”íŠ¸ë¦­: consumer lag, throughput, broker I/O)
* Kafka Connect(optional) ì—°ê²° ê°€ëŠ¥ êµ¬ì¡° ìœ ì§€

---

## **3.3 Realtime Compute Layer (Flink)**

* PyFlink Streaming Job ì‚¬ìš©
* Event Time ê¸°ë°˜ ì²˜ë¦¬
* Watermark ì ìš©
* Window Aggregation:

  * **1ë¶„ tumbling window**
  * **5ë¶„ tumbling window**
* ê³„ì‚° ë©”íŠ¸ë¦­:

  * CTR(clicks / impressions)
  * CPC(cost / clicks)
  * CPA(cost / conversions)
  * ROAS(revenue / cost)
* ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ DLQë¡œ ì „ì†¡
* Checkpoint/State Backend:

  * ë¡œì»¬: `./data/checkpoints`
    *(í–¥í›„ MinIO/S3ë¡œ í™•ì¥ ê°€ëŠ¥)*

---

## **3.4 Storage Layer**

### **PostgreSQL**

* Schema êµ¬ì„±:

  * `realtime` : ì‹¤ì‹œê°„ ìœˆë„ìš° ì§‘ê³„ ê²°ê³¼ ì €ì¥
  * `analytics` : DW ë° Mart í…Œì´ë¸”
  * `errors` : DLQ ì˜êµ¬ ì €ì¥
* Constraint:

  * primary key ê¸°ë°˜ upsert ì§€ì›
  * ì‹œê°„ íŒŒí‹°ì…˜ ê³ ë ¤

### **File Storage (Batch)**

* ê²½ë¡œ:

  * `./data/raw/`
  * `./data/processed/`
* Batch ETLì—ì„œ ì‚¬ìš©

---

## **3.5 Batch Layer**

### **Airflow (localhost:8080)**

DAGs:

1. `dag_daily_etl`

   * Raw â†’ Processed ë³€í™˜
   * ê²°ì¸¡ê°’/í˜•ì‹ ê²€ì¦

2. `dag_dbt_run`

   * dbt model run
   * dbt test ìˆ˜í–‰
   * Failed tests â†’ Slack Alert

### **dbt**

* staging / mart ëª¨ë¸ êµ¬ì„±
* í…ŒìŠ¤íŠ¸:

  * not_null
  * unique
  * relationship
* Logical Layer:

  * daily_impressions
  * daily_clicks
  * campaign_performance

---

## **3.6 Analytics Layer**

### **Streamlit (localhost:8501)**

* ì‹¤ì‹œê°„ CTR ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
* PostgreSQL realtime schemaì—ì„œ pull

### **Metabase (localhost:3000)**

* DW ê¸°ë°˜ ë¦¬í¬íŠ¸ ìƒì„±
* ê´‘ê³  ì„±ê³¼ ë¶„ì„(CTR/CPA/ROAS ë“±)

---

## **3.7 Monitoring & Alerting**

### **Prometheus**

* Kafka JMX Exporter ìˆ˜ì§‘
* Flink metrics ìˆ˜ì§‘
* Airflow API metrics ìˆ˜ì§‘(optional)

### **Grafana**

* Consumer lag dashboard
* Flink job throughput
* PG QPS trends

### **Slack Alerts**

* DLQ ì¬ì‹œë„ ì‹¤íŒ¨
* Airflow DAG ì‹¤íŒ¨
* dbt test ì‹¤íŒ¨

---

## **4. DLQ Requirements**

DLQ ì´ë²¤íŠ¸ëŠ” ë‹¤ìŒ ë°ì´í„°ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•œë‹¤:

* raw_payload
* error_type (parsing, schema, flink_logic)
* retry_count
* error_timestamp
* stacktrace(optional)

ì¬ì‹œë„ ì •ì±…:

* ìµœëŒ€ 3íšŒ ì¬ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ PostgreSQL `errors` í…Œì´ë¸”ì— ì €ì¥
* Slack ì•Œë¦¼ ë°œì†¡

---

## **5. ìš´ì˜ ìš”êµ¬ì‚¬í•­**

### **5.1 ë¡œì»¬ ê°œë°œ**

* ëª¨ë“  ì‹œìŠ¤í…œì€ Docker Compose ê¸°ë°˜ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•´ì•¼ í•œë‹¤.
* êµ¬ì„± ìš”ì†Œ:

  * Kafka, Zookeeper
  * Schema Registry
  * Prometheus, Grafana
  * PostgreSQL
  * Airflow
  * Streamlit
  * Metabase

### **5.2 í™•ì¥ì„±**

* partition ìˆ˜ ì¦ê°€ ì‹œ Flink ë³‘ë ¬ ì²˜ë¦¬ ìë™ í™•ì¥
* PostgreSQL â†’ Cloud warehouse(BigQuery/Snowflake/Athena)ë¡œ í™•ì¥ ê³ ë ¤

### **5.3 ì‹ ë¢°ì„±**

* Exactly-once ê°€ëŠ¥í•˜ë„ë¡ Flink ì²´í¬í¬ì¸íŠ¸ ì•ˆì •ì ì¸ backend í•„ìš”
* Airflow task-level retry ì ìš©
* Monitoring & Alerting í•„ìˆ˜

---

## **6. ì£¼ìš” KPI**

* ì‹¤ì‹œê°„ ì²˜ë¦¬ ì§€ì—°: < 5ì´ˆ
* Kafka ingestion TPS: 5k/s+
* Flink window latency: < 2s
* Daily batch ì²˜ë¦¬ ì†Œìš”: < 10ë¶„
* DLQ ë°œìƒë¥ : < 0.1%

---

## **7. ì‚°ì¶œë¬¼**

* ì „ì²´ ì•„í‚¤í…ì²˜ Flowchart (Mermaid)
* Docker Compose íŒŒì¼
* PyFlink streaming job
* Airflow DAG 2ì¢…
* dbt project
* Streamlit dashboard
* Metabase dashboards
* requirements.md(ë³¸ ë¬¸ì„œ)

